<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0" xml:base="https://news.mit.edu">
  <channel>
    <title>MIT News - Robotics</title>
    <link>https://news.mit.edu/topic/mitrobotics-rss.xml</link>
    <atom:link href="https://news.mit.edu/topic/mitrobotics-rss.xml" rel="self" type="application/rss+xml"/>
    <description>MIT news feed about: Robotics</description>
    <language>en</language>
    
    <lastBuildDate>Thu, 13 Feb 2025 00:00:00 -0500</lastBuildDate>
    <item>
  <title>Engineers enable a drone to determine its position in the dark and indoors</title>
  <link>https://news.mit.edu/2025/engineers-enable-drone-determine-its-position-dark-and-indoors-0213</link>
  <description><![CDATA[A new low-power system using radio frequency waves takes a major step toward autonomous, indoor drone navigation.]]></description>
  <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/engineers-enable-drone-determine-its-position-dark-and-indoors-0213</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;In the future, autonomous drones could be used to shuttle inventory between large warehouses. A drone might fly into a semi-dark structure the size of several football fields, zipping along hundreds of identical aisles before docking at the precise spot where its shipment is needed.&lt;/p&gt;&lt;p&gt;Most of today’s drones would likely struggle to complete this task, since drones typically navigate outdoors using GPS, which doesn’t work in indoor environments. For indoor navigation, some drones employ computer vision or lidar, but both techniques are unreliable in dark environments or rooms with plain walls or repetitive features.&lt;/p&gt;&lt;p&gt;MIT researchers have introduced a new approach that enables a drone to self-localize, or determine its position, in indoor, dark, and low-visibility environments. Self-localization is a key step in autonomous navigation.&lt;/p&gt;&lt;p&gt;The researchers developed a system called &lt;a href="https://www.mit.edu/~fadel/papers/mmDrone-infocom2025.pdf" target="_blank"&gt;MiFly&lt;/a&gt;, in which a drone uses radio frequency (RF) waves, reflected by a single tag placed in its environment, to autonomously self-localize.&lt;/p&gt;&lt;p&gt;Because MiFly enables self-localization with only one small tag, which could be affixed to a wall like a sticker, it would be cheaper and easier to implement than systems that require multiple tags. In addition, since the MiFly tag reflects signals sent by the drone, rather than generating its own signal, it can be operated with very low power.&lt;/p&gt;&lt;p&gt;Two off-the-shelf radars mounted on the drone enable it to localize in relation to the tag. Those measurements are fused with data from the drone’s onboard computer, which enables it to estimate its trajectory.&lt;/p&gt;&lt;p&gt;The researchers conducted hundreds of flight experiments with real drones in indoor environments, and found that MiFly consistently localized the drone to within fewer than 7 centimeters.&lt;/p&gt;&lt;p&gt;“As our understanding of perception and computing improves, we often forget about signals that are beyond the visible spectrum. Here, we’ve looked beyond GPS and computer vision to millimeter waves, and by doing so, we’ve opened up new capabilities for drones in indoor environments that were not possible before,” says Fadel Adib, associate professor in the Department of Electrical Engineering and Computer Science, director of the Signal Kinetics group in the MIT Media Lab, and senior author of a &lt;a href="https://www.mit.edu/~fadel/papers/mmDrone-infocom2025.pdf" target="_blank"&gt;paper on MiFly&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Adib is joined on the paper by co-lead authors and research assistants Maisy Lam and Laura Dodds; Aline Eid, a former postdoc who is now an assistant professor at the University of Michigan; and Jimmy Hester, CTO and co-founder of Atheraxon, Inc. The research will be presented at the IEEE Conference on Computer Communications.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Backscattered signals&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To enable drones to self-localize within dark, indoor environments, the researchers decided to utilize millimeter wave signals. Millimeter waves, which are commonly used in modern radars and 5G communication systems, work in the dark and can travel through everyday materials like cardboard, plastic, and interior walls.&lt;/p&gt;&lt;p&gt;They set out to create a system that could work with just one tag, so it would be cheaper and easier to implement in commercial environments. To ensure the device remained low power, they designed a backscatter tag that reflects millimeter wave signals sent by a drone’s onboard radar. The drone uses those reflections to self-localize.&lt;/p&gt;&lt;p&gt;But the drone’s radar would receive signals reflected from all over the environment, not just the tag. The researchers surmounted this challenge by employing a technique called modulation. They configured the tag to add a small frequency to the signal it scatters back to the drone.&lt;/p&gt;&lt;p&gt;“Now, the reflections from the surrounding environment come back at one frequency, but the reflections from the tag come back at a different frequency. This allows us to separate the responses and just look at the response from the tag,” Dodds says.&lt;/p&gt;&lt;p&gt;However, with just one tag and one radar, the researchers could only calculate distance measurements. They needed multiple signals to compute the drone’s location.&lt;/p&gt;&lt;p&gt;Rather than using more tags, they added a second radar to the drone, mounting one horizontally and one vertically. The horizontal radar has a horizontal polarization, which means it sends signals horizontally, while the vertical radar would have a vertical polarization.&lt;/p&gt;&lt;p&gt;They incorporated polarization into the tag’s antennas so it could isolate the separate signals sent by each radar.&lt;/p&gt;&lt;p&gt;“Polarized sunglasses receive a certain polarization of light and block out other polarizations. We applied the same concept to millimeter waves,” Lam explains.&lt;/p&gt;&lt;p&gt;In addition, they applied different modulation frequencies to the vertical and horizontal signals, further reducing interference.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Precise location estimation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This dual-polarization and dual-modulation architecture gives the drone’s spatial location. But drones also move at an angle and rotate, so to enable a drone to navigate, it must estimate its position in space with respect to six degrees of freedom — with trajectory data including pitch, yaw, and roll in addition to the usual forward/backward, left/right, and up/down.&lt;/p&gt;&lt;p&gt;“The drone rotation adds a lot of ambiguity to the millimeter wave estimates. This is a big problem because drones rotate quite a bit as they are flying,” Dodds says.&lt;/p&gt;&lt;p&gt;They overcame these challenges by utilizing the drone’s onboard inertial measurement unit, a sensor that measures acceleration as well as changes in altitude and attitude. By fusing this information with the millimeter wave measurements reflected by the tag, they enable MiFly to estimate the full six-degree-of-freedom pose of the drone in only a few milliseconds.&lt;/p&gt;&lt;p&gt;They tested a MiFly-equipped drone in several indoor environments, including their lab, the flight space at MIT, and the dim tunnels beneath the campus buildings. The system achieved high accuracy consistently across all environments, localizing the drone to within 7 centimeters in many experiments.&lt;/p&gt;&lt;p&gt;In addition, the system was nearly as accurate in situations where the tag was blocked from the drone’s view. They achieved reliable localization estimates up to 6 meters from the tag.&lt;/p&gt;&lt;p&gt;That distance could be extended in the future with the use of additional hardware, such as high-power amplifiers, or by improving the radar and antenna design. The researchers also plan to conduct further research by incorporating MiFly into an autonomous navigation system. This could enable a drone to decide where to fly and execute a flight path using millimeter wave technology.&lt;/p&gt;&lt;p&gt;“The infrastructure and localization algorithms we build up for this work are a strong foundation to go on and make them more robust to enable diverse commercial applications,” Lam says.&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the National Science Foundation and the MIT Media Lab.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202502/MIT-Self-Local-01-press.jpg?itok=2ojG61mS" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT researchers developed a system that enables a drone to determine its position in 6D space in indoor, dark, or low-visibility environments using radio frequency waves.  The drone has 2 radars; the horizontal radar has a horizontal polarization, which means it sends signals horizontally, while the vertical radar would have a vertical polarization.]]></media:description>
              <media:credit>Image: Figures courtesy the researchers; edited by MIT News</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/electronics">Electronics</category>
      <category domain="https://news.mit.edu/topic/wireless">Wireless</category>
      <category domain="https://news.mit.edu/topic/sensors">Sensors</category>
      <category domain="https://news.mit.edu/topic/drones">Drones</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>Engineering joy</title>
  <link>https://news.mit.edu/2025/engineering-joy-woodie-flowers-0209</link>
  <description><![CDATA[How the late Woodie Flowers helped create a new foundation for “the MIT way” of teaching.]]></description>
  <pubDate>Sun, 09 Feb 2025 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/engineering-joy-woodie-flowers-0209</guid>
        <dc:creator>Anne Wilson | Department of Mechanical Engineering</dc:creator>
  <content:encoded>&lt;p&gt;When the late professor emeritus Woodie Flowers SM ’68, MEng ’71, PhD ’73 was a student at MIT, most of his classes involved paper-and-pencil exercises with predetermined solutions. Flowers had an affinity for making things, and for making them work. When he transitioned from student to teacher, he chose to carry this approach into his method of instruction and, in doing so, he helped change the way engineering students are educated — at MIT, and around the world.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.mit.edu/2019/Professor-Emeritus-Woodie-Flowers-dies-75-1014"&gt;Flowers passed away in 2019&lt;/a&gt;, but his legacy lives on, and the magnitude of the educational revolution he helped to evolve was profound&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;In the 1970s, Flowers took over instruction of 2.70, now called class 2.007 (Design and Manufacturing I). The&amp;nbsp;capstone course is one that many first-year students today look forward to taking, but that wasn’t always the case. Before Flowers took over instruction, class instruction relied heavily on chalkboard demonstrations.&lt;/p&gt;&lt;p&gt;“Their idea of design at the time was to draw drawings of parts,” explains Professor Emeritus David Gossard PhD ’75, Flowers’ longtime friend and colleague. “Woody had a different idea. Give the entire class a kit of materials [and] a common goal, which was to build a machine — to climb a hill, or pick up golf balls, or whatever it did — and make a contest out of it. It was a phenomenal success. The kids loved it, the faculty loved it, the Institute loved it. And over a period of years, it became, I think it's fair to say, an institution.”&lt;/p&gt;&lt;p&gt;With Flowers at the lead, 2.70 transformed into a project-based, get-your-hands-dirty, robotics-competition-focused experience. By all accounts, he also made the experience incredibly fun — something he valued in his own life. He was fond of skydiving and was often seen rollerblading through the Infinite Corridor. The course, informed by his unique style, was at the forefront of a revolution in engineering education, and it quickly helped solidify the Department of Mechanical Engineering’s reputation for innovative education.&lt;/p&gt;&lt;p&gt;“A lot of kids had never started from scratch and built anything,” Flowers once told &lt;em&gt;The Boston Globe&lt;/em&gt;. His advisor, Robert Mann, had similar beliefs in a hands-on, modern pedagogy. Building on Mann’s philosophy, and incorporating his own approach, Flowers breathed new life and provided a new foundation for “the MIT way” of teaching. This was a reinvigoration at the right place and the right time that ultimately had a global butterfly effect on the popularity of science, technology, engineering, and math (STEM) instruction.&lt;/p&gt;&lt;p&gt;“Over the years lectures had displaced the hands-on stuff, and Woodie brought it back,” says Sanjay Sarma, the Fred Fort Flowers (1941) and Daniel Fort Flowers (1941) Professor in Mechanical Engineering.&amp;nbsp;“I can’t think of a single person to have impacted the field of robotics and design in undergraduate, or high school, education as much as Woodie.”&lt;/p&gt;&lt;p&gt;Flowers became interested in mechanical engineering and design at a young age, thanks in large part to his parents. His father was a welder with a penchant for tinkering, inventing, and building, his mother was an elementary school teacher. Flowers grew up taking things apart and putting them back together — an activity which he seemed to believe made students better engineers.&lt;/p&gt;&lt;p&gt;Speaking in 2010 with&lt;em&gt; InfiniteMIT&lt;/em&gt;, a digital archive of Institute history made possible by the generosity of Jane and A. Neil Pappalardo ’64,&amp;nbsp;&lt;a href="https://infinite.mit.edu/video/woodie-c-flowers-sm-68-me-71-phd-73"&gt;Flowers shared a story&lt;/a&gt; about a student who had accepted the task in her group of finding out whether a piece of reinforcement steel rebar could be bent into a tight loop and serve as a bearing.&lt;/p&gt;&lt;p&gt;“She came into lab and I was there early, and she had a slightly bent piece of rebar. It had been heated — you could tell that it had been hot, and she was going to report that she really can’t do that, it just kind of doesn’t work,” Flowers recalled. He suggested they try another approach.&lt;/p&gt;&lt;p&gt;“We went out in the lab and I found another big steel bar and I found the biggest vice I could find,” he continued. Flowers cranked the rebar down against the piece of steel he was going to wrap it around, then took a four-pound sledgehammer to it. “My father had a blacksmith pit, so that was familiar to me. I wrapped [the rebar around the steel and] made a fine bearing. As I finish the last blow, I looked up and three of the best students in the class — really sharp people — were standing there with their jaw open. They’d never seen anyone hit a piece of steel hard enough to just mold it.”&lt;/p&gt;&lt;p&gt;He continued, “that visceral understanding of the behavior of mechanics is really important. It doesn’t fall out of the sky and it certainly doesn’t come out of a textbook, it comes through real interaction. I believe I had been so lucky because when I encountered Castiglione’s theorem about deflection of materials, it kind of made sense.”&lt;/p&gt;&lt;p&gt;Course 2.70/2.007 is considered a landmark class in engineering education. It was one of the first hands-on classes to teach students not only how to design an object, but also how to build it and, by demonstrating the value of practical, project-based learning and robotics competitions, it has influenced the approach taken by many other programs. Today, it continues to develop students’ competence and self-confidence as design engineers, with an emphasis on the creative design process bolstered by application of physical laws, robustness, and manufacturability.&lt;/p&gt;&lt;p&gt;Notably, the course also served as the inspiration for development of the FIRST Robotics program, which Flowers and inventor Dean Kamen started in 1989. FIRST has programs for preschool through high school students and, to date, more than 3.2 million youth from more than 100 countries have participated in FIRST competitions.&lt;/p&gt;&lt;p&gt;In the 1970s, the parts kit — or as Flowers fondly referred to it, the “bag of junk” — included things like springs, tongue depressors, and rubber bands.&amp;nbsp;Flowers’ wife Margaret recalls spending many nights packing these kits and hosting advisees in their home. “We considered ourselves a team,” she says.&lt;/p&gt;&lt;p&gt;Today, in addition to using the kit of mechanical parts and materials, students in 2.007 might develop 3D printed components, and they incorporate electronics in their robots for an autonomous portion of the final competition.&lt;/p&gt;&lt;p&gt;The spring 2024 competition, themed after Cartoon Network’s popular animated science fiction sitcom “Rick and Morty,” featured a spaceship that students’ robots could interact with for points, vats of “acid” where balls could be collected and placed in tubes, and game pieces that paid homage to iconic episodes. The final task required the robot to travel up an elevator and send a character down a zipline.&lt;/p&gt;&lt;p&gt;In recent years, other themes have centered on tasks related to stories ranging from “Star Wars” to&lt;em&gt;&amp;nbsp;&lt;/em&gt;“Back to the Future”&lt;em&gt;&amp;nbsp;&lt;/em&gt;and “Wakanda Forever.” The 2022 theme, however, may have been the most poignant theme to date: “Legacy,” a celebration of Flowers’ life and work.&lt;/p&gt;&lt;p&gt;“[Woodie] revealed, unambiguously, that designing, fabricating, assembling and building things was fun,” says Gossard. “It was arguably the essence of engineering. There was joy in it.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;A version of this article appears in the Spring 2025 issue of MechE Connects, the magazine of the MIT Department of Mechanical Engineering.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202502/woodie-flowers-first-00.jpg?itok=Uj_I-9Jy" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Woodie Flowers helped change the way engineering students are educated at MIT and beyond. Notably, he transformed the class currently known as 2.007 (Design and Manufacturing I) into a project-based, robotics-competition-focused experience. This led to the development of the FIRST Robotics program, which Flowers and inventor Dean Kamen started in 1989.]]></media:description>
              <media:credit>Photo courtesy of FIRST.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/profile">Profile</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/stem-education">STEM education</category>
      <category domain="https://news.mit.edu/topic/history-mit">History of MIT</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/education">Education, teaching, academics</category>
      <category domain="https://news.mit.edu/topic/history-science">History of science</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-museum">MIT Museum</category>
    </item>
<item>
  <title>Eleven MIT faculty receive Presidential Early Career Awards</title>
  <link>https://news.mit.edu/2025/mit-faculty-receive-presidential-early-career-awards-0203</link>
  <description><![CDATA[Faculty members and additional MIT alumni are among 400 scientists and engineers recognized for outstanding leadership potential.]]></description>
  <pubDate>Mon, 03 Feb 2025 17:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-faculty-receive-presidential-early-career-awards-0203</guid>
        <dc:creator>Jordan Silva | School of Engineering</dc:creator>
  <content:encoded>&lt;p&gt;Eleven MIT faculty, including nine from the School of Engineering and two from the School of Science, were awarded the Presidential Early Career Award for Scientists and Engineers (PECASE). Fifteen additional MIT alumni were also honored.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Established in 1996 by President Bill Clinton, the PECASE is awarded to scientists and engineers “who show exceptional potential for leadership early in their research careers.” The latest recipients were&amp;nbsp;&lt;a href="https://bidenwhitehouse.archives.gov/ostp/news-updates/2025/01/14/president-biden-honors-nearly-400-federally-funded-early-career-scientists/" target="_blank"&gt;announced by the White House&lt;/a&gt; on Jan. 14 under President Joe Biden.&amp;nbsp;Fourteen government agencies recommended researchers for the award.&lt;/p&gt;&lt;p&gt;The MIT faculty and alumni honorees are among 400 scientists and engineers recognized for innovation and scientific contributions. Those from the School of Engineering and School of Science who were honored are:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.eecs.mit.edu/people/tamara-broderick/"&gt;Tamara Broderick&lt;/a&gt;, associate professor in&amp;nbsp;the Department of Electrical Engineering and Computer Science (EECS), was nominated by the Office of Naval Research for her project advancing “Lightweight representations for decentralized learning in data-rich environments.”&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.eecs.mit.edu/people/michael-carbin/"&gt;Michael James Carbin SM ’09, PhD ’15&lt;/a&gt;, associate professor in the Department of EECS,&amp;nbsp;was nominated by the National Science Foundation (NSF) for his CAREER award, a project that developed techniques to execute programs reliably on approximate and unreliable computation substrates.&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.eecs.mit.edu/people/christina-delimitrou/"&gt;Christina Delimitrou&lt;/a&gt;,&amp;nbsp;the KDD Career Development Professor in Communications and Technology and associate Professor in the Department of EECS, was nominated by the NSF for her group’s work on redesigning the cloud system stack given new cloud programming frameworks like microservices and serverless compute, as well as designing hardware acceleration techniques that make cloud data centers more predictable and resource-efficient.&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://physics.mit.edu/faculty/netta-engelhardt/" title="https://physics.mit.edu/faculty/netta-engelhardt/"&gt;Netta Engelhardt&lt;/a&gt;,&amp;nbsp;the&amp;nbsp;Biedenharn Career Development Associate Professor of Physics, was nominated by the Department of Energy for her research on the black hole information paradox and its implications for the fundamental quantum structure of space and time.&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://chemistry.mit.edu/profile/robert-j-gilliard/"&gt;Robert Gilliard Jr&lt;/a&gt;., the Novartis Associate Professor of Chemistry, was selected based the results generated from his 2020 National Science Foundation CAREER award entitled: "CAREER: Boracycles with Unusual Bonding as Creative Strategies for Main-Group Functional Materials.”&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://cheme.mit.edu/profile/heather-j-kulik/"&gt;Heather Janine Kulik PD ’09, PhD ’09&lt;/a&gt;,&amp;nbsp;the Lammot du Pont Professor of Chemical Engineering, was nominated by the NSF for her 2019 proposal entitled “CAREER: Revealing spin-state-dependent reactivity in open-shell single atom catalysts with systematically-improvable computational tools.”&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://web.mit.edu/nse/people/faculty/loureiro.html"&gt;Nuno Loureiro&lt;/a&gt;, professor in the departments of Nuclear Science and Engineering and Physics,&amp;nbsp;was nominated by the NSF for his work on the generation and amplification of magnetic fields in the universe.&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://dmse.mit.edu/faculty/robert-j-macfarlane/"&gt;Robert Macfarlane&lt;/a&gt;, associate professor in the Department of Materials Science and Engineering,&amp;nbsp;was nominated by the Department of Defense (DoD)’s Air Force Office of Scientific Research. His research focuses on making new materials using molecular and nanoscale building blocks.&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://meche.mit.edu/people/faculty/ritur%40mit.edu"&gt;Ritu Raman&lt;/a&gt;,&amp;nbsp;the Eugene Bell Career Development Professor of Tissue Engineering in the Department of Mechanical Engineering, was nominated by the DoD for her ARO-funded research that explored leveraging biological actuators in next-generation robots that can sense and adapt to their environments.&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://meche.mit.edu/people/faculty/etr@mit.edu"&gt;Ellen Roche&lt;/a&gt;,&amp;nbsp;the Latham Family Career Development Professor and associate department head in the Department of Mechanical Engineering, was nominated by the NSF for her CAREER award, a project that aims to create a cutting-edge benchtop model combining soft robotics and organic tissue to accurately simulate the motions of the heart and diaphragm.&lt;br&gt;&amp;nbsp;&lt;/li&gt;&lt;li&gt;&lt;a href="https://mlkscholars.mit.edu/scholars/justin-wilkerson/"&gt;Justin Wilkerson&lt;/a&gt;, a visiting associate professor in the Department of Aeronautics and Astronautics, was&amp;nbsp;nominated by the Air Force Office of Scientific Research (AFOSR) for his research primarily related to the design and optimization of novel multifunctional composite materials that can survive extreme environments.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Additional MIT alumni who were honored include:&amp;nbsp;Ambika Bajpayee MNG ’07, PhD ’15; Katherine Bouman SM ’13, PhD ’17; Walter Cheng-Wan Lee ’95, MNG ’95, PhD ’05; Ismaila Dabo PhD ’08; Ying Diao SM ’10, PhD ’12; Eno Ebong ’99; Soheil Feizi- Khankandi SM ’10, PhD ’16; Mark Finlayson SM ’01, PhD ’12; Chelsea B. Finn ’14; Grace Xiang Gu SM ’14, PhD ’18; David Michael Isaacson PhD ’06, AF ’16; Lewei Lin ’05; Michelle Sander PhD ’12;&amp;nbsp;Kevin Solomon SM ’08, PhD ’12; and&amp;nbsp;Zhiting Tian PhD ’14.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202502/mit-PECASE-winners.jpg?itok=iO3vvocm" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Top row, from left to right: Tamara Broderick, Michael James Carbin, Christina Delimitrou, and Netta Engelhardt. Middle row, left to right: Robert Gilliard Jr., Heather Janine Kulik, Nuno Loureiro, and Robert Macfarlane. Bottom row, left to right: Ritu Raman, Ellen Roche, and Justin Wilkerson.]]></media:description>
          </media:content>
        <category domain="https://news.mit.edu/topic/awards">Awards, honors and fellowships</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/chemical-engineering">Chemical engineering</category>
      <category domain="https://news.mit.edu/topic/chemistry-0">Chemistry</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/dmse">DMSE</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/nuclear-engineering">Nuclear science and engineering</category>
      <category domain="https://news.mit.edu/topic/physics">Physics</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/black-holes">Black holes</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/president-biden">President Biden</category>
      <category domain="https://news.mit.edu/topic/doe">Department of Energy (DoE)</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/department-defense-dod">Department of Defense (DoD)</category>
    </item>
<item>
  <title>MIT engineers help multirobot systems stay in the safety zone</title>
  <link>https://news.mit.edu/2025/mit-engineers-help-multirobot-systems-stay-safety-zone-0131</link>
  <description><![CDATA[New research could improve the safety of drone shows, warehouse robots, and self-driving cars.]]></description>
  <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/mit-engineers-help-multirobot-systems-stay-safety-zone-0131</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Drone shows are an increasingly popular form of large-scale light display. These shows incorporate hundreds to thousands of airborne bots, each programmed to fly in paths that together form intricate shapes and patterns across the sky. When they go as planned, drone shows can be spectacular. But when one or more drones malfunction, as has happened recently in Florida, New York, and elsewhere, they can be a serious hazard to spectators on the ground.&lt;/p&gt;&lt;p&gt;Drone show accidents highlight the challenges of maintaining safety in what engineers call “multiagent systems” — systems of multiple coordinated, collaborative, and computer-programmed agents, such as robots, drones, and self-driving cars.&lt;/p&gt;&lt;p&gt;Now, a team of MIT engineers has developed a training method for multiagent systems that can guarantee&amp;nbsp;their safe operation in crowded environments. The researchers found that once the method is used to train a small number of agents, the safety margins and controls learned by those agents can automatically scale to any larger number of agents, in a way that ensures the safety of the system as a whole.&lt;/p&gt;&lt;p&gt;In real-world demonstrations, the team trained a small number of palm-sized drones to safely carry out different objectives, from simultaneously switching positions midflight to landing on designated moving vehicles on the ground. In simulations, the researchers showed that the same programs, trained on a few drones, could be copied and scaled up to thousands of drones, enabling a large system of agents to safely accomplish the same tasks.&lt;/p&gt;&lt;p&gt;“This could be a standard for any application that requires a team of agents, such as warehouse robots, search-and-rescue drones, and self-driving cars,” says Chuchu Fan, associate professor of aeronautics and astronautics at MIT. “This provides a shield, or safety filter, saying each agent can continue with their mission, and we’ll tell you how to be safe.”&lt;/p&gt;&lt;p&gt;Fan and her colleagues report on their new method in a study &lt;a href="https://dspace.mit.edu/handle/1721.1/158072" target="_blank"&gt;appearing this month in the journal &lt;em&gt;IEEE Transactions on Robotics&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt; The study’s co-authors are MIT graduate students Songyuan Zhang and Oswin So as well as former MIT postdoc Kunal Garg, who is now an assistant professor at Arizona State University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mall margins&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When engineers design for safety in any multiagent system, they typically have to consider the potential paths of every single agent with respect to every other agent in the system. This pair-wise path-planning is a time-consuming and computationally expensive process. And even then, safety is not guaranteed.&lt;/p&gt;&lt;p&gt;“In a drone show, each drone is given a specific trajectory — a set of waypoints and a set of times — and then they essentially close their eyes and follow the plan,” says Zhang, the study’s lead author. “Since they only know where they have to be and at what time, if there are unexpected things that happen, they don’t know how to adapt.”&lt;/p&gt;&lt;p&gt;The MIT team looked instead to develop a method to train a small number of agents to maneuver safely, in a way that could efficiently scale to any number of agents in the system. And, rather than plan specific paths for individual agents, the method would enable agents to continually map their safety margins, or boundaries beyond which they might be unsafe. An agent could then take any number of paths to accomplish its task, as long as it stays within its safety margins.&lt;/p&gt;&lt;p&gt;In some sense, the team says the method is similar to how humans intuitively navigate their surroundings.&lt;/p&gt;&lt;p&gt;“Say you’re in a really crowded shopping mall,” So explains. “You don’t care about anyone beyond the people who are in your immediate neighborhood, like the 5 meters surrounding you, in terms of getting around safely and not bumping into anyone. Our work takes a similar local approach.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Safety barrier&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In their new study, the team presents their method, GCBF+, which stands for “Graph Control Barrier Function.” A barrier function is a mathematical term used in robotics that calculates a sort of safety barrier, or a boundary beyond which an agent has a high probability of being unsafe. For any given agent, this safety zone can change moment to moment, as the agent moves among other agents that are themselves moving within the system.&lt;/p&gt;&lt;p&gt;When designers calculate barrier functions for any one agent in a multiagent system, they typically have to take into account the potential paths and interactions with every other agent in the system. Instead, the MIT team’s method calculates the safety zones of just a handful of agents, in a way that is accurate enough to represent the dynamics of many more agents in the system.&lt;/p&gt;&lt;p&gt;“Then we can sort of copy-paste this barrier function for every single agent, and then suddenly we have a graph of safety zones that works for any number of agents in the system,” So says.&lt;/p&gt;&lt;p&gt;To calculate an agent’s barrier function, the team’s method first takes into account an agent’s “sensing radius,” or how much of the surroundings an agent can observe, depending on its sensor capabilities. Just as in the shopping mall analogy, the researchers assume that the agent only cares about the agents that are within its sensing radius, in terms of keeping safe and avoiding collisions with those agents.&lt;/p&gt;&lt;p&gt;Then, using computer models that capture an agent’s particular mechanical capabilities and limits, the team simulates a “controller,” or a set of instructions for how the agent and a handful of similar agents should move around. They then run simulations of multiple agents moving along certain trajectories, and record whether and how they collide or otherwise interact.&lt;/p&gt;&lt;p&gt;“Once we have these trajectories, we can compute some laws that we want to minimize, like say, how many safety violations we have in the current controller,” Zhang says. “Then we update the controller to be safer.”&lt;/p&gt;&lt;p&gt;In this way, a controller can be programmed into actual agents, which would enable them to continually map their safety zone based on any other agents they can sense in their immediate surroundings, and then move within that safety zone to accomplish their task.&lt;/p&gt;&lt;p&gt;“Our controller is reactive,” Fan says. “We don’t preplan a path beforehand. Our controller is constantly taking in information about where an agent is going, what is its velocity, how fast other drones are going. It’s using all this information to come up with a plan on the fly and it’s replanning every time. So, if the situation changes, it’s always able to adapt to stay safe.”&lt;/p&gt;&lt;p&gt;The team demonstrated GCBF+ on a system of eight Crazyflies — lightweight, palm-sized quadrotor drones that they tasked with flying and switching positions in midair. If the drones were to do so by taking the straightest path, they would surely collide. But after training with the team’s method, the drones were able to make real-time adjustments to maneuver around each other, keeping within their respective safety zones, to successfully switch positions on the fly.&lt;/p&gt;&lt;p&gt;In similar fashion, the team tasked the drones with flying around, then landing on specific Turtlebots — wheeled robots with shell-like tops. The Turtlebots drove continuously around in a large circle, and the Crazyflies were able to avoid colliding with each other as they made their landings.&lt;/p&gt;&lt;p&gt;“Using our framework, we only need to give the drones their destinations instead of the whole collision-free trajectory, and the drones can figure out how to arrive at their destinations without collision themselves,” says Fan, who envisions the method could be applied to any multiagent system to guarantee its safety, including collision avoidance systems in drone shows, warehouse robots, autonomous driving vehicles, and drone delivery systems.&lt;/p&gt;&lt;p&gt;This work was partly supported by the U.S. National Science Foundation, MIT Lincoln Laboratory under the Safety in Aerobatic Flight Regimes (SAFR) program, and the Defence Science and Technology Agency of Singapore.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202501/MIT-Multi-Agent-Control-01-PRESS.jpg?itok=pMX7kGKG" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT engineers developed a training method for multiagent systems, such as large numbers of drones, that can guarantee their safe operation in crowded environments.]]></media:description>
              <media:credit>Image: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/drones">Drones</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>New training approach could help AI agents perform better in uncertain conditions</title>
  <link>https://news.mit.edu/2025/new-training-approach-could-help-ai-perform-better-0129</link>
  <description><![CDATA[Sometimes, it might be better to train a robot in an environment that’s different from the one where it will be deployed.]]></description>
  <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/new-training-approach-could-help-ai-perform-better-0129</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;A home robot trained to perform household tasks in a factory may fail to effectively scrub the sink or take out the trash when deployed in a user’s kitchen, since this new environment differs from its training space.&lt;/p&gt;&lt;p&gt;To avoid this, engineers often try to match the simulated training environment as closely as possible with the real world where the agent will be deployed.&lt;/p&gt;&lt;p&gt;However, researchers from MIT and elsewhere have now found that, despite this conventional wisdom, sometimes training in a completely different environment yields a better-performing artificial intelligence agent.&lt;/p&gt;&lt;p&gt;Their results indicate that, in some situations, training a simulated AI agent in a world with less uncertainty, or “noise,” enabled it to perform better than a competing AI agent trained in the same, noisy world they used to test both agents.&lt;/p&gt;&lt;p&gt;The researchers call this unexpected phenomenon the indoor training effect.&lt;/p&gt;&lt;p&gt;“If we learn to play tennis in an indoor environment where there is no noise, we might be able to more easily master different shots. Then, if we move to a noisier environment, like a windy tennis court, we could have a higher probability of playing tennis well than if we started learning in the windy environment,” explains Serena Bono, a research assistant in the MIT Media Lab and lead author of a paper on the indoor training effect.&lt;/p&gt;&lt;p&gt;The researchers studied this phenomenon by training AI agents to play Atari games, which they modified by adding some unpredictability. They were surprised to find that the indoor training effect consistently occurred across Atari games and game variations.&lt;/p&gt;&lt;p&gt;They hope these results fuel additional research toward developing better training methods for AI agents.&lt;/p&gt;&lt;p&gt;“This is an entirely new axis to think about. Rather than trying to match the training and testing environments, we may be able to construct simulated environments where an AI agent learns even better,” adds co-author Spandan Madan, a graduate student at Harvard University.&lt;/p&gt;&lt;p&gt;Bono and Madan are joined on the paper by Ishaan Grover, an MIT graduate student; Mao Yasueda, a graduate student at Yale University; Cynthia Breazeal, professor of media arts and sciences and leader of the Personal Robotics Group in the MIT Media Lab; Hanspeter Pfister, the An Wang Professor of Computer Science at Harvard; and Gabriel Kreiman, a professor at Harvard Medical School. The research will be presented at the Association for the Advancement of Artificial Intelligence Conference.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Training troubles&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers set out to explore why reinforcement learning agents tend to have such dismal performance when tested on environments that differ from their training space.&lt;/p&gt;&lt;p&gt;Reinforcement learning is a trial-and-error method in which the agent explores a training space and learns to take actions that maximize its reward.&lt;/p&gt;&lt;p&gt;The team developed a technique to explicitly add a certain amount of noise to one element of the reinforcement learning problem called the transition function. The transition function defines the probability an agent will move from one state to another, based on the action it chooses.&lt;/p&gt;&lt;p&gt;If the agent is playing Pac-Man, a transition function might define the probability that ghosts on the game board will move up, down, left, or right. In standard reinforcement learning, the AI would be trained and tested using the same transition function.&lt;/p&gt;&lt;p&gt;The researchers added noise to the transition function with this conventional approach and, as expected, it hurt the agent’s Pac-Man performance.&lt;/p&gt;&lt;p&gt;But when the researchers trained the agent with a noise-free Pac-Man game, then tested it in an environment where they injected noise into the transition function, it performed better than an agent trained on the noisy game.&lt;/p&gt;&lt;p&gt;“The rule of thumb is that you should try to capture the deployment condition’s transition function as well as you can during training to get the most bang for your buck. We really tested this insight to death because we couldn’t believe it ourselves,” Madan says.&lt;/p&gt;&lt;p&gt;Injecting varying amounts of noise into the transition function let the researchers test many environments, but it didn’t create realistic games. The more noise they injected into Pac-Man, the more likely ghosts would randomly teleport to different squares.&lt;/p&gt;&lt;p&gt;To see if the indoor training effect occurred in normal Pac-Man games, they adjusted underlying probabilities so ghosts moved normally but were more likely to move up and down, rather than left and right. AI agents trained in noise-free environments still performed better in these realistic games.&lt;/p&gt;&lt;p&gt;“It was not only due to the way we added noise to create ad hoc environments. This seems to be a property of the reinforcement learning problem. And that was even more surprising to see,” Bono says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Exploration explanations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers dug deeper in search of an explanation, they saw some correlations in how the AI agents explore the training space.&lt;/p&gt;&lt;p&gt;When both AI agents explore mostly the same areas, the agent trained in the non-noisy environment performs better, perhaps because it is easier for the agent to learn the rules of the game without the interference of noise.&lt;/p&gt;&lt;p&gt;If their exploration patterns are different, then the agent trained in the noisy environment tends to perform better. This might occur because the agent needs to understand patterns it can’t learn in the noise-free environment.&lt;/p&gt;&lt;p&gt;“If I only learn to play tennis with my forehand in the non-noisy environment, but then in the noisy one I have to also play with my backhand, I won’t play as well in the non-noisy environment,” Bono explains.&lt;/p&gt;&lt;p&gt;In the future, the researchers hope to explore how the indoor training effect might occur in more complex reinforcement learning environments, or with other techniques like computer vision and natural language processing. They also want to build training environments designed to leverage the indoor training effect, which could help AI agents perform better in uncertain environments.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202501/MIT-IndoorTraining-01-press.jpg?itok=xVezkZp6" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT researchers trained AI agents to play Atari games that were modified to include some unpredictability.]]></media:description>
              <media:credit>Image: Jose-Luis Olivares, MIT</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/games">Games</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
    </item>
<item>
  <title>Expanding robot perception</title>
  <link>https://news.mit.edu/2025/expanding-robot-perception-luca-carlone-0128</link>
  <description><![CDATA[Associate Professor Luca Carlone is working to give robots a more human-like awareness of their environment.]]></description>
  <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/expanding-robot-perception-luca-carlone-0128</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Robots have come a long way since the Roomba. Today, drones are starting to deliver door to door, self-driving cars are navigating some roads, robo-dogs are aiding first responders, and still more bots are doing backflips and helping out on the factory floor. Still, Luca Carlone thinks the best is yet to come.&lt;/p&gt;&lt;p&gt;Carlone, who recently received tenure as an associate professor in MIT’s Department of Aeronautics and Astronautics (AeroAstro), directs the SPARK Lab, where he and his students are bridging a key gap between humans and robots: perception. The group does theoretical and experimental research, all toward expanding a robot’s awareness of its environment in ways that approach human perception. And perception, as Carlone often says, is more than detection.&lt;/p&gt;&lt;p&gt;While robots have grown by leaps and bounds in terms of their ability to detect and identify objects in their surroundings, they still have a lot to learn when it comes to making higher-level sense of their environment. As humans, we perceive objects with an intuitive sense of not just of their shapes and labels but also their physics — how they might be manipulated and moved — and how they relate to each other, their larger environment, and ourselves.&lt;/p&gt;&lt;p&gt;That kind of human-level perception is what Carlone and his group are hoping to impart to robots, in ways that enable them to safely and seamlessly interact with people in their homes, workplaces, and other unstructured environments.&lt;/p&gt;&lt;p&gt;Since joining the MIT faculty in 2017, Carlone has led his team in developing and applying perception and scene-understanding algorithms for various applications, including autonomous underground search-and-rescue vehicles, drones that can pick up and manipulate objects on the fly, and self-driving cars. They might also be useful for domestic robots that follow natural language commands and potentially even anticipate human’s needs based on higher-level contextual clues.&lt;/p&gt;&lt;p&gt;“Perception is a big bottleneck toward getting robots to help us in the real world,” Carlone says. “If we can add elements of cognition and reasoning to robot perception, I believe they can do a lot of good.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Expanding horizons&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Carlone was born and raised near Salerno, Italy, close to the scenic Amalfi coast, where he was the youngest of three boys. His mother is a retired elementary school teacher who taught math, and his father is a retired history professor and publisher, who has always taken an analytical approach to his historical research. The brothers may have unconsciously adopted their parents’ mindsets, as all three went on to be engineers — the older two pursued electronics and mechanical engineering, while Carlone landed on robotics, or mechatronics, as it was known at the time.&lt;/p&gt;&lt;p&gt;He didn’t come around to the field, however, until late in his undergraduate studies. Carlone attended the Polytechnic University of Turin, where he focused initially on theoretical work, specifically on control theory — a field that applies mathematics to develop algorithms that automatically control the behavior of physical systems, such as power grids, planes, cars, and robots. Then, in his senior year, Carlone signed up for a course on robotics that explored advances in manipulation and how robots can be programmed to move and function.&lt;/p&gt;&lt;p&gt;“It was love at first sight. Using algorithms and math to develop the brain of a robot and make it move and interact with the environment is one of the most fulfilling experiences,” Carlone says. “I immediately decided this is what I want to do in life.”&lt;/p&gt;&lt;p&gt;He went on to a dual-degree program at the Polytechnic University of Turin and the Polytechnic University of Milan, where he received master’s degrees in mechatronics and automation engineering, respectively. As part of this program, called the Alta Scuola Politecnica, Carlone also took courses in management, in which he and students from various academic backgrounds had to team up to conceptualize, build, and draw up a marketing pitch for a new product design. Carlone’s team developed a touch-free table lamp designed to follow a user’s hand-driven commands. The project pushed him to think about engineering from different perspectives.&lt;/p&gt;&lt;p&gt;“It was like having to speak different languages,” he says. “It was an early exposure to the need to look beyond the engineering bubble and think about how to create technical work that can impact the real world.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The next generation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Carlone stayed in Turin to complete his PhD in mechatronics. During that time, he was given freedom to choose a thesis topic, which he went about, as he recalls, “a bit naively.”&lt;/p&gt;&lt;p&gt;“I was exploring a topic that the community considered to be well-understood, and for which many researchers believed there was nothing more to say.” Carlone says. “I underestimated how established the topic was, and thought I could still contribute something new to it, and I was lucky enough to just do that.”&lt;/p&gt;&lt;p&gt;The topic in question was “simultaneous localization and mapping,” or SLAM — the problem of generating and updating a map of a robot’s environment while simultaneously keeping track of where the robot is within that environment. Carlone came up with a way to reframe the problem, such that algorithms could generate more precise maps without having to start with an initial guess, as most SLAM methods did at the time. His work helped to crack open a field where most roboticists thought one could not do better than the existing algorithms.&lt;/p&gt;&lt;p&gt;“SLAM is about figuring out the geometry of things and how a robot moves among those things,” Carlone says. “Now I’m part of a community asking, what is the next generation of SLAM?”&lt;/p&gt;&lt;p&gt;In search of an answer, he accepted a postdoc position at Georgia Tech, where he dove into coding and computer vision — a field that, in retrospect, may have been inspired by a brush with blindness: As he was finishing up his PhD in Italy, he suffered a medical complication that severely affected his vision.&lt;/p&gt;&lt;p&gt;“For one year, I could have easily lost an eye,” Carlone says. “That was something that got me thinking about the importance of vision, and artificial vision.”&lt;/p&gt;&lt;p&gt;He was able to receive good medical care, and the condition resolved entirely, such that he could continue his work. At Georgia Tech, his advisor, &lt;a href="https://dellaert.github.io/" target="_blank"&gt;Frank Dellaert&lt;/a&gt;, showed him ways to code in computer vision and formulate elegant mathematical representations of complex, three-dimensional problems. His advisor was also one of the first to develop an open-source SLAM library, called &lt;a href="https://gtsam.org/" target="_blank"&gt;GTSAM&lt;/a&gt;, which Carlone quickly recognized to be an invaluable resource. More broadly, he saw that making software available to all unlocked a huge potential for progress in robotics as a whole.&lt;/p&gt;&lt;p&gt;“Historically, progress in SLAM has been very slow, because people kept their codes proprietary, and each group had to essentially start from scratch,” Carlone says. “Then open-source pipelines started popping up, and that was a game changer, which has largely driven the progress we have seen over the last 10 years.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Spatial AI&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Following Georgia Tech, Carlone came to MIT in 2015 as a postdoc in the Laboratory for Information and Decision Systems (LIDS). During that time, he collaborated with Sertac Karaman, professor of aeronautics and astronautics, in developing software to help palm-sized drones navigate their surroundings using very little on-board power. A year later, he was promoted to research scientist, and then in 2017, Carlone accepted a faculty position in AeroAstro.&lt;/p&gt;&lt;p&gt;“One thing I fell in love with at MIT was that all decisions are driven by questions like: What are our values? What is our mission? It’s never about low-level gains. The motivation is really about how to improve society,” Carlone says. “As a mindset, that has been very refreshing.”&lt;/p&gt;&lt;p&gt;Today, Carlone’s group is developing ways to represent a robot’s surroundings, beyond characterizing their geometric shape and semantics. He is utilizing deep learning and large language models to develop algorithms that enable robots to perceive their environment through a higher-level lens, so to speak. Over the last six years, his lab has released more than 60 open-source &lt;a href="https://github.com/orgs/MIT-SPARK/" target="_blank"&gt;repositories&lt;/a&gt;, which are used by thousands of researchers and practitioners worldwide. The bulk of his work fits into a larger, emerging field known as “spatial AI.”&lt;/p&gt;&lt;p&gt;“Spatial AI is like SLAM on steroids,” Carlone says. “In a nutshell, it has to do with enabling robots to think and understand the world as humans do, in ways that can be useful.”&lt;/p&gt;&lt;p&gt;It’s a huge undertaking that could have wide-ranging impacts, in terms of enabling more intuitive, interactive robots to help out at home, in the workplace, on the roads, and in remote and potentially dangerous areas.&amp;nbsp;Carlone says there will be plenty of work ahead, in order to come close to how humans perceive the world.&lt;/p&gt;&lt;p&gt;“I have 2-year-old twin daughters, and I see them manipulating objects, carrying 10 different toys at a time, navigating across cluttered rooms with ease, and quickly adapting to new environments. Robot perception cannot yet match what a toddler can do,” Carlone says. “But we have new tools in the arsenal. And the future is bright.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202501/MIT-luca-carlone-01-press.jpg?itok=ZqdiBcYc" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[“Perception is a big bottleneck toward getting robots to help us in the real world,” Luca Carlone says. “If we can add elements of cognition and reasoning to robot perception, I believe they can do a lot of good.”]]></media:description>
              <media:credit>Photo: Bryce Vickmark</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/profile">Profile</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/automation">automation</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/software">Software</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/drones">Drones</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>This fast and agile robotic insect could someday aid in mechanical pollination</title>
  <link>https://news.mit.edu/2025/fast-agile-robotic-insect-could-someday-aid-mechanical-pollination-0115</link>
  <description><![CDATA[With a new design, the bug-sized bot was able to fly 100 times longer than prior versions.]]></description>
  <pubDate>Wed, 15 Jan 2025 14:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2025/fast-agile-robotic-insect-could-someday-aid-mechanical-pollination-0115</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;With a more efficient method for artificial pollination, farmers in the future could grow fruits and vegetables inside multilevel warehouses, boosting yields while mitigating some of agriculture’s harmful impacts on the environment.&lt;/p&gt;&lt;p&gt;To help make this idea a reality, MIT researchers are developing robotic insects that could someday swarm out of mechanical hives to rapidly perform precise pollination. However, even the best bug-sized robots are no match for natural pollinators like bees when it comes to endurance, speed, and maneuverability.&lt;/p&gt;&lt;p&gt;Now, inspired by the anatomy of these natural pollinators, the researchers have overhauled their design to produce tiny, aerial robots that are far more agile and durable than prior versions.&lt;/p&gt;&lt;p&gt;The new bots can hover for about 1,000 seconds, which is more than 100 times longer than previously demonstrated. The robotic insect, which weighs less than a paperclip, can fly significantly faster than similar bots while completing acrobatic maneuvers like double aerial flips.&lt;/p&gt;&lt;p&gt;The revamped robot is designed to boost flight precision and agility while minimizing the mechanical stress on its artificial wing flexures, which enables faster maneuvers, increased endurance, and a longer lifespan.&lt;/p&gt;&lt;p&gt;The new design also has enough free space that the robot could carry tiny batteries or sensors, which could enable it to fly on its own outside the lab.&lt;/p&gt;&lt;p&gt;“The amount of flight we demonstrated in this paper is probably longer than the entire amount of flight our field has been able to accumulate with these robotic insects. With the improved lifespan and precision of this robot, we are getting closer to some very exciting applications, like assisted pollination,” says Kevin Chen, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), head of the Soft and Micro Robotics Laboratory within the Research Laboratory of Electronics (RLE), and the senior author of an open-access paper on the new design.&lt;/p&gt;&lt;p&gt;Chen is joined on the paper by co-lead authors Suhan Kim and Yi-Hsuan Hsiao, who are EECS graduate students; as well as EECS graduate student Zhijian Ren and summer visiting student Jiashu Huang. The research &lt;a href="https://doi.org/10.1126/scirobotics.adp4256" target="_blank"&gt;appears today in &lt;em&gt;Science Robotics&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Boosting performance&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.mit.edu/2021/researchers-introduce-new-generation-tiny-agile-drones-0302" target="_blank"&gt;Prior versions of the robotic insect&lt;/a&gt; were composed of four identical units, each with two wings, combined into a rectangular device about the size of a microcassette.&lt;/p&gt;&lt;p&gt;“But there is no insect that has eight wings. In our old design, the performance of each individual unit was always better than the assembled robot,” Chen says.&lt;/p&gt;&lt;p&gt;This performance drop was partly caused by the arrangement of the wings, which would blow air into each other when flapping, reducing the lift forces they could generate.&lt;/p&gt;&lt;p&gt;The new design chops the robot in half. Each of the four identical units now has one flapping wing pointing away from the robot’s center, stabilizing the wings and boosting their lift forces. With half as many wings, this design also frees up space so the robot could carry electronics.&lt;/p&gt;&lt;p&gt;In addition, the researchers created more complex transmissions that connect the wings to the actuators, or artificial muscles, that flap them. These durable transmissions, which required the design of longer wing hinges, reduce the mechanical strain that limited the endurance of past versions.&lt;/p&gt;&lt;p&gt;“Compared to the old robot, we can now generate control torque three times larger than before, which is why we can do very sophisticated and very accurate path-finding flights,” Chen says.&lt;/p&gt;&lt;p&gt;Yet even with these design innovations, there is still a gap between the best robotic insects and the real thing. For instance, a bee has only two wings, yet it can perform rapid and highly controlled motions.&lt;/p&gt;&lt;p&gt;“The wings of bees are finely controlled by a very sophisticated set of muscles. That level of fine-tuning is something that truly intrigues us, but we have not yet been able to replicate,” he says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Less strain, more force&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The motion of the robot’s wings is driven by artificial muscles. These tiny, soft actuators are made from layers of elastomer sandwiched between two very thin carbon nanotube electrodes and then rolled into a squishy cylinder. The actuators rapidly compress and elongate, generating mechanical force that flaps the wings.&lt;/p&gt;&lt;p&gt;In previous designs, when the actuator’s movements reach the extremely high frequencies needed for flight, the devices often start buckling. That reduces the power and efficiency of the robot. The new transmissions inhibit this bending-buckling motion, which reduces the strain on the artificial muscles and enables them to apply more force to flap the wings.&lt;/p&gt;&lt;p&gt;Another new design involves a long wing hinge that reduces torsional stress experienced during the flapping-wing motion. Fabricating the hinge, which is about 2 centimeters long but just 200 microns in diameter, was among their greatest challenges.&lt;/p&gt;&lt;p&gt;“If you have even a tiny alignment issue during the fabrication process, the wing hinge will be slanted instead of rectangular, which affects the wing kinematics,” Chen says.&lt;/p&gt;&lt;p&gt;After many attempts, the researchers perfected a multistep laser-cutting process that enabled them to precisely fabricate each wing hinge.&lt;/p&gt;&lt;p&gt;With all four units in place, the new robotic insect can hover for more than 1,000 seconds, which equates to almost 17 minutes, without showing any degradation of flight precision.&lt;/p&gt;&lt;p&gt;“When my student Nemo was performing that flight, he said it was the slowest 1,000 seconds he had spent in his entire life. The experiment was extremely nerve-racking,” Chen says.&lt;/p&gt;&lt;p&gt;The new robot also reached an average speed of 35 centimeters per second, the fastest flight researchers have reported, while performing body rolls and double flips. It can even precisely track a trajectory that spells M-I-T.&lt;/p&gt;&lt;p&gt;“At the end of the day, we’ve shown flight that is 100 times longer than anyone else in the field has been able to do, so this is an extremely exciting result,” he says.&lt;/p&gt;&lt;p&gt;From here, Chen and his students want to see how far they can push this new design, with the goal of achieving flight for longer than 10,000 seconds.&lt;/p&gt;&lt;p&gt;They also want to improve the precision of the robots so they could land and take off from the center of a flower. In the long run, the researchers hope to install tiny batteries and sensors onto the aerial robots so they could fly and navigate outside the lab.&lt;/p&gt;&lt;p&gt;“This new robot platform is a major result from our group and leads to many exciting directions. For example, incorporating sensors, batteries, and computing capabilities on this robot will be a central focus in the next three to five years,” Chen says.&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the U.S. National Science Foundation and a Mathworks Fellowship.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202501/MIT-MicroAcrobatics-01-press.jpg?itok=BO3btyRe" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Weighing less than a paperclip, the robotic insect can fly significantly faster than similar bots while completing acrobatic maneuvers like double aerial flips. It can even precisely track a trajectory that spells M-I-T.]]></media:description>
              <media:credit>Credit: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/drones">Drones</category>
      <category domain="https://news.mit.edu/topic/electronics">Electronics</category>
      <category domain="https://news.mit.edu/topic/bioinspiration">Bioinspiration</category>
      <category domain="https://news.mit.edu/topic/research-laboratory-electronics-1">Research Laboratory of Electronics</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>How humans continuously adapt while walking stably</title>
  <link>https://news.mit.edu/2024/how-humans-continuously-adapt-while-walking-stably-1218</link>
  <description><![CDATA[Research could help improve motor rehabilitation programs and assistive robot control.<br>
]]></description>
  <pubDate>Wed, 18 Dec 2024 10:20:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/how-humans-continuously-adapt-while-walking-stably-1218</guid>
        <dc:creator>Department of Brain and Cognitive Sciences</dc:creator>
  <content:encoded>&lt;p&gt;Researchers have developed a model that explains how humans adapt continuously during complex tasks, like walking, while remaining stable.&lt;/p&gt;&lt;p&gt;The findings were detailed in a recent &lt;a href="https://www.nature.com/articles/s41467-024-53416-w?utm_source=rct_congratemailt&amp;amp;utm_medium=email&amp;amp;utm_campaign=oa_20241103&amp;amp;utm_content=10.1038/s41467-024-53416-w" target="_blank" rel="noopener noreferrer" tabindex="-1"&gt;paper published in the journal &lt;em&gt;Nature Communications&lt;/em&gt;&lt;/a&gt; authored by Nidhi Seethapathi, an assistant professor in MIT’s Department of Brain and Cognitive Sciences; Barrett C. Clark, a robotics software engineer at Bright Minds Inc.; and Manoj Srinivasan, an associate professor in the Department of Mechanical and Aerospace Engineering at Ohio State University.&lt;/p&gt;&lt;p&gt;In episodic tasks, like reaching for an object, errors during one episode do not affect the next episode. In tasks like locomotion, errors can have a cascade of short-term and long-term consequences to stability unless they are controlled. This makes the challenge of adapting locomotion in a new environment &amp;nbsp;more complex.&lt;/p&gt;&lt;p&gt;"Much of our prior theoretical understanding of adaptation has been limited to episodic tasks, such as reaching for an object in a novel environment," Seethapathi says. "This new theoretical model captures adaptation phenomena in continuous long-horizon tasks in multiple locomotor settings."&lt;/p&gt;&lt;p&gt;To build the model, the researchers identified general principles of locomotor adaptation across a variety of task settings, and &amp;nbsp;developed a unified modular and hierarchical model of locomotor adaptation, with each component having its own unique mathematical structure.&lt;/p&gt;&lt;p&gt;The resulting model successfully encapsulates how humans adapt their walking in novel settings such as on a split-belt treadmill with each foot at a different speed, wearing asymmetric leg weights, and wearing &amp;nbsp;an exoskeleton. The authors report that the model successfully reproduced human locomotor adaptation phenomena across novel settings in 10 prior studies and correctly predicted the adaptation behavior observed in two new experiments conducted as part of the study.&lt;/p&gt;&lt;p&gt;The model has potential applications in sensorimotor learning, rehabilitation, and wearable robotics.&lt;/p&gt;&lt;p&gt;"Having a model that can predict how a person will adapt to a new environment has immense utility for engineering better rehabilitation paradigms and wearable robot control," Seethapathi says. "You can think of a wearable robot itself as a new environment for the person to move in, and our model can be used to predict how a person will adapt for different robot settings. Understanding such human-robot adaptation is currently an experimentally intensive process, and our model &amp;nbsp;could help speed up the process by narrowing the search space."&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202412/walking.jpg?itok=Zany5XPV" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[A new model has potential applications in sensorimotor learning, rehabilitation, and wearable robotics. ]]></media:description>
          </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/brain-cognitive">Brain and cognitive sciences</category>
      <category domain="https://news.mit.edu/topic/mcgovern-institute-0">McGovern Institute</category>
      <category domain="https://news.mit.edu/topic/neuroscience">Neuroscience</category>
      <category domain="https://news.mit.edu/topic/wearables">Wearables</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Teaching a robot its limits, to complete open-ended tasks safely</title>
  <link>https://news.mit.edu/2024/teaching-robot-its-limits-complete-open-ended-tasks-safely-1212</link>
  <description><![CDATA[The “PRoC3S” method helps an LLM create a viable action plan by testing each step in a simulation. This strategy could eventually aid in-home robots to complete more ambiguous chore requests.]]></description>
  <pubDate>Thu, 12 Dec 2024 17:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/teaching-robot-its-limits-complete-open-ended-tasks-safely-1212</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-8a4c5d2f-7fff-0f2d-8a04-5aa86fc148ee"&gt;If someone advises you to “know your limits,” they’re likely suggesting you do things like exercise in moderation. To a robot, though, the motto represents learning constraints, or limitations of a specific task within the machine’s environment, to do chores safely and correctly.&lt;/p&gt;&lt;p dir="ltr"&gt;For instance, imagine asking a robot to clean your kitchen when it doesn’t understand the physics of its surroundings. How can the machine generate a practical multistep plan to ensure the room is spotless? Large language models (LLMs) can get them close, but if the model is only trained on text, it’s likely to miss out on key specifics about the robot’s physical constraints, like how far it can reach or whether there are nearby obstacles to avoid. Stick to LLMs alone, and you’re likely to end up cleaning pasta stains out of your floorboards.&lt;/p&gt;&lt;p dir="ltr"&gt;To guide robots in executing these open-ended tasks, researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) used vision models to see what’s near the machine and model its constraints. The team’s strategy involves an LLM sketching up a plan that’s checked in a simulator to ensure it’s safe and realistic. If that sequence of actions is infeasible, the language model will generate a new plan, until it arrives at one that the robot can execute.&lt;/p&gt;&lt;p dir="ltr"&gt;This trial-and-error method, which the researchers call “Planning for Robots via Code for Continuous Constraint Satisfaction” (PRoC3S), tests long-horizon plans to ensure they satisfy all constraints, and enables a robot to perform such diverse tasks as writing individual letters, drawing a star, and sorting and placing blocks in different positions. In the future, PRoC3S could help robots complete more intricate chores in dynamic environments like houses, where they may be prompted to do a general chore composed of many steps (like “make me breakfast”).&lt;/p&gt;&lt;p dir="ltr"&gt;“LLMs and classical robotics systems like task and motion planners can’t execute these kinds of tasks on their own, but together, their synergy makes open-ended problem-solving possible,” says PhD student Nishanth Kumar SM ’24, co-lead author of a new paper about PRoC3S. “We’re creating a simulation on-the-fly of what’s around the robot and trying out many possible action plans. Vision models help us create a very realistic digital world that enables the robot to reason about feasible actions for each step of a long-horizon plan.”&lt;/p&gt;&lt;p dir="ltr"&gt;The team’s work was presented this past month in a paper shown at the Conference on Robot Learning (CoRL) in Munich, Germany.&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers’ method uses an LLM pre-trained on text from across the internet. Before asking PRoC3S to do a task, the team provided their language model with a sample task (like drawing a square) that’s related to the target one (drawing a star). The sample task includes a description of the activity, a long-horizon plan, and relevant details about the robot’s environment.&lt;/p&gt;&lt;p dir="ltr"&gt;But how did these plans fare in practice? In simulations, PRoC3S successfully drew stars and letters eight out of 10 times each. It also could stack digital blocks in pyramids and lines, and place items with accuracy, like fruits on a plate. Across each of these digital demos, the CSAIL method completed the requested task more consistently than comparable approaches like&amp;nbsp;&lt;a href="https://arxiv.org/pdf/2403.11552"&gt;“LLM3”&lt;/a&gt; and&amp;nbsp;&lt;a href="https://arxiv.org/pdf/2209.07753"&gt;“Code as Policies”&lt;/a&gt;.&lt;br&gt;&lt;br&gt;The CSAIL engineers next brought their approach to the real world. Their method developed and executed plans on a robotic arm, teaching it to put blocks in straight lines. PRoC3S also enabled the machine to place blue and red blocks into matching bowls and move all objects near the center of a table.&lt;/p&gt;&lt;p dir="ltr"&gt;Kumar and co-lead author Aidan Curtis SM ’23, who’s also a PhD student working in CSAIL, say these findings indicate how an LLM can develop safer plans that humans can trust to work in practice. The researchers envision a home robot that can be given a more general request (like “bring me some chips”) and reliably figure out the specific steps needed to execute it. PRoC3S could help a robot test out plans in an identical digital environment to find a working course of action — and more importantly, bring you a tasty snack.&lt;/p&gt;&lt;p dir="ltr"&gt;For future work, the researchers aim to improve results using a more advanced physics simulator and to expand to more elaborate longer-horizon tasks via more scalable data-search techniques. Moreover, they plan to apply PRoC3S to mobile robots such as a quadruped for tasks that include walking and scanning surroundings.&lt;/p&gt;&lt;p dir="ltr"&gt;“Using foundation models like ChatGPT to control robot actions can lead to unsafe or incorrect behaviors due to hallucinations,” says The AI Institute researcher Eric Rosen, who isn’t involved in the research. “PRoC3S tackles this issue by leveraging foundation models for high-level task guidance, while employing AI techniques that explicitly reason about the world to ensure verifiably safe and correct actions. This combination of planning-based and data-driven approaches may be key to developing robots capable of understanding and reliably performing a broader range of tasks than currently possible.”&lt;/p&gt;&lt;p dir="ltr"&gt;Kumar and Curtis’ co-authors are also CSAIL affiliates: MIT undergraduate researcher Jing Cao and MIT Department of Electrical Engineering and Computer Science professors Leslie Pack Kaelbling and Tomás Lozano-Pérez. Their work was supported, in part, by the National Science Foundation, the Air Force Office of Scientific Research, the Office of Naval Research, the Army Research Office, MIT Quest for Intelligence, and The AI Institute.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202411/TestPRoC3S_0.jpg?itok=jmbmbHcq" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[PhD students Aidan Curtis (left) and Nishanth Kumar. To help robots execute open-ended tasks safely, the researchers used vision models to see what’s near the machine and model its constraints. Their “PRoC3S” strategy has an LLM sketch up an action plan that’s checked in a simulator to ensure it will work in the real world.]]></media:description>
              <media:credit> Mike Grimmett/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/quest-intelligence">Quest for Intelligence</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/natural-language-processing">Natural language processing</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
    </item>
<item>
  <title>Daniela Rus wins John Scott Award</title>
  <link>https://news.mit.edu/2024/daniela-rus-wins-john-scott-award-1209</link>
  <description><![CDATA[MIT CSAIL director and EECS professor named a co-recipient of the honor for her robotics research, which has expanded our understanding of what a robot can be.]]></description>
  <pubDate>Mon, 09 Dec 2024 17:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/daniela-rus-wins-john-scott-award-1209</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p&gt;Daniela Rus, director of MIT's Computer Science and Artificial Intelligence Laboratory and MIT professor of electrical engineering and computer science, was recently named a co-recipient of the 2024 John Scott Award by the board of directors of City Trusts. This prestigious honor, steeped in historical significance, celebrates scientific innovation at the very location where American independence was signed in Philadelphia, a testament to the enduring connection between scientific progress and human potential.&lt;/p&gt;&lt;p&gt;The Scott Award, the first science award in America established to honor Benjamin Franklin's scientific legacy, recognized Rus alongside professors Takeo Kanade from Carnegie Mellon University and Vijay Kumar from the University of Pennsylvania. The award acknowledged her robotics research that has fundamentally changed our understanding of the field, expanding the very notion of what a robot can be.&lt;/p&gt;&lt;p&gt;Rus' work extends beyond traditional robotics, focusing on developing machine intelligence that makes sense of the physical world through explainable algorithms. Her research represents a profound vision: creating robots as helpful tools that extend human strength, precision, and reach —&amp;nbsp;as collaborative partners that can solve real-world challenges.&lt;/p&gt;&lt;p&gt;In her speech, Rus reflected on her time as a graduate student, where she mused that the potential for intelligent machines lies in the synergy between the body and brain. “A robot's capabilities are defined by its physical body and the intelligence that controls it. Over the past decades, I've dedicated my research to developing both the mechanical and cognitive systems of robots, working alongside brilliant students, collaborators, and friends who share this transformative vision,” she said.&lt;/p&gt;&lt;p&gt;Her projects illustrate this commitment. The MiniSurgeon is a tiny ingestible origami robot that can remove dangerous button batteries from children's systems. Soft robotic creatures like fish and sea turtles enable unprecedented aquatic exploration. Modular robotic boats can self-assemble into bridges and platforms, demonstrating adaptive intelligence. More recently, she helped invent liquid neural networks, inspired by the elegantly simple neural system of a tiny worm. By designing algorithms that can operate with as few as 19 neurons, Rus has shown how machines can navigate complex environments with remarkable efficiency.&lt;/p&gt;&lt;p&gt;When asked about her most impactful work, Rus was unequivocal in saying it was not the metal robots, but the students and researchers she was able to support and mentor. This statement encapsulates her deeper mission: not just advancing technology, but nurturing the next generation of minds.&lt;/p&gt;&lt;p&gt;“The hardest problems in AI and robotics,” she says, “require long-term thinking and dedication. A robot must not only perceive the world but understand it, decide how to act, and navigate interactions with people and other robots.”&lt;/p&gt;&lt;p&gt;The John Scott Award celebrates not just individual achievement, but also where scientific exploration meets compassionate innovation —&amp;nbsp;as evidenced by previous luminary winners including Thomas Edison, Nikola Tesla, the Wright brothers, Marie Curie, Guglielmo Marconi, and 20 additional Nobel Prize winners.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202412/Daniela-Rus-JSA.jpg?itok=5vP9itgS" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[The Scott Award, the first science award in America established to honor Benjamin Franklin's scientific legacy, recognized MIT Professor Daniela Rus alongside Takeo Kanade from Carnegie Mellon University and Vijay Kumar from the University of Pennsylvania.]]></media:description>
              <media:credit>Photo: Charity Payne/University of Pennsylvania.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/awards">Awards, honors and fellowships</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Creating innovative health solutions for individuals and populations</title>
  <link>https://news.mit.edu/2024/physician-engineer-giovanni-traverso-creates-innovative-health-solutions-1127</link>
  <description><![CDATA[Physician and engineer Giovanni Traverso found an early passion for molecular genetics, leading to an interdisciplinary career helping others.]]></description>
  <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/physician-engineer-giovanni-traverso-creates-innovative-health-solutions-1127</guid>
        <dc:creator>Anne Wilson | Department of Mechanical Engineering</dc:creator>
  <content:encoded>&lt;p&gt;The factors impacting successful patient care are many and varied. Early diagnosis, proper adherence to prescription medication schedules, and effective monitoring and management of chronic disease, for example, all contribute to better outcomes. However, each of these factors can be hindered by outside influences — medication doesn’t work as well if it isn’t taken as prescribed, and disease can be missed or misdiagnosed in early stages if symptoms are mild or not present.&lt;/p&gt;&lt;p&gt;Giovanni Traverso, the Karl Van Tassel Career Development Professor, an associate professor of mechanical engineering, and a gastroenterologist in the Division of Gastroenterology, Brigham and Women’s Hospital (BWH), is working on a variety of innovative solutions to improve patient care. As a physician and an engineer, he brings a unique perspective.&lt;/p&gt;&lt;p&gt;“Bringing those two domains together is what really can help transform and accelerate our capacity to develop new biomedical devices or new therapies for a range of conditions,” he says. “As physicians, we're extremely fortunate to be able to help individuals. As scientists and engineers, not only can we help individuals … we can help populations.”&lt;/p&gt;&lt;p&gt;Traverso found a passion for this work early in life. His family lived in his father’s native Peru through much of his childhood, but left in the late 1980s at the height of the nation’s political instability, emigrating to Canada, where he began high school.&lt;/p&gt;&lt;p&gt;“In high school, I had the incredible opportunity to actually spend time in a lab,” he says. “I really fell in love with molecular genetics. I loved the lab environment and that ability to investigate a very specific problem, with the hopes that those developments would eventually help people.”&lt;/p&gt;&lt;p&gt;He started medical school immediately after high school, attending the University of Cambridge, but paused his medical training to pursue a PhD in medical sciences at Johns Hopkins University before returning to Cambridge. After completing medical school, he completed internal medicine residency at BWH and his gastroenterology fellowship training at Massachusetts General Hospital, both at Harvard Medical School. For his postdoctoral research, he transitioned to the fields of chemical and biomedical engineering in the laboratory of Professor Robert Langer.&lt;/p&gt;&lt;p&gt;Traverso’s research interests today include biomedical device development, ingestible and implantable robotics, and drug delivery for optimal drug adherence. His academic home at MIT is in the Department of Mechanical Engineering, but his work integrates multiple domains, including mechanical engineering, electrical engineering, material science, and synthetic biology.&lt;/p&gt;&lt;p&gt;“The mechanical engineering department is a tremendous place to engage with students, as well as faculty, towards the development of the next generation of medical devices,” he says. “At the core of many of those medical devices are fundamental mechanical principles.”&lt;/p&gt;&lt;p&gt;Traverso’s team in the &lt;a href="http://www.l4te.org/"&gt;Laboratory for Translational Engineering&lt;/a&gt; is developing pioneering biomedical devices such as drug delivery systems to enable safe, efficient delivery of therapeutics, and novel diagnostic tests to support early detection of diseases.&lt;/p&gt;&lt;p&gt;The heart of his work, he says, is “about trying to help others. Patients, of course, but also students, to help them see the arc of bench-to-bedside and help stimulate their interest in careers applying engineering to help improve human health.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202411/MIT-Gio-Traverso_press.jpg?itok=3QM7Koxt" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Giovanni Traverso is the Karl Van Tassel Career Development Professor and an associate professor of mechanical engineering, and a gastroenterologist in the Division of Gastroenterology, Brigham and Women’s Hospital.]]></media:description>
              <media:credit>Photo: John Freidah/MechE</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/health">Health sciences and technology</category>
      <category domain="https://news.mit.edu/topic/medicine">Medicine</category>
      <category domain="https://news.mit.edu/topic/medical-devices">Medical devices</category>
      <category domain="https://news.mit.edu/topic/drug-delivery">Drug delivery</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/profile">Profile</category>
    </item>
<item>
  <title>Can robots learn from machine dreams?</title>
  <link>https://news.mit.edu/2024/can-robots-learn-machine-dreams-1119</link>
  <description><![CDATA[MIT CSAIL researchers used AI-generated images to train a robot dog in parkour, without real-world data. Their LucidSim system demonstrates generative AI's potential for creating robotics training data.]]></description>
  <pubDate>Tue, 19 Nov 2024 14:50:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/can-robots-learn-machine-dreams-1119</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-2e9b7733-7fff-8d2e-4575-6f74b08de9bd"&gt;For roboticists, one challenge towers above all others: generalization — the ability to create machines that can adapt to any environment or condition. Since the 1970s, the field has evolved from writing sophisticated programs to using deep learning, teaching robots to learn directly from human behavior. But a critical bottleneck remains: data quality. To improve, robots need to encounter scenarios that push the boundaries of their capabilities, operating at the edge of their mastery. This process traditionally requires human oversight, with operators carefully challenging robots to expand their abilities. As robots become more sophisticated, this hands-on approach hits a scaling problem: the demand for high-quality training data far outpaces humans’ ability to provide it.&lt;/p&gt;&lt;p dir="ltr"&gt;Now, a team of MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers has developed a novel approach to robot training that could significantly accelerate the deployment of adaptable, intelligent machines in real-world environments. The new system, called “&lt;a href="https://arxiv.org/abs/2411.00083" target="_blank"&gt;LucidSim&lt;/a&gt;,” uses recent advances in generative AI and physics simulators to create diverse and realistic virtual training environments, helping robots achieve expert-level performance in difficult tasks without any real-world data.&lt;/p&gt;&lt;p dir="ltr"&gt;LucidSim combines physics simulation with generative AI models, addressing one of the most persistent challenges in robotics: transferring skills learned in simulation to the real world. “A fundamental challenge in robot learning has long been the ‘sim-to-real gap’ — the disparity between simulated training environments and the complex, unpredictable real world,” says MIT CSAIL postdoc Ge Yang, a lead researcher on LucidSim. “Previous approaches often relied on depth sensors, which simplified the problem but missed crucial real-world complexities.”&lt;/p&gt;&lt;p dir="ltr"&gt;The multipronged system is a blend of different technologies. At its core, LucidSim uses large language models to generate various structured descriptions of environments. These descriptions are then transformed into images using generative models. To ensure that these images reflect real-world physics, an underlying physics simulator is used to guide the generation process.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The birth of an idea: From burritos to breakthroughs&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The inspiration for LucidSim came from an unexpected place: a conversation outside Beantown Taqueria in Cambridge, Massachusetts. ​​“We wanted to teach vision-equipped robots how to improve using human feedback.&amp;nbsp;But then, we realized we didn’t have a pure vision-based policy to begin with,” says Alan Yu, an undergraduate student in electrical engineering and computer science (EECS) at MIT and co-lead author on LucidSim. “We kept talking about it as we walked down the street, and then we stopped outside the taqueria for about half-an-hour. That’s where we had our moment.”&lt;/p&gt;&lt;p dir="ltr"&gt;To cook up their data, the team generated realistic images by extracting depth maps, which provide geometric information, and semantic masks, which label different parts of an image, from the simulated scene. They quickly realized, however, that with tight control on the composition of the image content, the model would produce similar images that weren’t different from each other using the same prompt. So, they devised a way to source diverse text prompts from ChatGPT.&lt;/p&gt;&lt;p dir="ltr"&gt;This approach, however, only resulted in a single image. To make short, coherent videos that serve as little “experiences” for the robot, the scientists hacked together some image magic into another novel technique the team created, called “Dreams In Motion.” The system computes the movements of each pixel between frames, to warp a single generated image into a short, multi-frame video. Dreams In Motion does this by considering the 3D geometry of the scene and the relative changes in the robot’s perspective.&lt;/p&gt;&lt;p dir="ltr"&gt;“We outperform domain randomization, a method developed in 2017 that applies random colors and patterns to objects in the environment, which is still considered the go-to method these days,” says Yu.&amp;nbsp;“While this technique generates diverse data, it lacks realism. LucidSim addresses both diversity and realism problems. It’s exciting that even without seeing the real world during training, the robot can recognize and navigate obstacles in real environments.”&lt;/p&gt;&lt;p dir="ltr"&gt;The team is particularly excited about the potential of applying LucidSim to domains outside quadruped locomotion and parkour, their main test bed. One example is mobile manipulation, where a mobile robot is tasked to handle objects in an open area; also, color perception is critical. “Today, these robots still learn from real-world demonstrations,” says Yang. “Although collecting demonstrations is easy, scaling a real-world robot teleoperation setup to thousands of skills is challenging because a human has to physically set up each scene. We hope to make this easier, thus qualitatively more scalable, by moving data collection into a virtual environment.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Who's the real expert?&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The team put LucidSim to the test against an alternative, where an expert teacher demonstrates the skill for the robot to learn from. The results were surprising: Robots trained by the expert struggled, succeeding only 15 percent of the time — and even quadrupling the amount of expert training data barely moved the needle. But when robots collected their own training data through LucidSim, the story changed dramatically. Just doubling the dataset size catapulted success rates to 88 percent. “And giving our robot more data monotonically improves its performance — eventually, the student becomes the expert,” says Yang.&lt;/p&gt;&lt;p dir="ltr"&gt;“One of the main challenges in sim-to-real transfer for robotics is achieving visual realism in simulated environments,” says Stanford University assistant professor of electrical engineering Shuran Song, who wasn’t involved in the research. “The LucidSim framework provides an elegant solution by using generative models to create diverse, highly realistic visual data for any simulation. This work could significantly accelerate the deployment of robots trained in virtual environments to real-world tasks.”&lt;/p&gt;&lt;p dir="ltr"&gt;From the streets of Cambridge to the cutting edge of robotics research, LucidSim is paving the way toward a new generation of intelligent, adaptable machines — ones that learn to navigate our complex world without ever setting foot in it.&lt;/p&gt;&lt;p dir="ltr"&gt;Yu and Yang wrote the paper with four fellow CSAIL affiliates: Ran Choi, an MIT postdoc in mechanical engineering; Yajvan Ravan, an MIT undergraduate in EECS; John Leonard, the Samuel C. Collins Professor of Mechanical and Ocean Engineering in the MIT Department of Mechanical Engineering; and Phillip Isola, an MIT associate professor in EECS. Their work was supported, in part, by a Packard Fellowship, a Sloan Research Fellowship, the Office of Naval Research, Singapore’s Defence Science and Technology Agency, Amazon, MIT Lincoln Laboratory, and the National Science Foundation Institute for Artificial Intelligence and Fundamental Interactions. The researchers presented their work at the Conference on Robot Learning (CoRL) in early November.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202411/mit-csail-LucidSim.jpg?itok=OTITHmF5" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT CSAIL researchers Alan Yu (right), an undergraduate in electrical engineering and computer science (EECS); Phillip Isola (left), associate professor of EECS; and Ge Yang (center), a postdoc, developed an AI-powered simulator that generates unlimited, diverse, and realistic training data for robots. Robots trained in this virtual environment can seamlessly transfer their skills to the real world, performing at expert levels without additional fine-tuning. ]]></media:description>
              <media:credit>Photo: Michael Grimmett/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>A faster, better way to train general-purpose robots</title>
  <link>https://news.mit.edu/2024/training-general-purpose-robots-faster-better-1028</link>
  <description><![CDATA[Inspired by large language models, researchers develop a training technique that pools diverse data to teach robots new skills.  ]]></description>
  <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/training-general-purpose-robots-faster-better-1028</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;In the classic cartoon “The Jetsons,” Rosie the robotic maid seamlessly switches from vacuuming the house to cooking dinner to taking out the trash. But in real life, training a general-purpose robot remains a major challenge.&lt;/p&gt;&lt;p&gt;Typically, engineers collect data that are specific to a certain robot and task, which they use to train the robot in a controlled environment. However, gathering these data is costly and time-consuming, and the robot will likely struggle to adapt to environments or tasks it hasn’t seen before.&lt;/p&gt;&lt;p&gt;To train better general-purpose robots, MIT researchers developed a versatile technique that combines a huge amount of heterogeneous&amp;nbsp;data from many of sources into one system that can teach any robot a wide range of tasks.&lt;/p&gt;&lt;p&gt;Their method involves aligning data from varied domains, like simulations and real robots, and multiple modalities, including vision sensors and robotic arm position encoders, into a shared “language” that a generative AI model can process.&lt;/p&gt;&lt;p&gt;By combining such an enormous amount of data, this approach can be used to train a robot to perform a variety of tasks without the need to start training it from scratch each time.&lt;/p&gt;&lt;p&gt;This method could be faster and less expensive than traditional techniques because it requires far fewer task-specific data. In addition, it outperformed training from scratch by more than 20 percent in simulation and real-world experiments.&lt;/p&gt;&lt;p&gt;“In robotics, people often claim that we don’t have enough training data. But in my view, another big&amp;nbsp;problem is that the data come from so many different domains, modalities, and robot hardware. Our work shows how you’d be able to train a robot with all of them put together,” says Lirui Wang, an electrical engineering and computer science (EECS) graduate student and lead author of a &lt;a href="https://arxiv.org/pdf/2409.20537" target="_blank"&gt;paper on this technique&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Wang’s co-authors include fellow EECS graduate student Jialiang Zhao; Xinlei Chen, a research scientist at Meta; and senior author Kaiming He, an associate professor in EECS and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented&amp;nbsp;at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Inspired by LLMs&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A robotic “policy” takes in sensor observations, like camera images or proprioceptive measurements that track the speed and position a robotic arm, and then tells a robot how and where to move.&lt;/p&gt;&lt;p&gt;Policies are typically trained using imitation learning, meaning a human demonstrates actions or teleoperates a robot to generate data, which are fed into an AI model that learns the policy. Because this method uses a small amount of task-specific data, robots often fail when their environment or task changes.&lt;/p&gt;&lt;p&gt;To develop a better approach, Wang and his collaborators drew inspiration from large language models like GPT-4.&lt;/p&gt;&lt;p&gt;These models are pretrained using an enormous amount of diverse language data and then fine-tuned by feeding them a small amount of task-specific data. Pretraining on so much data helps the models adapt to perform well on a variety of tasks.&lt;/p&gt;&lt;p&gt;“In the language domain, the data are all just sentences. In robotics, given all the heterogeneity in the data, if you want to pretrain in a similar manner, we need a different architecture,” he says.&lt;/p&gt;&lt;p&gt;Robotic data take many forms, from camera images to language instructions to depth maps. At the same time, each robot is mechanically unique, with a different number and orientation of arms, grippers, and sensors. Plus, the environments where data are collected vary widely.&lt;/p&gt;&lt;p&gt;The MIT researchers developed a new architecture called Heterogeneous Pretrained Transformers (HPT) that unifies data from these varied modalities and domains.&lt;/p&gt;&lt;p&gt;They put a machine-learning model known as a transformer into the middle of their architecture, which processes vision and proprioception inputs. A transformer is the same type of model that forms the backbone of large language models.&lt;/p&gt;&lt;p&gt;The researchers align data from vision and proprioception into the same type of input, called a token, which the transformer can process. Each input is represented with the same fixed number of tokens.&lt;/p&gt;&lt;p&gt;Then the transformer maps all inputs into one shared space, growing into a huge, pretrained model as it processes and learns from more data. The larger the transformer becomes, the better it will perform.&lt;/p&gt;&lt;p&gt;A user only needs to feed HPT a small amount of data on their robot’s design, setup, and the task they want it to perform. Then HPT transfers the knowledge the transformer grained during pretraining to learn the new task.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Enabling dexterous motions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;One of the biggest challenges of developing HPT was building the massive dataset to pretrain the transformer, which included 52 datasets with more than 200,000 robot trajectories in four categories, including human demo videos and simulation.&lt;/p&gt;&lt;p&gt;The researchers also needed to develop an efficient way to turn raw proprioception signals from an array of sensors into data the transformer could handle.&lt;/p&gt;&lt;p&gt;“Proprioception is key to enable a lot of dexterous motions. Because the number of tokens is in our architecture always the same, we place the same importance on proprioception and vision,” Wang explains.&lt;/p&gt;&lt;p&gt;When they tested HPT, it improved robot performance by more than 20 percent on simulation and real-world tasks, compared with training from scratch each time. Even when the task was very different from the pretraining data, HPT still improved performance.&lt;/p&gt;&lt;p&gt;“This paper provides a novel approach to training a single policy across multiple robot embodiments. This enables training across diverse datasets, enabling robot learning methods to significantly scale up the size of datasets that they can train on.&amp;nbsp;It also allows the model to quickly adapt to new&amp;nbsp;robot embodiments, which is important as new robot designs are continuously being produced,” says David Held, associate professor at the Carnegie Mellon University Robotics Institute, who was not involved with this work.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to study how data diversity could boost the performance of HPT. They also want to enhance HPT so it can process unlabeled data like GPT-4 and other large language models.&lt;/p&gt;&lt;p&gt;“Our dream is to have a universal robot brain that you could download and use for your robot without any training at all. While we are just in the early stages, we are going to keep pushing hard and hope scaling leads to a breakthrough in robotic policies, like it did with large language models,” he says.&lt;/p&gt;&lt;p&gt;This work was funded, in part, by the Amazon Greater Boston Tech Initiative and the Toyota Research Institute.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202410/MIT-HeteroGenRobot-01-press.jpg?itok=lQhdjhmT" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Researchers filmed multiple instances of a robotic arm feeding co-author Jialiang Zhao's adorable dog, Momo. The videos were included in datasets to train the robot.]]></media:description>
              <media:credit>Image: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Combining next-token prediction and video diffusion in computer vision and robotics</title>
  <link>https://news.mit.edu/2024/combining-next-token-prediction-video-diffusion-computer-vision-robotics-1016</link>
  <description><![CDATA[A new method can train a neural network to sort corrupted data while anticipating next steps. It can make flexible plans for robots, generate high-quality video, and help AI agents navigate digital environments. ]]></description>
  <pubDate>Wed, 16 Oct 2024 16:10:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/combining-next-token-prediction-video-diffusion-computer-vision-robotics-1016</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-3e5396e8-7fff-84d6-3a6d-905774137181"&gt;In the current AI zeitgeist, sequence models have skyrocketed in popularity for their ability to analyze data and predict what to do next. For instance, you’ve likely used next-token prediction models like ChatGPT, which anticipate each word (token) in a sequence to form answers to users’ queries. There are also full-sequence diffusion models like Sora, which convert words into dazzling, realistic visuals by successively “denoising” an entire video sequence.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-3e5396e8-7fff-84d6-3a6d-905774137181"&gt;Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have proposed a simple change to the diffusion training scheme that makes this sequence denoising considerably more flexible.&lt;/p&gt;&lt;p dir="ltr"&gt;When applied to fields like computer vision and robotics, the next-token and full-sequence diffusion models have capability trade-offs. Next-token models can spit out sequences that vary in length. However, they make these generations while being unaware of desirable states in the far future — such as steering its sequence generation toward a certain goal 10 tokens away — and thus require additional mechanisms for long-horizon (long-term) planning. Diffusion models can perform such future-conditioned sampling, but lack the ability of next-token models to generate variable-length sequences.&lt;/p&gt;&lt;p dir="ltr"&gt;Researchers from CSAIL want to combine the strengths of both models, so they created a sequence model training technique called “Diffusion Forcing.” The name comes from “Teacher Forcing,” the conventional training scheme that breaks down full sequence generation into the smaller, easier steps of next-token generation (much like a good teacher simplifying a complex concept).&lt;/p&gt;&lt;p dir="ltr"&gt;Diffusion Forcing found common ground between diffusion models and teacher forcing: They both use training schemes that involve predicting masked (noisy) tokens from unmasked ones. In the case of diffusion models, they gradually add noise to data, which can be viewed as fractional masking. The MIT researchers’ Diffusion Forcing method trains neural networks to cleanse a collection of tokens, removing different amounts of noise within each one while simultaneously predicting the next few tokens. The result: a flexible, reliable sequence model that resulted in higher-quality artificial videos and more precise decision-making for robots and AI agents.&lt;/p&gt;&lt;p dir="ltr"&gt;By sorting through noisy data and reliably predicting the next steps in a task, Diffusion Forcing can aid a robot in ignoring visual distractions to complete manipulation tasks. It can also generate stable and consistent video sequences and even guide an AI agent through digital mazes. This method could potentially enable household and factory robots to generalize to new tasks and improve AI-generated entertainment.&lt;/p&gt;&lt;p dir="ltr"&gt;“Sequence models aim to condition on the known past and predict the unknown future, a type of binary masking. However, masking doesn’t need to be binary,” says lead author, MIT electrical engineering and computer science (EECS) PhD student, and CSAIL member Boyuan Chen. “With Diffusion Forcing, we add different levels of noise to each token, effectively serving as a type of fractional masking. At test time, our system can “unmask” a collection of tokens and diffuse a sequence in the near future at a lower noise level. It knows what to trust within its data to overcome out-of-distribution inputs.”&lt;/p&gt;&lt;p dir="ltr"&gt;In several experiments, Diffusion Forcing thrived at ignoring misleading data to execute tasks while anticipating future actions.&lt;/p&gt;&lt;p dir="ltr"&gt;When implemented into a robotic arm, for example, it helped swap two toy fruits across three circular mats, a minimal example of a family of long-horizon tasks that require memories. The researchers trained the robot by controlling it from a distance (or teleoperating it) in virtual reality. The robot is trained to mimic the user’s movements from its camera. Despite starting from random positions and seeing distractions like a shopping bag blocking the markers, it placed the objects into its target spots.&lt;/p&gt;&lt;p dir="ltr"&gt;To generate videos, they trained Diffusion Forcing on “Minecraft” game play and colorful digital environments created within Google’s&amp;nbsp;&lt;a href="https://github.com/google-deepmind/lab"&gt;DeepMind Lab Simulator&lt;/a&gt;. When given a single frame of footage, the method produced more stable, higher-resolution videos than comparable baselines like a Sora-like full-sequence diffusion model and ChatGPT-like next-token models. These approaches created videos that appeared inconsistent, with the latter sometimes failing to generate working video past just 72 frames.&lt;/p&gt;&lt;p dir="ltr"&gt;Diffusion Forcing not only generates fancy videos, but can also serve as a motion planner that steers toward desired outcomes or rewards. Thanks to its flexibility, Diffusion Forcing can uniquely generate plans with varying horizon, perform tree search, and incorporate the intuition that the distant future is more uncertain than the near future. In the task of solving a 2D maze, Diffusion Forcing outperformed six baselines by generating faster plans leading to the goal location, indicating that it could be an effective planner for robots in the future.&lt;/p&gt;&lt;p dir="ltr"&gt;Across each demo, Diffusion Forcing acted as a full sequence model, a next-token prediction model, or both. According to Chen, this versatile approach could potentially serve as a powerful backbone for a “world model,” an AI system that can simulate the dynamics of the world by training on billions of internet videos. This would allow robots to perform novel tasks by imagining what they need to do based on their surroundings. For example, if you asked a robot to open a door without being trained on how to do it, the model could produce a video that’ll show the machine how to do it.&lt;/p&gt;&lt;p dir="ltr"&gt;The team is currently looking to scale up their method to larger datasets and the latest transformer models to improve performance. They intend to broaden their work to build a ChatGPT-like robot brain that helps robots perform tasks in new environments without human demonstration.&lt;/p&gt;&lt;p dir="ltr"&gt;“With Diffusion Forcing, we are taking a step to bringing video generation and robotics closer together,” says senior author Vincent Sitzmann, MIT assistant professor and member of CSAIL, where he leads the Scene Representation group. “In the end, we hope that we can use all the knowledge stored in videos on the internet to enable robots to help in everyday life. Many more exciting research challenges remain, like how robots can learn to imitate humans by watching them even when their own bodies are so different from our own!”&lt;/p&gt;&lt;p dir="ltr"&gt;Chen and Sitzmann wrote the paper alongside recent MIT visiting researcher Diego Martí Monsó, and CSAIL affiliates: Yilun Du, a EECS graduate student; Max Simchowitz, former postdoc and incoming Carnegie Mellon University assistant professor; and Russ Tedrake, the Toyota Professor of EECS, Aeronautics and Astronautics, and Mechanical Engineering at MIT, vice president of robotics research at the Toyota Research Institute, and CSAIL member. Their work was supported, in part, by the U.S. National Science Foundation, the Singapore Defence Science and Technology Agency, Intelligence Advanced Research Projects Activity via the U.S. Department of the Interior, and the Amazon Science Hub. They will present their research at NeurIPS in December.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202410/MIT-CSAIL-diffusion-forcing.jpg?itok=rYThw1xq" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[The “Diffusion Forcing” method can sort through noisy data and reliably predict the next steps in a task, helping a robot complete manipulation tasks, for example. In one experiment, it helped a robotic arm rearrange toy fruits into target spots on circular mats despite starting from random positions and visual distractions.]]></media:description>
              <media:credit>Photo: Mike Grimmett/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/video">Video</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>How AI is improving simulations with smarter sampling techniques</title>
  <link>https://news.mit.edu/2024/how-ai-improving-simulations-smarter-sampling-techniques-1002</link>
  <description><![CDATA[MIT CSAIL researchers created an AI-powered method for low-discrepancy sampling, which uniformly distributes data points to boost simulation accuracy. ]]></description>
  <pubDate>Wed, 02 Oct 2024 11:50:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/how-ai-improving-simulations-smarter-sampling-techniques-1002</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-45928ee4-7fff-7f21-9652-c072472d3668"&gt;Imagine you’re tasked with sending a team of football players onto a field to assess the condition of the grass (a likely task for them, of course). If you pick their positions randomly, they might cluster together in some areas while completely neglecting others. But if you give them a strategy, like spreading out uniformly across the field, you might get a far more accurate picture of the grass condition.&lt;/p&gt;&lt;p dir="ltr"&gt;Now, imagine needing to spread out not just in two dimensions, but across tens or even hundreds. That's the challenge MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers are getting ahead of. They've developed an AI-driven approach to “low-discrepancy sampling,” a method that improves simulation accuracy by distributing data points more uniformly across space.&lt;/p&gt;&lt;p dir="ltr"&gt;A key novelty lies in using graph neural networks (GNNs), which allow points to “communicate” and self-optimize for better uniformity. Their approach marks a pivotal enhancement for simulations in fields like robotics, finance, and computational science, particularly in handling complex, multidimensional problems critical for accurate simulations and numerical computations.&lt;/p&gt;&lt;p dir="ltr"&gt;“In many problems, the more uniformly you can spread out points, the more accurately you can simulate complex systems,” says T. Konstantin Rusch, lead author of the new paper and MIT CSAIL postdoc. “We've developed a method called Message-Passing Monte Carlo (MPMC) to generate uniformly spaced points, using geometric deep learning techniques. This further allows us to generate points that emphasize dimensions which are particularly important for a problem at hand, a property that is highly important in many applications. The model’s underlying graph neural networks lets the points 'talk' with each other, achieving far better uniformity than previous methods.”&lt;/p&gt;&lt;p dir="ltr"&gt;Their work was &lt;a href="https://www.pnas.org/doi/full/10.1073/pnas.2409913121" target="_blank"&gt;published in the September issue of the &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Take me to Monte Carlo&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The idea of Monte Carlo methods is to learn about a system by simulating it with random sampling. Sampling is the selection of a subset of a population to estimate characteristics of the whole population. Historically, it was already used in the 18th century,&amp;nbsp; when mathematician Pierre-Simon Laplace employed it to estimate the population of France without having to count each individual.&lt;/p&gt;&lt;p dir="ltr"&gt;Low-discrepancy sequences, which are sequences with low discrepancy, i.e., high uniformity, such as Sobol’, Halton, and Niederreiter, have long been the gold standard for quasi-random sampling, which exchanges random sampling with low-discrepancy sampling. They are widely used in fields like computer graphics and computational finance, for everything from pricing options to risk assessment, where uniformly filling spaces with points can lead to more accurate results.&amp;nbsp;&lt;br&gt;&lt;br&gt;The MPMC framework suggested by the team transforms random samples into points with high uniformity. This is done by processing the random samples with a GNN that minimizes a specific discrepancy measure.&lt;/p&gt;&lt;p dir="ltr"&gt;One big challenge of using AI for generating highly uniform points is that the usual way to measure point uniformity is very slow to compute and hard to work with. To solve this, the team switched to a quicker and more flexible uniformity measure called L2-discrepancy. For high-dimensional problems, where this method isn’t enough on its own, they use a novel technique that focuses on important lower-dimensional projections of the points. This way, they can create point sets that are better suited for specific applications.&lt;/p&gt;&lt;p dir="ltr"&gt;The implications extend far beyond academia, the team says. In computational finance, for example, simulations rely heavily on the quality of the sampling points. “With these types of methods, random points are often inefficient, but our GNN-generated low-discrepancy points lead to higher precision,” says Rusch. “For instance, we considered a classical problem from computational finance in 32 dimensions, where our MPMC points beat previous state-of-the-art quasi-random sampling methods by a factor of four to 24.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Robots in Monte Carlo&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;In robotics, path and motion planning often rely on sampling-based algorithms, which guide robots through real-time decision-making processes. The improved uniformity of MPMC could lead to more efficient robotic navigation and real-time adaptations for things like autonomous driving or drone technology. “In fact, in a recent preprint, we demonstrated that our MPMC points achieve a fourfold improvement over previous low-discrepancy methods when applied to real-world robotics motion planning problems,” says Rusch.&lt;/p&gt;&lt;p dir="ltr"&gt;“Traditional low-discrepancy sequences were a major advancement in their time, but the world has become more complex, and the problems we're solving now often exist in 10, 20, or even 100-dimensional spaces,” says Daniela Rus, CSAIL director and MIT professor of electrical engineering and computer science. “We needed something smarter, something that adapts as the dimensionality grows. GNNs are a paradigm shift in how we generate low-discrepancy point sets. Unlike traditional methods, where points are generated independently, GNNs allow points to 'chat' with one another so the network learns to place points in a way that reduces clustering and gaps — common issues with typical approaches.”&lt;/p&gt;&lt;p dir="ltr"&gt;Going forward, the team plans to make MPMC points even more accessible to everyone, addressing the current limitation of training a new GNN for every fixed number of points and dimensions.&lt;/p&gt;&lt;p dir="ltr"&gt;“Much of applied mathematics uses continuously varying quantities, but computation typically allows us to only use a finite number of points,” says Art B. Owen, Stanford University professor of statistics, who wasn’t involved in the research. “The century-plus-old field of discrepancy uses abstract algebra and number theory to define effective sampling points. This paper uses graph neural networks to find input points with low discrepancy compared to a continuous distribution. That approach already comes very close to the best-known low-discrepancy point sets in small problems and is showing great promise for a 32-dimensional integral from computational finance. We can expect this to be the first of many efforts to use neural methods to find good input points for numerical computation.”&lt;/p&gt;&lt;p dir="ltr"&gt;Rusch and Rus wrote the paper with University of Waterloo researcher Nathan Kirk, Oxford University’s DeepMind Professor of AI and former CSAIL affiliate Michael Bronstein, and University of Waterloo Statistics and Actuarial Science Professor Christiane Lemieux. Their research was supported, in part, by the AI2050 program at Schmidt Sciences, Boeing, the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator, the Swiss National Science Foundation, Natural Science and Engineering Research Council of Canada, and an EPSRC Turing AI World-Leading Research Fellowship.&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202409/AI-improving-simulations.jpg?itok=Ma8YMY8F" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Using graph neural networks (GNNs) allows points to “communicate” and self-optimize for better uniformity. Their approach helps optimize point placement to handle complex, multidimensional problems necessary for accurate simulations. ]]></media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Helping robots zero in on the objects that matter</title>
  <link>https://news.mit.edu/2024/helping-robots-focus-on-objects-that-matter-0930</link>
  <description><![CDATA[A new method called Clio enables robots to quickly map a scene and identify the items they need to complete a given set of tasks.]]></description>
  <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/helping-robots-focus-on-objects-that-matter-0930</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Imagine having to straighten up a messy kitchen, starting with a counter littered with sauce packets. If your goal is to wipe the counter clean, you might sweep up the packets as a group. If, however, you wanted to first pick out the mustard packets before throwing the rest away, you would sort more discriminately, by sauce type. And if, among the mustards, you had a hankering for Grey Poupon, finding this specific brand would entail a more careful search.&lt;/p&gt;&lt;p&gt;MIT engineers have developed a method that enables robots to make similarly intuitive, task-relevant decisions.&lt;/p&gt;&lt;p&gt;The team’s new approach, named Clio, enables a robot to identify the parts of a scene that matter, given the tasks at hand. With Clio, a robot takes in a list of tasks described in natural language and, based on those tasks, it then determines the level of granularity required to interpret its surroundings and “remember” only the parts of a scene that are relevant.&lt;/p&gt;&lt;p&gt;In real experiments ranging from a cluttered cubicle to a five-story building on MIT’s campus, the team used Clio to automatically segment a scene at different levels of granularity, based on a set of tasks specified in natural-language prompts such as “move rack of magazines” and “get first aid kit.”&lt;/p&gt;&lt;p&gt;The team also ran Clio in real-time on a quadruped robot. As the robot explored an office building, Clio identified and mapped only those parts of the scene that related to the robot’s tasks (such as retrieving a dog toy while ignoring piles of office supplies), allowing the robot to grasp the objects of interest.&lt;/p&gt;&lt;p&gt;Clio is named after the Greek muse of history, for its ability to identify and remember only the elements that matter for a given task. The researchers envision that Clio would be useful in many situations and environments in which a robot would have to quickly survey and make sense of its surroundings in the context of its given task.&lt;/p&gt;&lt;p&gt;“Search and rescue is the motivating application for this work, but Clio can also power domestic robots and robots working on a factory floor alongside humans,” says Luca Carlone, associate professor in MIT’s Department of Aeronautics and Astronautics (AeroAstro), principal investigator in the Laboratory for Information and Decision Systems (LIDS), and director of the MIT SPARK Laboratory. “It’s really about helping the robot understand the environment and what it has to remember in order to carry out its mission.”&lt;/p&gt;&lt;p&gt;The team details their results in a &lt;a href="https://dspace.mit.edu/handle/1721.1/157072" target="_blank"&gt;study appearing today&lt;/a&gt; in the journal &lt;em&gt;Robotics and Automation Letters&lt;/em&gt;. Carlone’s co-authors include members of the SPARK Lab: Dominic Maggio, Yun Chang, Nathan Hughes, and Lukas Schmid; and members of MIT Lincoln Laboratory: Matthew Trang, Dan Griffith, Carlyn Dougherty, and Eric Cristofalo.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Open fields&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Huge advances in the fields of computer vision and natural language processing have enabled robots to identify objects in their surroundings. But until recently, robots were only able to do so in “closed-set” scenarios, where they are programmed to work in a carefully curated and controlled environment, with a finite number of objects that the robot has been pretrained to recognize.&lt;/p&gt;&lt;p&gt;In recent years, researchers have taken a more “open” approach to enable robots to recognize objects in more realistic settings. In the field of open-set recognition, researchers have leveraged deep-learning tools to build neural networks that can process billions of images from the internet, along with each image’s associated text (such as a friend’s Facebook picture of a dog, captioned “Meet my new puppy!”).&lt;/p&gt;&lt;p&gt;From millions of image-text pairs, a neural network learns from, then identifies, those segments in a scene that are characteristic of certain terms, such as a dog. A robot can then apply that neural network to spot a dog in a totally new scene.&lt;/p&gt;&lt;p&gt;But a challenge still remains as to how to parse a scene in a useful way that is relevant for a particular task.&lt;/p&gt;&lt;p&gt;“Typical methods will pick some arbitrary, fixed level of granularity for determining how to fuse segments of a scene into what you can consider as one ‘object,’” Maggio says. “However, the granularity of what you call an ‘object’ is actually related to what the robot has to do. If that granularity is fixed without considering the tasks, then the robot may end up with a map that isn’t useful for its tasks.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Information bottleneck&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;With Clio, the MIT team aimed to enable robots to interpret their surroundings with a level of granularity that can be automatically tuned to the tasks at hand.&lt;/p&gt;&lt;p&gt;For instance, given a task of moving a stack of books to a shelf, the robot should be able to&amp;nbsp; determine that the entire stack of books is the task-relevant object. Likewise, if the task were to move only the green book from the rest of the stack, the robot should distinguish the green book as a single target object and disregard the rest of the scene — including the other books in the stack.&lt;/p&gt;&lt;p&gt;The team’s approach combines state-of-the-art computer vision and large language models comprising neural networks that make connections among millions of open-source images and semantic text. They also incorporate mapping tools that automatically split an image into many small segments, which can be fed into the neural network to determine if certain segments are semantically similar. The researchers then leverage an idea from classic information theory called the “information bottleneck,” which they use to compress a number of image segments in a way that picks out and stores segments that are semantically most relevant to a given task.&lt;/p&gt;&lt;p&gt;“For example, say there is a pile of books in the scene and my task is just to get the green book. In that case we push all this information about the scene through this bottleneck and end up with a cluster of segments that represent the green book,” Maggio explains. “All the other segments that are not relevant just get grouped in a cluster which we can simply remove. And we’re left with an object at the right granularity that is needed to support my task.”&lt;/p&gt;&lt;p&gt;The researchers demonstrated Clio in different real-world environments.&lt;/p&gt;&lt;p&gt;“What we thought would be a really no-nonsense experiment would be to run Clio in my apartment, where I didn’t do any cleaning beforehand,” Maggio says.&lt;/p&gt;&lt;p&gt;The team drew up a list of natural-language tasks, such as “move pile of clothes” and then applied Clio to images of Maggio’s cluttered apartment. In these cases, Clio was able to quickly segment scenes of the apartment and feed the segments through the Information Bottleneck algorithm to identify those segments that made up the pile of clothes.&lt;/p&gt;&lt;p&gt;They also ran Clio on Boston Dynamic’s quadruped robot, Spot. They gave the robot a list of tasks to complete, and as the robot explored and mapped the inside of an office building, Clio ran in real-time on an on-board computer mounted to Spot, to pick out segments in the mapped scenes that visually relate to the given task. The method generated an overlaying map showing just the target objects, which the robot then used to approach the identified objects and physically complete the task.&lt;/p&gt;&lt;p&gt;“Running Clio in real-time was a big accomplishment for the team,” Maggio says. “A lot of prior work can take several hours to run.”&lt;/p&gt;&lt;p&gt;Going forward, the team plans to adapt Clio to be able to handle higher-level tasks and build upon recent advances in photorealistic visual scene representations.&lt;/p&gt;&lt;p&gt;“We’re still giving Clio tasks that are somewhat specific, like ‘find deck of cards,’” Maggio says. “For search and rescue, you need to give it more high-level tasks, like ‘find survivors,’ or ‘get power back on.’ So, we want to get to a more human-level understanding of how to accomplish more complex tasks.”&lt;/p&gt;&lt;p&gt;This research was supported, in part, by&amp;nbsp;the U.S. National Science Foundation, the Swiss National Science Foundation, MIT Lincoln Laboratory, the U.S. Office of Naval Research, and the U.S.&amp;nbsp;Army Research Lab Distributed and Collaborative Intelligent Systems and Technology Collaborative Research Alliance.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202409/MIT-Scene-Tasks-01-press.jpg?itok=buD_D2mI" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[From left to right: team members Lukas Schmid, Nathan Hughes, Dominic Maggio, Yun Chang, and Luca Carlone. ]]></media:description>
              <media:credit>Credit: Andy Ryan</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/disaster-response">Disaster response</category>
      <category domain="https://news.mit.edu/topic/lincoln-laboratory-0">Lincoln Laboratory</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/natural-disasters">Natural disasters</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>For developing designers, there’s magic in 2.737 (Mechatronics)</title>
  <link>https://news.mit.edu/2024/for-developing-designers-magic-in-mechatronics-0903</link>
  <description><![CDATA[Mechatronics combines electrical and mechanical engineering, but above all else it’s about design. ]]></description>
  <pubDate>Tue, 03 Sep 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/for-developing-designers-magic-in-mechatronics-0903</guid>
        <dc:creator>Anne Wilson | Department of Mechanical Engineering</dc:creator>
  <content:encoded>&lt;p&gt;The field of mechatronics is multidisciplinary and interdisciplinary, occupying the intersection of mechanical systems, electronics, controls, and computer science. Mechatronics engineers work in a variety of industries — from space exploration to semiconductor manufacturing to product design — and specialize in the integrated design and development of intelligent systems. For students wanting to learn mechatronics, it might come as a surprise that one of the most powerful teaching tools available for the subject matter is simply a pen and a piece of paper.&lt;/p&gt;&lt;p&gt;“Students have to be able to work out things on a piece of paper, and make sketches, and write down key calculations in order to be creative,” says MIT professor of mechanical engineering &lt;a href="https://meche.mit.edu/people/faculty/trumper@mit.edu"&gt;David Trumper&lt;/a&gt;, who has been teaching class 2.737 (Mechatronics) since he joined the Institute faculty in the early 1990s. The subject is electrical and mechanical engineering combined, he says, but more than anything else, it’s design.&lt;/p&gt;&lt;p&gt;“If you just do electronics, but have no idea how to make the mechanical parts work, you can’t find really creative solutions. You have to see ways to solve problems across different domains,” says Trumper. “MIT students tend to have seen lots of math and lots of theory. The hands-on part is really critical to build that skill set; with hands-on experiences they’ll be more able to imagine how other things might work when they’re designing them.”&lt;/p&gt;&lt;p&gt;Audrey Cui ’24, now a graduate student in electrical engineering and computer science, confirms that Trumper “really emphasizes being able to do back-of-the-napkin calculations.” This simplicity is by design, and the critical thinking it promotes is essential for budding designers.&lt;/p&gt;&lt;p&gt;“Sitting behind a computer terminal, you’re using some existing tool in the menu system and not thinking creatively,” says Trumper. “To see the trade-offs, and get the clutter out of your thinking, it helps to work with a really simple tool — a piece of paper and, hopefully, multicolored pens to code things — you can design so much more creatively than if you’re stuck behind a screen. The ability to sketch things is so important.”&lt;/p&gt;&lt;p&gt;Trumper studies precision mechatronics, broadly, with a particular interest in mechatronic systems for demanding resolutions. Examples include projects that employ magnetic levitation, linear motors for driving precision manufacturing for semiconductors, and spacecraft attitude control. His work also explores lathes, milling applications, and even bioengineering platforms.&lt;/p&gt;&lt;p&gt;Class 2.737, which is offered every two years, is lab-based. Sketches and concepts come to life in focused experiences designed to expose students to key principles in a hands-on way and are very much informed by what Trumper has found important in his research. The two-week-long lab explorations range from controlling a motor to evaluating electronic scales to vibration isolations systems built on a speaker. One year, students constructed a working atomic force microscope.&lt;/p&gt;&lt;p&gt;“The touch and sense of how things actually work is really important,” Trumper says. “As a designer, you have to be able to imagine. If you think of some new configuration of a motor, you need to imagine how it would work and see it working, so you can do design iterations in your imagined space — to make that real requires that you’ve had experience with the actual thing.”&lt;/p&gt;&lt;p&gt;He says his former late colleague, Woodie Flowers&amp;nbsp;SM ’68, MEng ’71, PhD ’73, used to call it “running the movie.” Trumper explains, “once you have the image in your mind, you can more easily picture what’s going on with the problem — what’s getting hot, where’s the stress, what do I like and not like about this design. If you can do that with a piece of paper and your imagination, now you design new things pretty creatively.”&lt;/p&gt;&lt;p&gt;Flowers had been the Pappalardo Professor Emeritus of Mechanical Engineering at the time of his passing in October 2019. &lt;a href="https://meche.mit.edu/woodie-flowers"&gt;He is remembered &lt;/a&gt;for pioneering approaches to education, and was instrumental in shaping MIT’s hands-on approach to engineering design education.&lt;/p&gt;&lt;p&gt;Class 2.737 tends to attract students who like to design and build their own things. “I want people who are heading toward being hardware geeks,” says Trumper, laughing. “And I mean that lovingly.” He says his most important objective for this class is that students learn real tools that they will find useful years from now in their own engineering research or practice.&lt;/p&gt;&lt;p&gt;“Being able to see how multiple pieces fit in together and create one whole working system is just really empowering to me as an aspiring engineer,” says Cui.&lt;/p&gt;&lt;p&gt;For fellow 2.737 student Zach Francis, the course offered foundations for the future along with a meaningful tie to the past. “This class reminded me about what I enjoy about engineering. You look at it when you’re a young kid and you're like ‘that looks like magic!’ and then as an adult you can now make that. It's the closest thing I've been to a wizard, and I like that a lot.”&amp;nbsp;&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202408/mit-Mechatronics-david-trumper.JPG?itok=Ae8dhJc6" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Professor of mechanical engineering David Trumper has been teaching 2.737 (Mechatronics) since he joined the MIT faculty in the early 1990s.]]></media:description>
              <media:credit>Photo: Lauren Futami/Department of Mechanical Engineering</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/classes-and-programs">Classes and programs</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/electronics">Electronics</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Engineering and matters of the heart </title>
  <link>https://news.mit.edu/2024/ellen-roche-studies-engineering-and-the-heart-0821</link>
  <description><![CDATA[Professor Ellen Roche is creating the next generation of medical devices to help repair hearts, lungs, and other tissues.]]></description>
  <pubDate>Wed, 21 Aug 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/ellen-roche-studies-engineering-and-the-heart-0821</guid>
        <dc:creator>Michaela Jarvis | MIT News correspondent</dc:creator>
  <content:encoded>&lt;div&gt;&lt;p&gt;Before she had even earned her bachelor’s degree, MIT professor and biomedical engineer Ellen Roche was gaining research experience in the medical device industry. In her third year at the National University of Ireland at Galway, Roche participated in a biomedical engineering program in which students worked at companies developing new devices for patient care.&lt;/p&gt;&lt;p&gt;“I worked on cardiovascular implants during my placement and loved it,” says Roche, an associate professor&amp;nbsp;at MIT’s Institute for Medical Engineering and Science (IMES) and Department of Mechanical Engineering. “For me, early experience in the medical device industry was very influential because it showed me the elaborate process of what happens from the time a technology is designed at the bench, as it is developed into a meticulously tested and reliable device that will actually be implanted in a human.”&lt;/p&gt;&lt;p&gt;In graduate school, a similar program led Roche first to Mednova Ltd. in Galway and then to its sister company, Abbott Vascular in California, initially for a six-month stay. Roche enjoyed the work so much that she ended up staying three and a half years. While at Mednova and Abbott, she worked on a carotid artery filter designed to prevent stroke during the procedure when a stent is implanted. She also investigated coating parts of the stents with drugs that prevent arteries from becoming occluded.&lt;/p&gt;&lt;p&gt;Roche, who earned tenure at MIT in July 2023, directs the Therapeutic Technology Design and Development Lab, which incorporates soft robotics, advanced fabrication methods, and computational analysis tools to develop novel devices that help to heal the heart, lungs, and other tissues. Some of the devices her team designs are intended for implantation into patients, such as a soft robotic ventilator, while others, such as a 3D-printed replica of a patient’s heart, enable research and testing of other therapies.&lt;/p&gt;&lt;p&gt;She encourages her students to find ways to collaborate and be flexible — and to get some kind of industry experience while still in school. She says she tells them, “Be open to accepting good opportunities as they arise, work with like-minded people, and work hard at what you are doing, but readapt when you need to."&lt;/p&gt;&lt;p&gt;“There’s so much that’s very hard to even imagine until you spend some time in industry, including regulatory submissions, quality control, clinical studies, manufacturing considerations, sterilization, reliability, packaging, labeling, distribution, and sales. It really is a concerted effort of many teams with many skills to get a device to first-in-human studies,” Roche says. “Having said that, it’s one of the most rewarding.”&lt;/p&gt;&lt;p&gt;Born in Galway, the daughter of a civil engineer father and a mother who was a radiographer, Roche always loved math, science, and building things, and was drawn to medicine as well. She says she chose biomedical engineering because of its interdisciplinary nature and its potential for impacting society.&lt;/p&gt;&lt;p&gt;Roche says her mother had a “huge influence” on her career choices.&lt;/p&gt;&lt;p&gt;“She brought me to the hospital to meet with people using various medical devices, and introduced me to one of my mentors in industry,” she says. “She had taught herself, as the local girls’ school she attended did not teach advanced (or honors) math.”&lt;/p&gt;&lt;p&gt;After working at Abbott, Roche says she found she wanted to expand her studies and learn new technologies that could be applied to medical devices. She returned to school, enrolling in a bioengineering master’s program at Trinity College in Dublin. While earning her degree, she also worked at Medtronic, where she helped develop a replacement valve for the aorta that was brought all the way from conception to clinical application in humans, a process she says she was fortunate to experience firsthand.&lt;/p&gt;&lt;p&gt;She also studied medicine at the Royal College of Surgeons in Ireland before being awarded at Fulbright Scholarship to pursue her PhD.&lt;/p&gt;&lt;p&gt;“Receiving the Fulbright Science and Technology award solidified my plans to pursue graduate study in the U.S.,” she says. She chose as PhD advisors David Mooney, a professor of bioengineering, and Conor Walsh, a professor of engineering and applied sciences, at Harvard University. “They were (and still are) amazingly supportive of my personal and professional development,” she says.&lt;/p&gt;&lt;p&gt;Roche has worked on a number of medical devices, including the soft, implantable ventilator; a mechanism that prevents the buildup of scar tissue; and the robotic heart, created by using 3D printing. For the robotic heart, Roche and her team start with an MRI scan of a patient’s heart and, using a soft material, print a replica of the heart, matching the anatomy, including any defects. With such a realistic model, the researchers can then apply different treatments, such as prosthetic valves or other implantable devices, in order to test them and learn more about the biomechanics that are involved.&lt;/p&gt;&lt;p&gt;“We can look at various devices and tune the heart, depending on what we’re trying to test,” Roche&amp;nbsp;&lt;a href="https://news.mit.edu/podcast/podcast-curiosity-unbounded-episode-6-healing-ailing-heart" target="_blank"&gt;said in the “Curiosity Unbounded” podcast&lt;/a&gt; with MIT President Sally Kornbluth.&lt;/p&gt;&lt;p&gt;The 3D-printed heart, and other medical simulators Roche has worked on, greatly facilitate and improve the testing of patient interventions — and may one day also be used as implantable devices in humans.&lt;/p&gt;&lt;p&gt;“You can envision the people who are at end-stage heart failure, who are waiting for a transplant and on these long lists, could actually have a printed, entirely synthetic, beating heart,” Roche told Kornbluth.&lt;/p&gt;&lt;p&gt;Roche’s work has garnered many awards, including a National Science Foundation CAREER award in 2019, and boosts to her entrepreneurship. Her medical device startup, Spheric Bio, which is developing a minimally invasive heart implant aimed at preventing strokes, won the Faculty Founders Initiative Grand Prize in 2022 and the Lab Central Ignite Golden Ticket, which supports startup founders from traditionally underrepresented groups in biotechnology.&lt;/p&gt;&lt;p&gt;Meanwhile, in a dual faculty appointment in mechanical and medical engineering, Roche won the Thomas McMahon Mentoring Award in 2020, which each year goes to a person who “through the warmth of their personality, inspires and nurtures [Harvard-MIT Program in Health Sciences and Technology] students in their scientific and personal growth.” She also received the Harold E. Edgerton Faculty Achievement Award in 2023, in recognition of exceptional teaching, research, and service.&lt;/p&gt;&lt;p&gt;The current research advances that excite Roche most, she says, include treatments and devices that can be customized to be patient-specific, such as in silico trials and digital twins where computational approaches can facilitate the investigation various interventions and prediction of their outcomes.&lt;/p&gt;&lt;p&gt;Roche’s expanding research on physical biorobotic simulators and computational models has attracted interest from industry and clinical teams. She was recently approached by a local hospital to build models for training heart surgeons on how to select which pump or ventricular assist device to use depending on a patient’s particular case. The models allow the surgeons to explore the efficacy of the assist devices at work.&lt;/p&gt;&lt;p&gt;Roche has three young daughters, whom she often brings to work, where “they love the environment, the students, and the lab,” she says.&lt;/p&gt;&lt;p&gt;Somehow, she also finds time to do triathlons, travel, and sample some of the local brews of New England. She’s currently planning to participate in a triathlon with her two PhD co-advisors, Mooney and Walsh. Luckily, she says she does her best thinking while running, biking, or swimming — or late at night.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Active and successful in so many realms, Roche provides seemingly simple advice to her students who want to have an impact on the world: “Find a way to combine what you love, what you are good at, and what will help others.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202408/MIT-Ellen-Roche_Hilton-01-press.jpg?itok=LaKGHjdV" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[The medical simulators Ellen Roche works on greatly facilitate and improve the testing of patient interventions — and may one day also be used as implantable devices in humans.]]></media:description>
              <media:credit>Photo: Jodi Hilton</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/profile">Profile</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/institute-medical-engineering-and-science-imes-0">Institute for Medical Engineering and Science (IMES)</category>
      <category domain="https://news.mit.edu/topic/bio-bioeng-biotech">Bioengineering and biotechnology</category>
      <category domain="https://news.mit.edu/topic/biomechanics">Biomechanics</category>
      <category domain="https://news.mit.edu/topic/medical-devices">Medical devices</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/industry">Industry</category>
      <category domain="https://news.mit.edu/topic/innovation">Innovation and Entrepreneurship (I&amp;E)</category>
      <category domain="https://news.mit.edu/topic/harvard-mit-health-sciences-and-technology">Harvard-MIT Health Sciences and Technology</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>AI assistant monitors teamwork to promote effective collaboration</title>
  <link>https://news.mit.edu/2024/ai-assistant-monitors-teamwork-promote-effective-collaboration-0819</link>
  <description><![CDATA[An AI team coordinator aligns agents’ beliefs about how to achieve a task, intervening when necessary to potentially help with tasks in search and rescue, hospitals, and video games.]]></description>
  <pubDate>Mon, 19 Aug 2024 15:50:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/ai-assistant-monitors-teamwork-promote-effective-collaboration-0819</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p&gt;On a research cruise around Hawaii in 2018, Yuening Zhang SM ’19, PhD ’24 saw how difficult it was to keep a tight ship. The careful coordination required to map underwater terrain could sometimes led to a stressful environment for team members, who might have different understandings of which tasks must be completed in spontaneously changing conditions. During these trips, Zhang considered how a robotic companion could have helped her and her crewmates achieve their goals more efficiently.&lt;br&gt;&lt;br&gt;Six years later, as a research assistant in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), Zhang developed what could be considered a missing piece: an AI assistant that communicates with team members to align roles and accomplish a common goal. In a paper presented at the International Conference on Robotics and Automation (ICRA) and &lt;a href="https://ieeexplore.ieee.org/document/10609865" target="_blank"&gt;published on IEEE Xplore on Aug. 8&lt;/a&gt;, she and her colleagues present a system that can oversee a team of both human and AI agents, intervening when needed to potentially increase teamwork effectiveness in domains like search-and-rescue missions, medical procedures, and strategy video games.&lt;br&gt;&lt;br&gt;The CSAIL-led group has developed a theory of mind model for AI agents, which represents how humans think and understand each other’s possible plan of action when they cooperate in a task. By observing the actions of its fellow agents, this new team coordinator can infer their plans and their understanding of each other from a prior set of beliefs. When their plans are incompatible, the AI helper intervenes by aligning their beliefs about each other, instructing their actions, as well as asking questions when needed.&lt;br&gt;&lt;br&gt;For example, when a team of rescue workers is out in the field to triage victims, they must make decisions based on their beliefs about each other’s roles and progress. This type of epistemic planning could be improved by CSAIL’s software, which can send messages about what each agent intends to do or has done to ensure task completion and avoid duplicate efforts. In this instance, the AI helper may intervene to communicate that an agent has already proceeded to a certain room, or that none of the agents are covering a certain area with potential victims.&lt;br&gt;&lt;br&gt;“Our work takes into account the sentiment that ‘I believe that you believe what someone else believes,’” says Zhang, who is now a research scientist at Mobi Systems. “Imagine you’re working on a team and you ask yourself, ‘What exactly is that person doing? What am I going to do? Does he know what I am about to do?’ We model how different team members understand the overarching plan and communicate what they need to accomplish to help complete their team’s overall goal.”&lt;br&gt;&lt;br&gt;&lt;strong&gt;AI to the rescue&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Even with a sophisticated plan, both human and robotic agents will encounter confusion and even make mistakes if their roles are unclear. This plight looms especially large in search-and-rescue missions, where the objective may be to locate someone in danger despite limited time and a vast area to scan. Thankfully, communication technology augmented with the new robotic assistant could potentially notify the search parties about what each group is doing and where they’re looking. In turn, the agents could navigate their terrain more efficiently.&lt;/p&gt;&lt;p&gt;This type of task organization could aid in other high-stakes scenarios like surgeries. In these cases, the nurse first needs to bring the patient to the operation room, then the anesthesiologist puts the patient to sleep before the surgeons begin the operation. Throughout the operation, the team must continuously monitor the patient’s condition while dynamically responding to the actions of each colleague. To ensure that each activity within the procedure remains well-organized, the AI team coordinator could oversee and intervene if confusion about any of these tasks arises.&lt;/p&gt;&lt;p&gt;Effective teamwork is also integral to video games like “Valorant,” where players collaboratively coordinate who needs to attack and defend against another team online. In these scenarios, an AI assistant could pop up on the screen to alert individual users about where they’ve misinterpreted which tasks they need to complete.&lt;/p&gt;&lt;p&gt;Before she led the development of this model, Zhang designed EPike, a computational model that can act as a team member. In a 3D simulation program, this algorithm controlled a robotic agent that needed to match a container to the drink chosen by the human. As rational and sophisticated as they may be, cases arise where these AI-simulated bots are limited by their misconceptions about their human partners or the task. The new AI coordinator can correct the agents’ beliefs when needed to resolve potential problems, and it consistently intervened in this instance. The system sent messages to the robot about the human’s true intentions to ensure it matched the container correctly.&lt;/p&gt;&lt;p&gt;“In our work on human-robot collaboration, we’ve been both humbled and inspired over the years by how fluid human partners can be,” says Brian C. Williams, MIT professor of aeronautics and astronautics, CSAIL member, and senior author on the study. “Just look at a young couple with kids, who work together to get their kids breakfast and off to school. If one parent sees their partner serving breakfast and still in their bathrobe, the parent knows to shower quickly and shuffle the kids off to school, without the need to say a word. Good partners are well in tune with the beliefs and goals of each other, and our work on epistemic planning strives to capture this style of reasoning.”&lt;/p&gt;&lt;p&gt;The researchers' method incorporates probabilistic reasoning with recursive mental modeling of the agents, allowing the AI assistant to make risk-bounded decisions. In addition, they focused on modeling agents’ understanding of plans and actions, which could complement previous work on modeling beliefs about the current world or environment. The AI assistant currently infers agents’ beliefs based on a given prior of possible beliefs, but the MIT group envisions applying machine learning techniques to generate new hypotheses on the fly. To apply this counterpart to real-life tasks, they also aim to consider richer plan representations in their work and reduce computation costs further.&lt;/p&gt;&lt;p&gt;Dynamic Object Language Labs President Paul Robertson, Johns Hopkins University Assistant Professor Tianmin Shu, and former CSAIL affiliate Sungkweon Hong PhD ’23 join Zhang and Williams on the paper. Their work was supported, in part, by the U.S. Defense Advanced Research Projects Agency (DARPA) Artificial Social Intelligence for Successful Teams (ASIST) program.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202404/MIT-csail-overseer.png?itok=5p9AU7sw" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[A system developed by MIT CSAIL researchers can oversee a team of both human and AI agents, communicating with team members to align roles and accomplish a common goal.]]></media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/games">Games</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/darpa">Defense Advanced Research Projects Agency (DARPA)</category>
    </item>
<item>
  <title>MIT engineers design tiny batteries for powering cell-sized robots</title>
  <link>https://news.mit.edu/2024/mit-engineers-design-tiny-batteries-powering-cell-sized-robots-0815</link>
  <description><![CDATA[These zinc-air batteries, smaller than a grain of sand, could help miniscule robots sense and respond to their environment.]]></description>
  <pubDate>Thu, 15 Aug 2024 15:30:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/mit-engineers-design-tiny-batteries-powering-cell-sized-robots-0815</guid>
        <dc:creator>Anne Trafton | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;A tiny battery designed by MIT engineers could enable the deployment of cell-sized, autonomous robots for drug delivery within in the human body, as well as other applications such as locating leaks in gas pipelines.&lt;/p&gt;&lt;p&gt;The new battery, which is 0.1 millimeters long and 0.002 millimeters&amp;nbsp;thick — roughly the thickness of a human hair — can capture oxygen from air and use it to oxidize zinc, creating a current with a potential of up to 1 volt. That is enough to power a small circuit, sensor, or actuator, the researchers showed.&lt;/p&gt;&lt;p&gt;“We think this is going to be very enabling for robotics,” says Michael Strano, the Carbon P. Dubbs Professor of Chemical Engineering at MIT and the senior author of the study. “We’re building robotic functions onto the battery and starting to put these components together into devices.”&lt;/p&gt;&lt;p&gt;Ge Zhang PhD ’22 and Sungyun Yang, an MIT graduate student,&amp;nbsp;are the lead author of the &lt;a href="https://www.science.org/doi/10.1126/scirobotics.ade4642" target="_blank"&gt;paper&lt;/a&gt;, which appears in &lt;em&gt;Science Robotics&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Powered by batteries&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For several years, Strano’s lab has been working on &lt;a href="https://news.mit.edu/2018/cell-sized-robots-sense-their-environment-0723" target="_blank"&gt;tiny robots&lt;/a&gt; that can sense and respond to stimuli in their environment. One of the major challenges in developing such tiny robots is making sure that they have enough power.&lt;/p&gt;&lt;p&gt;Other researchers have shown that they can power microscale devices using solar power, but the limitation to that approach is that the robots must have a laser or another light source pointed at them at all times. Such devices are known as “marionettes” because they are controlled by an external power source. Putting a power source such as a battery inside these tiny devices could free them to roam much farther.&lt;/p&gt;&lt;p&gt;“The marionette systems don’t really need a battery because they’re getting all the energy they need from outside,” Strano says. “But if you want a small robot to be able to get into spaces that you couldn’t access otherwise, it needs to have a greater level of autonomy. A battery is essential for something that’s not going to be tethered to the outside world.”&lt;/p&gt;&lt;p&gt;To create robots that could become more autonomous, Strano’s lab decided to use a type of battery known as a zinc-air battery. These batteries, which have a longer lifespan than many other types of batteries due to their high energy density, are often used in hearing aids.&lt;/p&gt;&lt;p&gt;The battery that they designed consists of a zinc electrode connected to a platinum electrode, embedded into a strip of a polymer called SU-8, which is commonly used for microelectronics. When these electrodes interact with oxygen molecules from the air, the zinc becomes oxidized and releases electrons that flow to the platinum electrode, creating a current.&lt;/p&gt;&lt;p&gt;In this study, the researchers showed that this battery could provide enough energy to power an actuator — in this case, a robotic arm that can be raised and lowered. The battery could also power a memristor, an electrical component that can store memories of events by changing its electrical resistance, and a clock circuit, which allows robotic devices to keep track of time.&lt;/p&gt;&lt;p&gt;The battery also provides enough power to run two different types of sensors that change their electrical resistance when they encounter chemicals in the environment. One of the sensors is made from atomically thin molybdenum disulfide and the other from carbon nanotubes.&lt;/p&gt;&lt;p&gt;“We’re making the basic building blocks in order to build up functions at the cellular level,” Strano says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Robotic swarms&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In this study, the researchers used a wire to connect their battery to an external device, but in future work they plan to build robots in which the battery is incorporated into a device.&lt;/p&gt;&lt;p&gt;“This is going to form the core of a lot of our robotic efforts,” Strano says. “You can build a robot around an energy source, sort of like you can build an electric car around the battery.”&lt;/p&gt;&lt;p&gt;One of those efforts revolves around designing tiny robots that could be injected into the human body, where they could seek out a target site and then release a drug such as insulin. For use in the human body, the researchers envision that the devices would be made of biocompatible materials that would break apart once they were no longer needed.&lt;/p&gt;&lt;p&gt;The researchers are also working on increasing the voltage of the battery, which may enable additional applications.&lt;/p&gt;&lt;p&gt;The research was funded by the U.S. Army Research Office, the U.S. Department of Energy, the National Science Foundation, and a MathWorks Engineering Fellowship.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202408/MIT-Microbatteries-01-PRESS.jpg?itok=8z19Li01" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[The zinc-air battery is 0.1 millimeters long and 0.002 millimeters thick.]]></media:description>
              <media:credit>Credit: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/energy">Energy</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/batteries">Batteries</category>
      <category domain="https://news.mit.edu/topic/sensors">Sensors</category>
      <category domain="https://news.mit.edu/topic/drug-delivery">Drug delivery</category>
      <category domain="https://news.mit.edu/topic/chemical-engineering">Chemical engineering</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/doe">Department of Energy (DoE)</category>
    </item>
<item>
  <title>A new model offers robots precise pick-and-place solutions</title>
  <link>https://news.mit.edu/2024/new-model-offers-robots-precise-pick-place-solutions-0809</link>
  <description><![CDATA[SimPLE learns to pick, regrasp, and place objects using the objects’ computer-aided design model.]]></description>
  <pubDate>Fri, 09 Aug 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/new-model-offers-robots-precise-pick-place-solutions-0809</guid>
        <dc:creator>Anne Wilson | Department of Mechanical Engineering</dc:creator>
  <content:encoded>&lt;p&gt;Pick-and-place machines are a type of automated equipment used to place objects into structured, organized locations. These machines are used for a variety of applications — from electronics assembly to packaging, bin picking, and even inspection — but many current pick-and-place solutions are limited. Current solutions lack “precise generalization,” or the ability to solve many tasks without compromising on accuracy.&lt;/p&gt;&lt;p&gt;“In industry, you often see that [manufacturers] end up with very tailored solutions to the particular problem that they have, so a lot of engineering and not so much flexibility in terms of the solution,” Maria Bauza Villalonga PhD ’22, a senior research scientist at Google DeepMind where she works on robotics and robotic manipulation. “SimPLE solves this problem and provides a solution to pick-and-place that is flexible and still provides the needed precision.”&lt;/p&gt;&lt;p&gt;A new &lt;a href="https://www.science.org/doi/10.1126/scirobotics.adi8808" target="_blank"&gt;paper&lt;/a&gt; by MechE researchers published in the journal&amp;nbsp;&lt;em&gt;Science Robotics&lt;/em&gt;&amp;nbsp;explores pick-and-place solutions with more precision. In precise pick-and-place, also known as kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement.&amp;nbsp;The approach, dubbed SimPLE (Simulation to Pick Localize and placE), learns to pick, regrasp and place objects using the object’s computer-aided design (CAD) model, and all without any prior experience or encounters with the specific objects.&lt;/p&gt;&lt;p&gt;“The promise of SimPLE is that we can solve many different tasks with the same hardware and software using simulation to learn models that adapt to each specific task,” says &lt;a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU"&gt;Alberto Rodriguez&lt;/a&gt;, an MIT visiting scientist who is a former member of the MechE faculty and now associate director of manipulation research for Boston Dynamics. SimPLE was developed by members of the Manipulation and Mechanisms Lab at MIT (MCube) under Rodriguez’ direction.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“In this work we show that it is possible to achieve the levels of positional accuracy that are required for many industrial pick and place tasks without any other specialization,” Rodriguez says.&lt;/p&gt;&lt;p&gt;Using&amp;nbsp;a dual-arm robot equipped with visuotactile sensing, the SimPLE solution employs three main components: task-aware grasping, perception by sight and touch (visuotactile perception), and regrasp planning. Real observations are matched against a set of simulated observations through supervised learning so that a distribution of likely object poses can be estimated, and placement accomplished.&lt;/p&gt;&lt;p&gt;In experiments, SimPLE successfully demonstrated the ability to pick-and-place diverse objects spanning a wide range of shapes, achieving successful placements over 90 percent of the time for 6 objects, and over 80 percent of the time for 11 objects.&lt;/p&gt;&lt;p&gt;“There’s an intuitive understanding in the robotics community that vision and touch are both useful, but [until now] there haven’t been many systematic demonstrations of how it can be useful for complex robotics tasks,” says mechanical engineering doctoral student Antonia Delores Bronars SM ’22. Bronars, who is now working with Pulkit Agrawal, assistant&amp;nbsp;professor&amp;nbsp;in the department of Electrical Engineering and Computer Science (EECS), is continuing her PhD work investigating the incorporation of tactile capabilities into robotic systems.&lt;/p&gt;&lt;p&gt;“Most work on grasping ignores the downstream tasks,” says Matt Mason, chief scientist at&amp;nbsp;Berkshire Grey&amp;nbsp;and professor emeritus at Carnegie Mellon University who was not involved in the work. “This paper goes beyond the desire to mimic humans, and shows from a strictly functional viewpoint the utility of combining tactile sensing, vision, with two hands.”&lt;/p&gt;&lt;p&gt;Ken Goldberg, the William S. Floyd Jr. Distinguished Chair in Engineering&amp;nbsp;at the University of California at Berkeley,&amp;nbsp;who was also not involved in the study, says the robot manipulation&amp;nbsp;methodology described in the paper offers a&amp;nbsp;valuable alternative to the&amp;nbsp;trend toward AI and machine learning methods.&lt;/p&gt;&lt;p&gt;“The authors combine well-founded geometric algorithms that can&amp;nbsp;reliably achieve high-precision for a specific set of object shapes and&amp;nbsp;demonstrate that this combination can significantly improve performance&amp;nbsp;over AI methods,” says Goldberg, who is also co-founder and chief scientist for Ambi Robotics and Jacobi Robotics. “This can be immediately useful in industry and is an&amp;nbsp;excellent example of what I call 'good old fashioned engineering' (GOFE).”&lt;/p&gt;&lt;p&gt;Bauza and Bronars say this work was informed by several generations of collaboration.&lt;/p&gt;&lt;p&gt;“In order to really demonstrate how vision and touch can be useful together, it’s necessary to build a full robotic system, which is something that’s very difficult to do as one person over a short horizon of time,” says Bronars. “Collaboration, with each other and with Nikhil [Chavan-Dafle PhD ‘20] and Yifan [Hou PhD ’21 CMU], and across many generations and labs really allowed us to build an end-to-end system.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202408/mit-simple-00.JPG?itok=suByri6K" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[SimPLE, an approach to object manipulation developed by Department of Mechanical Engineering researchers, aims to “reduce the burden of introducing new objects to make it so that robots can interact still precisely but more flexibly,” says doctoral student Antonia Delores Bronars SM ’22.]]></media:description>
              <media:credit>Image: John Freidah/MIT Department of Mechanical Engineering</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/manufacturing">Manufacturing</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Helping robots practice skills independently to adapt to unfamiliar environments</title>
  <link>https://news.mit.edu/2024/helping-robots-practice-skills-independently-adapt-unfamiliar-environments-0808</link>
  <description><![CDATA[A new algorithm helps robots practice skills like sweeping and placing objects, potentially helping them improve at important tasks in houses, hospitals, and factories.]]></description>
  <pubDate>Thu, 08 Aug 2024 10:45:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/helping-robots-practice-skills-independently-adapt-unfamiliar-environments-0808</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p&gt;The phrase “practice makes perfect” is usually reserved for humans, but it’s also a great maxim for robots newly deployed in unfamiliar environments.&lt;/p&gt;&lt;p&gt;Picture a robot arriving in a warehouse. It comes packaged with the skills it was trained on, like placing an object, and now it needs to pick items from a shelf it’s not familiar with. At first, the machine struggles with this, since it needs to get acquainted with its new surroundings. To improve, the robot will need to understand which skills within an overall task it needs improvement on, then specialize (or parameterize) that action.&lt;/p&gt;&lt;p&gt;A human onsite could program the robot to optimize its performance, but researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and The AI Institute have developed a more effective alternative. Presented at the Robotics: Science and Systems Conference last month, their “Estimate, Extrapolate, and Situate” (EES) algorithm enables these machines to practice on their own, potentially helping them improve at useful tasks in factories, households, and hospitals.&amp;nbsp;&lt;br&gt;&lt;br&gt;&lt;strong&gt;Sizing up the situation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To help robots get better at activities like sweeping floors, EES works with a vision system that locates and tracks the machine’s surroundings. Then, the algorithm estimates how reliably the robot executes an action (like sweeping) and whether it would be worthwhile to practice more. EES forecasts how well the robot could perform the overall task if it refines that particular skill, and finally, it practices. The vision system subsequently checks whether that skill was done correctly after each attempt.&lt;/p&gt;&lt;p&gt;EES could come in handy in places like a hospital, factory, house, or coffee shop. For example, if you wanted a robot to clean up your living room, it would need help practicing skills like sweeping. According to Nishanth Kumar SM ’24 and his colleagues, though, EES could help that robot improve without human intervention, using only a few practice trials.&lt;/p&gt;&lt;p&gt;“Going into this project, we wondered if this specialization would be possible in a reasonable amount of samples on a real robot,” says Kumar, co-lead author of a &lt;a href="https://arxiv.org/pdf/2402.15025.pdf" target="_blank"&gt;paper&lt;/a&gt; describing the work, PhD student in electrical engineering and computer science, and a CSAIL affiliate. “Now, we have an algorithm that enables robots to get meaningfully better at specific skills in a reasonable amount of time with tens or hundreds of data points, an upgrade from the thousands or millions of samples that a standard reinforcement learning algorithm requires.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See Spot sweep&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;EES’s knack for efficient learning was evident when implemented on Boston Dynamics’ Spot quadruped during research trials at The AI Institute. The robot, which has an arm attached to its back, completed manipulation tasks after practicing for a few hours. In one demonstration, the robot learned how to securely place a ball and ring on a slanted table in roughly three hours. In another, the algorithm guided the machine to improve at sweeping toys into a bin within about two hours. Both results appear to be an upgrade from previous frameworks, which would have likely taken more than 10 hours per task.&lt;br&gt;&lt;br&gt;“We aimed to have the robot collect its own experience so it can better choose which strategies will work well in its deployment,” says co-lead author Tom Silver SM ’20, PhD ’24, an electrical engineering and computer science (EECS) alumnus and CSAIL affiliate who is now an assistant professor at Princeton University. “By focusing on what the robot knows, we sought to answer a key question: In the library of skills that the robot has, which is the one that would be most useful to practice right now?”&lt;/p&gt;&lt;p&gt;EES could eventually help streamline autonomous practice for robots in new deployment environments, but for now, it comes with a few limitations. For starters, they used tables that were low to the ground, which made it easier for the robot to see its objects. Kumar and Silver also 3D printed an attachable handle that made the brush easier for Spot to grab. The robot didn’t detect some items and identified objects in the wrong places, so the researchers counted those errors as failures.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Giving robots homework&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers note that the practice speeds from the physical experiments could be accelerated further with the help of a simulator. Instead of physically working at each skill autonomously, the robot could eventually combine real and virtual practice. They hope to make their system faster with less latency, engineering EES to overcome the imaging delays the researchers experienced. In the future, they may investigate an algorithm that reasons over sequences of practice attempts instead of planning which skills to refine.&lt;br&gt;&lt;br&gt;“Enabling robots to learn on their own is both incredibly useful and extremely challenging,” says Danfei Xu, an assistant professor in the School of Interactive Computing at Georgia Tech and a research scientist at NVIDIA AI, who was not involved with this work. “In the future, home robots will be sold to all sorts of households and expected to perform a wide range of tasks. We can't possibly program everything they need to know beforehand, so it’s essential that they can learn on the job. However, letting robots loose to explore and learn without guidance can be very slow and might lead to unintended consequences. The research by Silver and his colleagues introduces an algorithm that allows robots to practice their skills autonomously in a structured way. This is a big step towards creating home robots that can continuously evolve and improve on their own.”&lt;br&gt;&lt;br&gt;Silver and Kumar’s co-authors are The AI Institute researchers Stephen Proulx and Jennifer Barry, plus four CSAIL members: Northeastern University PhD student and visiting researcher Linfeng Zhao, MIT EECS PhD student Willie McClinton, and MIT EECS professors Leslie Pack Kaelbling and Tomás Lozano-Pérez. Their work was supported, in part, by The AI Institute, the U.S. National Science Foundation, the U.S. Air Force Office of Scientific Research, the U.S. Office of Naval Research, the U.S. Army Research Office, and MIT Quest for Intelligence, with high-performance computing resources from the MIT SuperCloud and Lincoln Laboratory Supercomputing Center.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202407/MIT-csail-EES-algorithm.jpg?itok=nUpl80qM" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[A new algorithm developed by researchers at MIT CSAIL helps robots practice skills on their own. In experiments, it guided a quadruped with sweeping and placing various items.]]></media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/quest-intelligence">Quest for Intelligence</category>
      <category domain="https://news.mit.edu/topic/lincoln-laboratory-0">Lincoln Laboratory</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>Precision home robots learn with real-to-sim-to-real</title>
  <link>https://news.mit.edu/2024/precision-home-robotics-real-sim-real-0731</link>
  <description><![CDATA[CSAIL researchers introduce a novel approach allowing robots to be trained in simulations of scanned home environments, paving the way for customized household automation accessible to anyone.]]></description>
  <pubDate>Wed, 31 Jul 2024 15:45:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/precision-home-robotics-real-sim-real-0731</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-96286cd1-7fff-4eea-27e1-f7ca9ca5bfe7"&gt;At the top of many automation wish lists is a particularly time-consuming task: chores.&amp;nbsp;&lt;br&gt;&lt;br&gt;The moonshot of many roboticists is cooking up the proper hardware and software combination so that a machine can learn “generalist” policies (the rules and strategies that guide robot behavior) that work everywhere, under all conditions.&amp;nbsp;Realistically, though, if you have a home robot, you probably don’t care much about it working for your neighbors.&amp;nbsp;MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers decided, with that in mind, to attempt to find a solution to easily train robust robot policies for very specific environments.&lt;/p&gt;&lt;p dir="ltr"&gt;“We aim for robots to perform exceptionally well under disturbances, distractions, varying lighting conditions, and changes in object poses, all within a single environment,” says Marcel Torne Villasevil, MIT CSAIL research assistant in the Improbable AI lab and lead author on a recent &lt;a href="https://arxiv.org/abs/2403.03949"&gt;paper&lt;/a&gt; about the work. “We propose a method to create digital twins on the fly using the latest advances in computer vision. With just their phones, anyone can capture a digital replica of the real world, and the robots can train in a simulated environment much faster than the real world, thanks to GPU parallelization. Our approach eliminates the need for extensive reward engineering by leveraging a few real-world demonstrations to jump-start the training process.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Taking your robot home&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;RialTo, of course, is a little more complicated than just a simple wave of a phone and (boom!) home bot at your service. It begins by using your device to scan the target environment using tools like NeRFStudio, ARCode, or Polycam. Once the scene is reconstructed, users can upload it to RialTo’s interface to make detailed adjustments, add necessary joints to the robots, and more.&lt;/p&gt;&lt;p dir="ltr"&gt;The refined scene is exported and brought into the simulator. Here, the aim is to develop a policy based on real-world actions and observations, such as one for grabbing a cup on a counter. These real-world demonstrations are replicated in the simulation, providing some valuable data for reinforcement learning. “This helps in creating a strong policy that works well in both the simulation and the real world. An enhanced algorithm using reinforcement learning helps guide this process, to ensure the policy is effective when applied outside of the simulator,” says Torne.&lt;/p&gt;&lt;p dir="ltr"&gt;Testing showed that RialTo created strong policies for a variety of tasks, whether in controlled lab settings or more unpredictable real-world environments, improving 67 percent over imitation learning with the same number of demonstrations. The tasks involved opening a toaster, placing a book on a shelf, putting a plate on a rack, placing a mug on a shelf, opening a drawer, and opening a cabinet. For each task, the researchers tested the system’s performance under three increasing levels of difficulty: randomizing object poses, adding visual distractors, and applying physical disturbances during task executions. When paired with real-world data, the system outperformed traditional imitation-learning methods, especially in situations with lots of visual distractions or physical disruptions.&lt;/p&gt;&lt;p dir="ltr"&gt;“These experiments show that if we care about being very robust to one particular environment, the best idea is to leverage digital twins instead of trying to obtain robustness with large-scale data collection in diverse environments,” says Pulkit Agrawal, director of Improbable AI Lab, MIT electrical engineering and computer science (EECS) associate professor, MIT CSAIL principal investigator, and senior author on the work.&lt;/p&gt;&lt;p dir="ltr"&gt;As far as limitations, RialTo currently takes three days to be fully trained. To speed this up, the team mentions improving the underlying algorithms and using foundation models. Training in simulation also has its limitations, and currently it’s difficult to do effortless sim-to-real transfer and simulate deformable objects or liquids.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The next level&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;So what’s next for RialTo’s journey? Building on previous efforts, the scientists are working on preserving robustness against various disturbances while improving the model’s adaptability to new environments. “Our next endeavor is this approach to using pre-trained models, accelerating the learning process, minimizing human input, and achieving broader generalization capabilities,” says Torne.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’re incredibly enthusiastic about our 'on-the-fly' robot programming concept, where robots can autonomously scan their environment and learn how to solve specific tasks in simulation. While our current method has limitations — such as requiring a few initial demonstrations by a human and significant compute time for training these policies (up to three days) — we see it as a significant step towards achieving 'on-the-fly' robot learning and deployment,” says Torne. “This approach moves us closer to a future where robots won’t need a preexisting policy that covers every scenario. Instead, they can rapidly learn new tasks without extensive real-world interaction. In my view, this advancement could expedite the practical application of robotics far sooner than relying solely on a universal, all-encompassing policy.”&lt;/p&gt;&lt;p dir="ltr"&gt;“To deploy robots in the real world, researchers have traditionally relied on methods such as imitation learning from expert data, which can be expensive, or reinforcement learning, which can be unsafe,” says Zoey Chen, a computer science PhD student at the University of Washington who wasn’t involved in the paper. “RialTo directly addresses both the safety constraints of real-world RL [robot learning], and efficient data constraints for data-driven learning methods, with its novel real-to-sim-to-real pipeline. This novel pipeline not only ensures safe and robust training in simulation before real-world deployment, but also significantly improves the efficiency of data collection. RialTo has the potential to significantly scale up robot learning and allows robots to adapt to complex real-world scenarios much more effectively.”&lt;/p&gt;&lt;p dir="ltr"&gt;"Simulation has shown impressive capabilities on real robots by providing inexpensive, possibly infinite data for policy learning,” adds Marius Memmel, a computer science PhD student at the University of Washington who wasn’t involved in the work. “However, these methods are limited to a few specific scenarios, and constructing the corresponding simulations is expensive and laborious. RialTo provides an easy-to-use tool to reconstruct real-world environments in minutes instead of hours. Furthermore, it makes extensive use of collected demonstrations during policy learning, minimizing the burden on the operator and reducing the sim2real gap. RialTo demonstrates robustness to object poses and disturbances, showing incredible real-world performance without requiring extensive simulator construction and data collection.”&lt;/p&gt;&lt;p dir="ltr"&gt;Torne wrote this paper alongside senior authors Abhishek Gupta, assistant professor at the University of Washington, and Agrawal. Four other CSAIL members are also credited: EECS PhD student Anthony Simeonov SM ’22, research assistant Zechu Li, undergraduate student April Chan, and Tao Chen PhD ’24. Improbable AI Lab and WEIRD Lab members also contributed valuable feedback and support in developing this project.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;This work was supported, in part, by the Sony Research Award, the U.S. government, and Hyundai Motor Co., with assistance from the WEIRD (Washington Embodied Intelligence and Robotics Development) Lab. The researchers presented their work at the Robotics Science and Systems (RSS) conference earlier this month.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202407/MIT-RialTo.png?itok=votDke2J" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[With the help of “digital twins,” RialTo, developed by MIT researchers Marcel Torne Villasevil (left) and Pulkit Agrawal, can guide a robot to practice in a simulated environment much faster than it would if it honed its skills in the real world.]]></media:description>
              <media:credit>Photo: Mike Grimmett/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Flying high to enable sustainable delivery, remote care</title>
  <link>https://news.mit.edu/2024/flying-high-enable-sustainable-delivery-remote-care-0725</link>
  <description><![CDATA[Drone company founders with MIT Advanced Study Program roots seek to bring aerial delivery to the mainstream.]]></description>
  <pubDate>Thu, 25 Jul 2024 16:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/flying-high-enable-sustainable-delivery-remote-care-0725</guid>
        <dc:creator>Elisabeth Erskine | MIT Professional Education</dc:creator>
  <content:encoded>&lt;p&gt;Five years ago, what began as three nervous Norwegians spotting each other across a study room has evolved into a drone company enabling sustainable deliveries, elder care, and more against a backdrop of unforgiving conditions.&lt;br&gt;&lt;br&gt;Lars Erik Fagernæs, Herman Øie Kolden, and Bernhard Paus Græsdal all attended the Norwegian University of Science and Technology, but their paths first crossed in the &lt;a href="https://professional.mit.edu/?utm_source=mit-news&amp;amp;utm_medium=web&amp;amp;utm_campaign=pe-2025&amp;amp;utm_content=aviant"&gt;MIT Professional Education&lt;/a&gt; &lt;a href="https://professional.mit.edu/advanced-study-program?utm_source=mit-news&amp;amp;utm_medium=web&amp;amp;utm_campaign=asp-spring-2024&amp;amp;utm_content=aviant"&gt;Advanced Study Program&lt;/a&gt; lounge in 2019, while they were apprehensive about their impending English exam. From there, they each pursued different tracks of study through the Advanced Study Program: Fagernæs studied computer science, Kolden took applied physics classes, and Græsdal, robotics. Months later, when the world shut down due to the Covid-19 pandemic, the trio’s professional trajectories intertwined.&lt;br&gt;&lt;br&gt;At the height of the pandemic in 2020, Fagernæs, Kolden, and Græsdal launched &lt;a href="https://www.aviant.no/"&gt;Aviant&lt;/a&gt; — a drone delivery service company. Aviant flew blood samples across Norway’s vast countryside to assist remote hospitals in diagnosing Covid. Today, their drones are delivering groceries, over-the-counter medicines, and takeout food to populations outside city centers.&amp;nbsp;&lt;br&gt;&lt;br&gt;&lt;strong&gt;Capitalizing on momentum&lt;/strong&gt;&lt;br&gt;&lt;br&gt;The pandemic waned, but the need for medical sample delivery did not. Remote hospitals still require reliable and rapid sample transportation, which Aviant continues to supply through its commercial contracts. In 2021, instead of sticking with commercial-only deliveries, the Aviant founders decided to use their momentum to reach for the largest market within autonomous transportation: last-mile delivery.&lt;br&gt;&lt;br&gt;“Yes, you need a higher volume for the business case to make sense,” explains Fagernæs of the expansion. “Yes, it is a lot more risky, but if you make it, it’s such a big opportunity.” The Norwegian government and various venture capital firms backing Aviant agree that this risk was worth their investment. Aviant has secured millions in funding to explore the consumer market through its newest offering, &lt;a href="https://kyte.delivery/"&gt;Kyte&lt;/a&gt;.&amp;nbsp;&lt;br&gt;&lt;br&gt;To scale operations, work still needs to be done to ingratiate drone delivery to the general population. Emphasizing the environmental benefits of aerial versus traditional road deliveries, the founders say, may be the most compelling factors that propel drones to the mainstream.&lt;br&gt;&lt;br&gt;So far, Aviant has flown more than 30,000 kilometers, saving 4,440 kilograms of carbon dioxide that would have been emitted through traditional transportation methods. “It doesn’t make sense to use a two- to four-ton vehicle to transport one kilogram or two kilograms of sushi or medicine,” Fagernæs reasons. “You also have cars eroding the roads, you have a lot of car accidents. Not only do you remove the cars from roads by flying [deliveries] with drones, it’s also a lot more energy efficient.”&lt;br&gt;&lt;br&gt;Aviant’s competitors — among them Alphabet — are spurring Fagernæs and Kolden to further improve their nicknamed “Viking drones.” Designed to sustain Norway’s harsh winter conditions and high winds, Aviant drones are well-adapted to service remote areas across Europe and the United States, a market they hope to break into soon.&lt;br&gt;&lt;br&gt;&lt;strong&gt;The unmatched MIT work ethic&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Fagernæs and Kolden owe much to MIT: It’s where they met and hatched their company. After his time with the Advanced Study Program, Græsdal decided to return to MIT to pursue his doctorate. The professors and mentors they engaged with across the Institute were instrumental in getting Aviant off the ground.&lt;br&gt;&lt;br&gt;Fagernæs recalls the beginning stages of discovering the drones’ theoretical flying limit; however, he quickly ran into the hurdle that neither he nor his peers had experience deriving such data. At that moment, there was perhaps no better place on Earth to be. “We figured, OK, we’re at MIT, we might as well just ask someone.” Fagernæs started knocking on doors and was eventually pointed in the direction of Professor Mark Drela’s office.&amp;nbsp;&lt;br&gt;&lt;br&gt;“I remember meeting Mark. Very, very humble guy, just talking to me like ‘Lars, yes, this, I will help you out, read this book, look at this paper.’” It was only when Fagernæs met back up with Kolden and Græsdal that he realized he had asked elementary questions to one of the leading experts in aeronautical engineering, and he truly appreciated Drela’s patience and helpfulness. The trio also credit Professor Russ Tedrake as being an inspiration to their current careers.&lt;br&gt;&lt;br&gt;Additionally, the work ethic of their fellow Beavers inspires them to work hard to this day. “I was finishing an assignment, and I think I left the Strata Student Center at 5:30 [in the morning] and it was half-full,” Kolden remembers. “And that has really stuck with me. And even when we run Aviant now, we know that in order to succeed, you have to work really, really hard.”&lt;br&gt;&lt;br&gt;“I’m impressed with how much Aviant has accomplished in such a short time,” says Drela. “Introducing drones to a wider population is going to make large improvements in high-value and time-critical payload delivery, and at much lower costs than the current alternatives. I’m looking forward to seeing how Aviant grows in the next few years.”&amp;nbsp;&lt;br&gt;&lt;br&gt;&lt;strong&gt;“For the betterment of humankind”&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Drones are the future, and Kolden is proud that Aviant’s electric drones are setting a sustainable precedent. “We had the choice to use gasoline drones. It was very tempting, because they can fly 10 times farther if you just use gasoline. But we just came from MIT, we worked on climate-related problems. We just couldn’t look ourselves in the mirror if we used gasoline-driven drones. So, we chose to go for the electric path, and that’s now paid off.”&lt;br&gt;&lt;br&gt;In the age of automation and perceived diminishing human connections, Kolden did have a moment of doubt about whether drones were part of the dilemma. “Are we creating a dystopian society where my grandfather is just meeting a robot, saying, ‘Here is your food,’ and then flying off again?” Kolden asked himself. After deep conversations with industry experts, and considering the low birth rate and aging population in Norway, he now concludes that drones are part of the solution. “Drones are going to help out a lot and actually make it possible to take care of all people and give them food and medicine when there simply aren’t enough people to do it.”&lt;br&gt;&lt;br&gt;Fagernæs also takes to heart the section of the MIT mission where students are urged to “work wisely, creatively, and effectively for the betterment of humankind.” He says, “When we started the company, it was all about using drones to help out society. We started to fly during the Covid pandemic to improve the logistics of the health-care sector in Norway, where people weren’t being diagnosed for Covid because of lacking logistics.”&lt;br&gt;&lt;br&gt;“The story of the success of Lars Erik, Herman, and Aviant makes us proud of what we do at MIT Professional Education.” says Executive Director &lt;a href="https://professional.mit.edu/programs/faculty-profiles/bhaskar-pant?utm_source=mit-news&amp;amp;utm_medium=web&amp;amp;utm_campaign=pe-2025&amp;amp;utm_content=aviant"&gt;Bhaskar Pant&lt;/a&gt;. “Share MIT knowledge that leads people to be innovative, entrepreneurial, and above all pursue the MIT mission of working toward the betterment of humankind. Kyte is a shining example of that.”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202407/mit-spinout-aviant-Founders.JPG?itok=Dzi3YHsj" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Former students in the MIT Advanced Study Program Herman Øie Kolden (left) and Lars Erik Fagernæs expanded their drone company, delivering essentials and peace of mind to remote communities.]]></media:description>
              <media:credit>Photo: Axel Brandtzaeg</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/startups">Startups</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/drones">Drones</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/mit-professional-education-0">MIT Professional Education</category>
      <category domain="https://news.mit.edu/topic/sustainability">Sustainability</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/innovation">Innovation and Entrepreneurship (I&amp;E)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Creating and verifying stable AI-controlled systems in a rigorous and flexible way</title>
  <link>https://news.mit.edu/2024/creating-verifying-stable-ai-controlled-systems-rigorous-flexible-way-0717</link>
  <description><![CDATA[Neural network controllers provide complex robots with stability guarantees, paving the way for the safer deployment of autonomous vehicles and industrial machines.]]></description>
  <pubDate>Wed, 17 Jul 2024 21:20:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/creating-verifying-stable-ai-controlled-systems-rigorous-flexible-way-0717</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p&gt;Neural networks have made a seismic impact on how engineers design controllers for robots, catalyzing more adaptive and efficient machines. Still, these brain-like machine-learning systems are a double-edged sword: Their complexity makes them powerful, but it also makes it difficult to guarantee that a robot powered by a neural network will safely accomplish its task.&lt;br&gt;&lt;br&gt;The traditional way to verify safety and stability is through techniques called Lyapunov functions. If you can find a Lyapunov function whose value consistently decreases, then you can know that unsafe or unstable situations associated with higher values will never happen. For robots controlled by neural networks, though, prior approaches for verifying Lyapunov conditions didn’t scale well to complex machines.&lt;/p&gt;&lt;p&gt;Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and elsewhere have now developed new techniques that rigorously certify Lyapunov calculations in more elaborate systems. Their algorithm efficiently searches for and verifies a Lyapunov function, providing a stability guarantee for the system. This approach could potentially enable safer deployment of robots and autonomous vehicles, including aircraft and spacecraft.&lt;/p&gt;&lt;p&gt;To outperform previous algorithms, the researchers found a frugal shortcut to the training and verification process. They generated cheaper counterexamples — for example, adversarial data from sensors that could’ve thrown off the controller — and then optimized the robotic system to account for them. Understanding these edge cases helped machines learn how to handle challenging circumstances, which enabled them to operate safely in a wider range of conditions than previously possible. Then, they developed a novel verification formulation that enables the use of a scalable neural network verifier, α,β-CROWN, to provide rigorous worst-case scenario guarantees beyond the counterexamples.&lt;/p&gt;&lt;p&gt;“We’ve seen some impressive empirical performances in AI-controlled machines like humanoids and robotic dogs, but these AI controllers lack the formal guarantees that are crucial for safety-critical systems,” says Lujie Yang, MIT electrical engineering and computer science (EECS) PhD student and CSAIL affiliate who is a co-lead author of a new paper on the project alongside Toyota Research Institute researcher Hongkai Dai SM ’12, PhD ’16. “Our work bridges the gap between that level of performance from neural network controllers and the safety guarantees needed to deploy more complex neural network controllers in the real world,” notes Yang.&lt;/p&gt;&lt;p&gt;For a digital demonstration, the team simulated how a quadrotor drone with lidar sensors would stabilize in a two-dimensional environment. Their algorithm successfully guided the drone to a stable hover position, using only the limited environmental information provided by the lidar sensors. In two other experiments, their approach enabled the stable operation of two simulated robotic systems over a wider range of conditions: an inverted pendulum and a path-tracking vehicle. These experiments, though modest, are relatively more complex than what the neural network verification community could have done before, especially because they included sensor models.&lt;br&gt;&lt;br&gt;“Unlike common machine learning problems, the rigorous use of neural networks as Lyapunov functions requires solving hard global optimization problems, and thus scalability is the key bottleneck,” says Sicun Gao, associate professor of computer science and engineering at the University of California at San Diego, who wasn’t involved in this work. “The current work makes an important contribution by developing algorithmic approaches that are much better tailored to the particular use of neural networks as Lyapunov functions in control problems. It achieves impressive improvement in scalability and the quality of solutions over existing approaches. The work opens up exciting directions for further development of optimization algorithms for neural Lyapunov methods and the rigorous use of deep learning in control and robotics in general.”&lt;/p&gt;&lt;p&gt;Yang and her colleagues’ stability approach has potential wide-ranging applications where guaranteeing safety is crucial. It could help ensure a smoother ride for autonomous vehicles, like aircraft and spacecraft. Likewise, a drone delivering items or mapping out different terrains could benefit from such safety guarantees.&lt;/p&gt;&lt;p&gt;The techniques developed here are very general and aren’t just specific to robotics; the same techniques could potentially assist with other applications, such as biomedicine and industrial processing, in the future.&lt;br&gt;&lt;br&gt;While the technique is an upgrade from prior works in terms of scalability, the researchers are exploring how it can perform better in systems with higher dimensions. They’d also like to account for data beyond lidar readings, like images and point clouds.&lt;/p&gt;&lt;p&gt;As a future research direction, the team would like to provide the same stability guarantees for systems that are in uncertain environments and subject to disturbances. For instance, if a drone faces a strong gust of wind, Yang and her colleagues want to ensure it’ll still fly steadily and complete the desired task.&amp;nbsp;&lt;br&gt;&lt;br&gt;Also, they intend to apply their method to optimization problems, where the goal would be to minimize the time and distance a robot needs to complete a task while remaining steady. They plan to extend their technique to humanoids and other real-world machines, where a robot needs to stay stable while making contact with its surroundings.&lt;br&gt;&lt;br&gt;Russ Tedrake, the Toyota Professor of EECS, Aeronautics and Astronautics, and Mechanical Engineering at MIT, vice president of robotics research at TRI, and CSAIL member, is a senior author of this research. The paper also credits University of California at Los Angeles PhD student Zhouxing Shi and associate professor Cho-Jui Hsieh, as well as University of Illinois Urbana-Champaign assistant professor Huan Zhang. Their work was supported, in part, by Amazon, the National Science Foundation, the Office of Naval Research, and the AI2050 program at Schmidt Sciences. The researchers’ paper will be presented at the 2024 International Conference on Machine Learning.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202407/MIT-Lyapunov_0.png?itok=6ZQzm3yD" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT CSAIL researchers helped design a new technique that can guarantee the stability of robots controlled by neural networks. This development could eventually lead to safer autonomous vehicles and industrial robots.]]></media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/drones">Drones</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/safety">Safety</category>
      <category domain="https://news.mit.edu/topic/networks">Networks</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
    </item>
<item>
  <title>Designing for outer space</title>
  <link>https://news.mit.edu/2024/designing-outer-space-0623</link>
  <description><![CDATA[With NASA planning permanent bases in space and on the moon, MIT students develop prototypes for habitats far from planet Earth.]]></description>
  <pubDate>Sun, 23 Jun 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/designing-outer-space-0623</guid>
        <dc:creator>Maria Iacobo | School of Architecture and Planning</dc:creator>
  <content:encoded>&lt;p&gt;A new MIT course this spring asked students to design what humans might need to comfortably work in and inhabit space. The time for these creations is now. While the NASA Apollo missions saw astronauts land on the moon, collect samples, and return home, the missions planned under Artemis, NASA’s current moon exploration program, include establishing long-term bases in orbit as well as on the surface of the moon.&lt;/p&gt;&lt;p&gt;The cross-disciplinary design course MAS.S66/4.154/16.89 (Space Architectures) was run in parallel with the departments of Architecture, and Aeronautics and Astronautics (AeroAstro), and the MIT Media Lab’s Space Exploration Initiatives group. Thirty-five students from across the Institute registered to imagine, design, prototype, and test what might be needed to support human habitation and activities on the moon.&lt;/p&gt;&lt;p&gt;The course’s popularity was not surprising to the instructors.&lt;/p&gt;&lt;p&gt;“A&amp;nbsp;lot of students at MIT are excited about space,” says&amp;nbsp;Jeffrey Hoffman, one of the course instructors and professor of the practice in AeroAstro. Before teaching at MIT, Hoffman was a NASA astronaut who flew five missions aboard the space shuttle.&amp;nbsp;“Certainly in AeroAstro, half the students want to be astronauts eventually, so it’s not like they hadn’t thought about living in space before. This was an opportunity to use that inspiration and work on a project that might become an actual design for real lunar habitats.”&lt;/p&gt;&lt;p&gt;MIT’s history with NASA, and with&amp;nbsp;the &lt;a href="https://news.mit.edu/2019/behind-scenes-apollo-mission-0718"&gt;Apollo missions&lt;/a&gt; in particular, is well documented. NASA’s first major contract for the Apollo program was awarded to MIT in 1961. Dava Newman, director of the MIT Media Lab and former NASA deputy administrator, was also a course instructor.&lt;/p&gt;&lt;p&gt;Preparing students for the next phase of working and living in space was the goal of this class. In addition to the Artemis missions, the rise of commercial spaceflight foretells the need to investigate these designs.&lt;/p&gt;&lt;p&gt;“MIT Architecture has always succeeded best at the intersection of research and practice,” says Nicholas de Monchaux, a course instructor and architecture department head. “With more and more designers being called on to design for extreme environments and conditions — including space — we see an important opportunity for research, collaboration, and new forms of practice, including an ongoing collaboration with the Media Lab and AeroAstro on designing for extreme environments.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Designing lunar habitats&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A defining aspect of the class is the blend of architecture and engineering students. &lt;a href="https://www.media.mit.edu/courses/mas-s66/"&gt;Each group&lt;/a&gt; brought different mindsets and approaches to the questions and challenges put before them. Shared activities, guest lectures, and a week touring NASA’s Johnson Space Center in Houston, Texas; the SpaceX launch facility in Brownsville, Texas; and ICON’s 3D printing facilities for construction in Austin, Texas, provided the students with an introduction to teams already working in this field. Paramount among their lessons: an understanding of the harsh environments for which they will be designing.&lt;/p&gt;&lt;p&gt;Hoffman doesn’t sugarcoat what life in space is like.&lt;/p&gt;&lt;p&gt;“Space is one of the most hostile environments you can imagine,” he says. “You're sitting inside a spacecraft looking out the window, realizing that on the other side of that window, I'd be dead in a few seconds.”&lt;/p&gt;&lt;p&gt;The students were divided into seven teams to develop their projects, and the value of collaboration quickly became apparent. The teams began with a concept phase where the visions of the architects — whose impulse was to create a comfortable and livable habitat — sometimes conflicted with those of the engineers, who were more focused on the realities of the extreme environment.&lt;/p&gt;&lt;p&gt;Inflatable designs emerged in several projects: a modular inflatable mobile science library that could support up to four people; an inflatable habitat that can be deployed within minutes to provide short-term shelter and protection for a crew on the moon; and a semi-permanent in situ habitat for space exploration ahead of an established lunar base.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Finding a common language&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“Architects and engineers tend to approach the design process differently,” says Annika Thomas, a mechanical engineering doctoral student and member of the&amp;nbsp;&lt;a href="https://www.media.mit.edu/projects/lunar-bricks/overview/"&gt;MoonBRICCS team&lt;/a&gt;.&amp;nbsp;“While it was a challenge to integrate these ideas early on, we found ways over time to communicate and coordinate our ideas, brought together by a common vision for the end of the project.”&lt;/p&gt;&lt;p&gt;Thomas’s teammates, architecture students Juan Daniel Hurtado Salazar and&amp;nbsp;Mikita Klimenka, say that technical considerations in architecture are often resolved toward the middle and end of a project.&lt;/p&gt;&lt;p&gt;“This gives us too much space to put off the implications of our design decisions while leaving little time to resolve them,” says Salazar.&amp;nbsp;“The insight of our engineers challenged every design decision from the onset with mechanical, economic, and technological implications of current space technology and material regimes. It also provided a fruitful arena to cooperatively discuss the concern that the most materially and economically optimal solutions are not always the most culturally or morally justified, as the emergence of long-term habitats brings the full gamut of an astronaut’s functional, social, and emotional needs to the forefront.”&lt;/p&gt;&lt;p&gt;Says Klimenka, “The wealth of knowledge and experience present within the team allowed us to meaningfully consider possible responses to producing a viable long-term habitat. While navigating both engineering and design constraints certainly required additional effort, the thinking process overall was extremely refreshing as we exposed ourselves to totally different sets of challenges that we do not typically deal with in our domains.”&lt;/p&gt;&lt;p&gt;Architecture graduate student&amp;nbsp;Kaicheng Zhuang, who worked with engineers on&amp;nbsp;the&amp;nbsp;&lt;a href="https://www.media.mit.edu/projects/lunar-sandbags/overview/"&gt;Lunar Sandbags project&lt;/a&gt;, says communication skills were “crucial” to the team working successfully together.&lt;/p&gt;&lt;p&gt;“With the engineers, it’s essential to focus on the technical feasibility and practical implementation, making sure every design element can be realistically achieved,” says Zhuang.&amp;nbsp;“They needed clear, precise information about structural integrity, material properties, and functionality. On the other hand, within our architecture team, discussions often revolve around the conceptual and aesthetic aspects, such as the visual impact, spatial dynamics, and user experience.”&lt;/p&gt;&lt;p&gt;Molly Johnson, an AeroAstro graduate student and team member on the&amp;nbsp;&lt;a href="https://www.media.mit.edu/projects/expandable-rv/overview/"&gt;lunarNOMAD project&lt;/a&gt;, concurs. “Traditionally, for a systems engineer such as myself it is easy to wave away the small design details and say they'll be addressed without going into detail about&amp;nbsp;how&amp;nbsp;they'll be addressed. The architects brought in a new level of detail that helped clarify our intentions.”&lt;/p&gt;&lt;p&gt;The team behind&amp;nbsp;&lt;a href="https://www.media.mit.edu/articles/momo-a-self-assembling-lunar-habitat-featured-in-designboom/"&gt;Momo: a Self-Assembling Lunar Habitat&lt;/a&gt; created a mission profile for their design. The semi-permanent in situ habitat was designed for space exploration ahead of establishing a permanent base on the moon. The module is flexible enough to fold nearly flat for easy transport. Their project was&amp;nbsp;&lt;a href="https://www.designboom.com/technology/momo-self-assembling-lunar-habitat-mit-media-lab-05-26-2024/"&gt;recently profiled&lt;/a&gt; in &lt;em&gt;DesignBoom&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Beyond Earth&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The final projects showed the vast differences among the teams despite there being a “limited number of ways that you can actually keep people alive on the lunar surface,” says&amp;nbsp;Cody Paige,&amp;nbsp;director of Space Exploration Initiatives and a course instructor. Students needed to consider what types of materials were needed; how these would be transported and assembled; how long their structures would remain functional; and what social or human experience would be supported, among other concerns.&lt;/p&gt;&lt;p&gt;The hands-on experience to create life-size models was especially important in this course given that AI is becoming a larger component of so many tasks and areas of decision-making, according to Paige.&lt;/p&gt;&lt;p&gt;“A computer doesn’t always translate exactly into the real world, and so having the students make prototypes shows them that there is a lot of benefit in understanding the materials you’re working with, how they function in real life, and the tactile ability that you can gather by working with these materials,” says Paige.&lt;/p&gt;&lt;p&gt;As fantastical as some of the projects appeared — with their combination of architecture, engineering, and design — they may very well be viable soon, especially as more architects are hired to design for space and students are understanding the landscape and needs for the demanding environments.&lt;/p&gt;&lt;p&gt;“We need to train our students to be the pioneers at the forefront of this field,” says&amp;nbsp;Skylar Tibbits, a professor in the architecture department and one of the course instructors. “The longer astronauts are in space or on the moon, we need to be designing habitats for human experiences that people will want to live in for a long time.”&lt;/p&gt;&lt;p&gt;The need for architects and engineers skilled in this specific field is thriving. Thomas — the engineering student on the MoonBRICCS team — is currently working on robotics for space application. Her teammate — Palak Patel — is an engineering doctoral student working on extreme environment materials for space applications. With the enthusiasm of the students, as well as the considerable real-world occupational need, the three academic units plan to continue to offer the course in the future.&lt;/p&gt;&lt;p&gt;“We see extending this into a multi-year program in designing for extreme environments — in space and on Earth —&amp;nbsp;and are actively discussing sponsorships and partnerships,” says de Monchaux.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202406/katie-chun-momo-00_0.jpg?itok=wfHWIARm" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Yiwei Xue steps out of Momo, a semi-permanent in situ habitat designed for space exploration ahead of establishing a permanent base on the moon. ]]></media:description>
              <media:credit>Photo: Chenyue “xdd44” Dai</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/architecture">Architecture</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/nasa">NASA</category>
      <category domain="https://news.mit.edu/topic/spaceflight">Spaceflight</category>
      <category domain="https://news.mit.edu/topic/classes-and-programs">Classes and programs</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/students">Students</category>
      <category domain="https://news.mit.edu/topic/space">Space</category>
    </item>
<item>
  <title>Researchers use large language models to help robots navigate</title>
  <link>https://news.mit.edu/2024/researchers-use-large-language-models-to-help-robots-navigate-0612</link>
  <description><![CDATA[The method uses language-based inputs instead of costly visual data to direct a robot through a multistep navigation task.]]></description>
  <pubDate>Wed, 12 Jun 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/researchers-use-large-language-models-to-help-robots-navigate-0612</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Someday, you may want your home robot to carry a load of dirty clothes downstairs and deposit them in the washing machine in the far-left corner of the basement. The robot will need to combine your instructions with its visual observations to determine the steps it should take to complete this task.&lt;/p&gt;&lt;p&gt;For an AI agent, this is easier said than done. Current approaches often utilize multiple hand-crafted machine-learning models to tackle different parts of the task, which require a great deal of human effort and expertise to build. These methods, which use visual representations to directly make navigation decisions, demand massive amounts of visual data for training, which are often hard to come by.&lt;/p&gt;&lt;p&gt;To overcome these challenges, researchers from MIT and the MIT-IBM Watson AI Lab devised a navigation method that converts visual representations into pieces of language, which are then fed into one large language model that achieves all parts of the multistep navigation task.&lt;/p&gt;&lt;p&gt;Rather than encoding visual features from images of a robot’s surroundings as visual representations, which is computationally intensive, their method creates text captions that describe the robot’s point-of-view. A large language model uses the captions to predict the actions a robot should take to fulfill a user’s language-based instructions.&lt;/p&gt;&lt;p&gt;Because their method utilizes purely language-based representations, they can use a large language model to efficiently generate a huge amount of synthetic training data.&lt;/p&gt;&lt;p&gt;While this approach does not outperform techniques that use visual features, it performs well in situations that lack enough visual data for training. The researchers found that combining their language-based inputs with visual signals leads to better navigation performance.&lt;/p&gt;&lt;p&gt;“By purely using language as the perceptual representation, ours is a more straightforward approach. Since all the inputs can be encoded as language, we can generate a human-understandable trajectory,” says Bowen Pan, an electrical engineering and computer science (EECS) graduate student and lead author of a &lt;a href="https://arxiv.org/pdf/2310.07889" target="_blank"&gt;paper on this approach&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Pan’s co-authors include his advisor, Aude Oliva, director of strategic industry engagement at the MIT Schwarzman College of Computing, MIT director of the MIT-IBM Watson AI Lab, and a senior research scientist in the Computer Science and Artificial Intelligence Laboratory (CSAIL); Philip Isola, an associate professor of EECS and a member of CSAIL; senior author Yoon Kim, an assistant professor of EECS and a member of CSAIL; and others at the MIT-IBM Watson AI Lab and Dartmouth College. The research will be presented at the Conference of the North American Chapter of the Association for Computational Linguistics.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Solving a vision problem with language&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Since large language models are the most powerful machine-learning models available, the researchers sought to incorporate them into the complex task known as vision-and-language navigation, Pan says.&lt;/p&gt;&lt;p&gt;But such models take text-based inputs and can’t process visual data from a robot’s camera. So, the team needed to find a way to use language instead.&lt;/p&gt;&lt;p&gt;Their technique utilizes a simple captioning model to obtain text descriptions of a robot’s visual observations. These captions are combined with language-based instructions and fed into a large language model, which decides what navigation step the robot should take next.&lt;/p&gt;&lt;p&gt;The large language model outputs a caption of the scene the robot should see after completing that step. This is used to update the trajectory history so the robot can keep track of where it has been.&lt;/p&gt;&lt;p&gt;The model repeats these processes to generate a trajectory that guides the robot to its goal, one step at a time.&lt;/p&gt;&lt;p&gt;To streamline the process, the researchers designed templates so observation information is presented to the model in a standard form — as a series of choices the robot can make based on its surroundings.&lt;/p&gt;&lt;p&gt;For instance, a caption might say “to your 30-degree left is a door with a potted plant beside it, to your back is a small office with a desk and a computer,” etc. The model chooses whether the robot should move toward the door or the office.&lt;/p&gt;&lt;p&gt;“One of the biggest challenges was figuring out how to encode this kind of information into language in a proper way to make the agent understand what the task is and how they should respond,” Pan says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Advantages of language&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When they tested this approach, while it could not outperform vision-based techniques, they found that it offered several advantages.&lt;/p&gt;&lt;p&gt;First, because text requires fewer computational resources to synthesize than complex image data, their method can be used to rapidly generate synthetic training data. In one test, they generated 10,000 synthetic trajectories based on 10 real-world, visual trajectories.&lt;/p&gt;&lt;p&gt;The technique can also bridge the gap that can prevent an agent trained with a simulated environment from performing well in the real world. This gap often occurs because computer-generated images can appear quite different from real-world scenes due to elements like lighting or color. But language that describes a synthetic versus a real image would be much harder to tell apart, Pan says.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Also, the representations their model uses are easier for a human to understand because they are written in natural language.&lt;/p&gt;&lt;p&gt;“If the agent fails to reach its goal, we can more easily determine where it failed and why it failed. Maybe the history information is not clear enough or the observation ignores some important details,” Pan says.&lt;/p&gt;&lt;p&gt;In addition, their method could be applied more easily to varied tasks and environments because it uses only one type of input. As long as data can be encoded as language, they can use the same model without making any modifications.&lt;/p&gt;&lt;p&gt;But one disadvantage is that their method naturally loses some information that would be captured by vision-based models, such as depth information.&lt;/p&gt;&lt;p&gt;However, the researchers were surprised to see that combining language-based representations with vision-based methods improves an agent’s ability to navigate.&lt;/p&gt;&lt;p&gt;“Maybe this means that language can capture some higher-level information than cannot be captured with pure vision features,” he says.&lt;/p&gt;&lt;p&gt;This is one area the researchers want to continue exploring. They also want to develop a navigation-oriented captioner that could boost the method’s performance. In addition, they want to probe the ability of large language models to exhibit spatial awareness and see how this could aid language-based navigation.&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the MIT-IBM Watson AI Lab.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202406/MIT_LangNAV-01.jpg?itok=vdy3n1UY" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[A new navigation method uses language-based inputs to direct a robot through a multistep navigation task like doing laundry.]]></media:description>
              <media:credit>Credit: iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
    </item>
<item>
  <title>Helping robots grasp the unpredictable</title>
  <link>https://news.mit.edu/2024/helping-robots-grasp-unpredictable-0603</link>
  <description><![CDATA[MIT CSAIL’s frugal deep-learning model infers the hidden physical properties of objects, then adapts to find the most stable grasps for robots in unstructured environments like homes and fulfillment centers.]]></description>
  <pubDate>Mon, 03 Jun 2024 15:20:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/helping-robots-grasp-unpredictable-0603</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-a7b7b40c-7fff-662e-9296-024994db96ca"&gt;When robots come across unfamiliar objects, they struggle to account for a simple truth: Appearances aren’t everything. They may attempt to grasp a block, only to find out it’s a&amp;nbsp;&lt;a href="https://www.businessinsider.com/cake-memes-twitter-fondant-realistic-tasty-viral-video-explained-2020-7"&gt;literal piece of cake&lt;/a&gt;. The misleading appearance of that object could lead the robot to miscalculate physical properties like the object’s weight and center of mass, using the wrong grasp and applying more force than needed.&lt;/p&gt;&lt;p dir="ltr"&gt;To see through this illusion, MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers designed the &lt;a href="http://groups.csail.mit.edu/rrg/papers/noseworthy_shaw_icra24.pdf" target="_blank"&gt;Grasping Neural Process&lt;/a&gt;, a predictive physics model capable of inferring these hidden traits in real time for more intelligent robotic grasping. Based on limited interaction data, their deep-learning system can assist robots in domains like warehouses and households at a fraction of the computational cost of previous algorithmic and statistical models.&lt;/p&gt;&lt;p dir="ltr"&gt;The Grasping Neural Process is trained to infer invisible physical properties from a history of attempted grasps, and uses the inferred properties to guess which grasps would work well in the future. Prior models often only identified robot grasps from visual data alone.&lt;/p&gt;&lt;p dir="ltr"&gt;Typically, methods that infer physical properties build on traditional statistical methods that require many known grasps and a great amount of computation time to work well. The Grasping Neural Process enables these machines to execute good grasps more efficiently by using far less interaction data and finishes its computation in less than a tenth of a second, as opposed seconds (or minutes) required by traditional methods.&lt;br&gt;&lt;br&gt;The researchers note that the Grasping Neural Process thrives in unstructured environments like homes and warehouses, since both house a plethora of unpredictable objects. For example, a robot powered by the MIT model could quickly learn how to handle tightly packed boxes with different food quantities without seeing the inside of the box, and then place them where needed. At a fulfillment center, objects with different physical properties and geometries would be placed in the corresponding box to be shipped out to customers.&lt;/p&gt;&lt;p dir="ltr"&gt;Trained on 1,000 unique geometries and 5,000 objects, the Grasping Neural Process achieved stable grasps in simulation for novel 3D objects generated in the ShapeNet repository. Then, the CSAIL-led group tested their model in the physical world via two weighted blocks, where their work outperformed a baseline that only considered object geometries. Limited to 10 experimental grasps beforehand, the robotic arm successfully picked up the boxes on 18 and 19 out of 20 attempts apiece, while the machine only yielded eight and 15 stable grasps when unprepared.&lt;/p&gt;&lt;p dir="ltr"&gt;While less theatrical than an actor, robots that complete inference tasks also have a three-part act to follow: training, adaptation, and testing. During the training step, robots practice on a fixed set of objects and learn how to infer physical properties from a history of successful (or unsuccessful) grasps. The new CSAIL model amortizes the inference of the objects’ physics, meaning it trains a neural network to learn to predict the output of an otherwise expensive statistical algorithm. Only a single pass through a neural network with limited interaction data is needed to simulate and predict which grasps work best on different objects.&lt;/p&gt;&lt;p dir="ltr"&gt;Then, the robot is introduced to an unfamiliar object during the adaptation phase. During this step, the Grasping Neural Process helps a robot experiment and update its position accordingly, understanding which grips would work best. This tinkering phase prepares the machine for the final step: testing, where the robot formally executes a task on an item with a new understanding of its properties.&lt;/p&gt;&lt;p dir="ltr"&gt;“As an engineer, it’s unwise to assume a robot knows all the necessary information it needs to grasp successfully,” says lead author Michael Noseworthy, an MIT PhD student in electrical engineering and computer science (EECS) and CSAIL affiliate. “Without humans labeling the properties of an object, robots have traditionally needed to use a costly inference process.” According to fellow lead author, EECS PhD student, and CSAIL affiliate Seiji Shaw, their Grasping Neural Process could be a streamlined alternative: “Our model helps robots do this much more efficiently, enabling the robot to imagine which grasps will inform the best result.”&amp;nbsp;&lt;br&gt;&lt;br&gt;“To get robots out of controlled spaces like the lab or warehouse and into the real world, they must be better at dealing with the unknown and less likely to fail at the slightest variation from their programming. This work is a critical step toward realizing the full transformative potential of robotics,” says Chad Kessens, an autonomous robotics researcher at the U.S. Army’s DEVCOM Army Research Laboratory, which sponsored the work.&lt;/p&gt;&lt;p dir="ltr"&gt;While their model can help a robot infer hidden static properties efficiently, the researchers would like to augment the system to adjust grasps in real time for multiple tasks and objects with dynamic traits. They envision their work eventually assisting with several tasks in a long-horizon plan, like picking up a carrot and chopping it. Moreover, their model could adapt to changes in mass distributions in less static objects, like when you fill up an empty bottle.&lt;br&gt;&lt;br&gt;Joining the researchers on the paper is Nicholas Roy, MIT professor of aeronautics and astronautics and CSAIL member, who is a senior author. The group recently &lt;a href="http://groups.csail.mit.edu/rrg/papers/noseworthy_shaw_icra24.pdf" target="_blank"&gt;presented this work&lt;/a&gt; at the IEEE International Conference on Robotics and Automation.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/grasping-neural-process.png?itok=wjDyX09T" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[The Grasping Neural Process uses limited interaction data to help robots understand unclear objects in real-time.]]></media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>A technique for more effective multipurpose robots</title>
  <link>https://news.mit.edu/2024/technique-for-more-effective-multipurpose-robots-0603</link>
  <description><![CDATA[With generative AI models, researchers combined robotics data from different sources to help robots learn better. ]]></description>
  <pubDate>Mon, 03 Jun 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/technique-for-more-effective-multipurpose-robots-0603</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Let’s say you want to train a robot so it understands how to use tools and can then quickly learn to make repairs around your house with a hammer, wrench, and screwdriver. To do that, you would need an enormous amount of data demonstrating tool use.&lt;/p&gt;&lt;p&gt;Existing robotic datasets vary widely in modality — some include color images while others are composed of tactile imprints, for instance. Data could also be collected in different domains, like simulation or human demos. And each dataset may capture a unique task and environment.&lt;/p&gt;&lt;p&gt;It is difficult to efficiently incorporate data from so many sources in one machine-learning model, so many methods use just one type of data to train a robot. But robots trained this way, with a relatively small amount of task-specific data, are often unable to perform new tasks in unfamiliar environments.&lt;/p&gt;&lt;p&gt;In an effort to train better multipurpose robots, MIT researchers developed a technique to combine multiple sources of data across domains, modalities, and tasks using a type of generative AI known as diffusion models.&lt;/p&gt;&lt;p&gt;They train a separate diffusion model to learn a strategy, or policy, for completing one task using one specific dataset. Then they combine the policies learned by the diffusion models into a general policy that enables a robot to perform multiple tasks in various settings.&lt;/p&gt;&lt;p&gt;In simulations and real-world experiments, this training approach enabled a robot to perform multiple tool-use tasks and adapt to new tasks it did not see during training. The method, known as Policy Composition (PoCo), led to a 20 percent improvement in task performance when compared to baseline techniques.&lt;/p&gt;&lt;p&gt;“Addressing heterogeneity in robotic datasets is like a chicken-egg problem. If we want to use a lot of data to train general robot policies, then we first need deployable robots to get all this data. I think that leveraging all the heterogeneous data available, similar to what researchers have done with ChatGPT, is an important step for the robotics field,” says Lirui Wang, an electrical engineering and computer science (EECS) graduate student and lead author of a &lt;a href="https://arxiv.org/pdf/2402.02511" target="_blank"&gt;paper on PoCo&lt;/a&gt;.&lt;w:sdtpr&gt;&lt;/w:sdtpr&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;p&gt;Wang’s coauthors include Jialiang Zhao, a mechanical engineering graduate student; Yilun Du, an EECS graduate student; Edward Adelson, the John and Dorothy Wilson Professor of Vision Science in the Department of Brain and Cognitive Sciences and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author Russ Tedrake, the Toyota Professor of EECS, Aeronautics and Astronautics, and Mechanical Engineering, and a member of CSAIL. The research will be presented at the Robotics: Science and Systems Conference.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Combining disparate datasets&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A robotic policy is a machine-learning model that takes inputs and uses them to perform an action. One way to think about a policy is as a strategy. In the case of a robotic arm, that strategy might be a trajectory, or a series of poses that move the arm so it picks up a hammer and uses it to pound a nail.&lt;/p&gt;&lt;p&gt;Datasets used to learn robotic policies are typically small and focused on one particular task and environment, like packing items into boxes in a warehouse.&lt;/p&gt;&lt;p&gt;“Every single robotic warehouse is generating terabytes of data, but it only belongs to that specific robot installation working on those packages. It is not ideal if you want to use all of these data to train a general machine,” Wang says.&lt;/p&gt;&lt;p&gt;The MIT researchers developed a technique that can take a series of smaller datasets, like those gathered from many robotic warehouses, learn separate policies from each one, and combine the policies in a way that enables a robot to generalize to many tasks.&lt;/p&gt;&lt;p&gt;They represent each policy using a type of generative AI model known as a diffusion model. Diffusion models, often used for image generation, learn to create new data samples that resemble samples in a training dataset by iteratively refining their output.&lt;/p&gt;&lt;p&gt;But rather than teaching a diffusion model to generate images, the researchers teach it to generate a trajectory for a robot. They do this by adding noise to the trajectories in a training dataset. The diffusion model gradually removes the noise and refines its output into a trajectory.&lt;/p&gt;&lt;p&gt;This technique, known as &lt;a href="https://arxiv.org/pdf/2303.04137.pdf" target="_blank"&gt;Diffusion Policy&lt;/a&gt;, was previously introduced by researchers at MIT, Columbia University, and the Toyota Research Institute. PoCo builds off this Diffusion Policy work.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The team trains each diffusion model with a different type of dataset, such as one with human video demonstrations and another gleaned from teleoperation of a robotic arm.&lt;/p&gt;&lt;p&gt;Then the researchers perform a weighted combination of the individual policies learned by all the diffusion models, iteratively refining the output so the combined policy satisfies the objectives of each individual policy.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Greater than the sum of its parts&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“One of the benefits of this approach is that we can combine policies to get the best of both worlds. For instance, a policy trained on real-world data might be able to achieve more dexterity, while a policy trained on simulation might be able to achieve more generalization,” Wang says.&lt;/p&gt;&lt;img src="/sites/default/files/images/inline/spatula_0.gif" data-align="center" data-entity-uuid="be482964-50e1-4348-a78c-617ff01da27f" data-entity-type="file" alt="Animation of robot arm using a spatula to lift toy pancake" width="300" height="247" data-caption="With policy composition, researchers are able to combine datasets from multiple sources so they can teach a robot to effectively use a wide range of tools, like a hammer, screwdriver, or this spatula.&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Image: Courtesy of the researchers"&gt;&lt;p&gt;Because the policies are trained separately, one could mix and match diffusion policies to achieve better results for a certain task. A user could also add data in a new modality or domain by training an additional Diffusion Policy with that dataset, rather than starting the entire process from scratch.&lt;/p&gt;&lt;img src="/sites/default/files/images/inline/hammer_0.gif" data-align="center" data-entity-uuid="767d5173-0540-42c6-bd46-842d3999215c" data-entity-type="file" alt="Animation of robot arm using toy hammer as objects are being placed randomly next around it." width="300" height="296" data-caption="The policy composition technique the researchers developed can be used to effectively teach a robot to use tools even when objects are placed around it to try and distract it from its task, as seen here.&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Image: Courtesy of the researchers"&gt;&lt;p&gt;The researchers tested PoCo in simulation and on real robotic arms that performed a variety of tools tasks, such as using a hammer to pound a nail and flipping an object with a spatula. PoCo led to a 20 percent improvement in task performance compared to baseline methods.&lt;/p&gt;&lt;p&gt;“The striking thing was that when we finished tuning and visualized it, we can clearly see that the composed trajectory looks much better than either one of them individually,” Wang says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to apply this technique to long-horizon tasks where a robot would pick up one tool, use it, then switch to another tool. They also want to incorporate larger robotics datasets to improve performance.&lt;/p&gt;&lt;p&gt;“We will need all three kinds of data to succeed for robotics: internet data, simulation data, and real robot data. How to combine them effectively will be the million-dollar question. PoCo is a solid step on the right track,” says Jim Fan, senior research scientist at NVIDIA and leader of the AI Agents Initiative, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research is funded, in part, by Amazon, the Singapore Defense Science and Technology Agency, the U.S. National Science Foundation, and the Toyota Research Institute.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/MIT-Policy-Comp-A1.jpg?itok=wx82cdsY" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Three different data domains — simulation (top), robot tele-operation (middle) and human demos (bottom) — allow a robot to learn to use different tools.]]></media:description>
              <media:credit>Image: Courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/brain-cognitive">Brain and cognitive sciences</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
    </item>
<item>
  <title>School of Engineering welcomes new faculty</title>
  <link>https://news.mit.edu/2024/school-engineering-welcomes-new-faculty-0523</link>
  <description><![CDATA[Fifteen new faculty members join six of the school’s academic departments. ]]></description>
  <pubDate>Thu, 23 May 2024 14:35:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/school-engineering-welcomes-new-faculty-0523</guid>
        <dc:creator>Zach Goodale | School of Engineering</dc:creator>
  <content:encoded>&lt;p&gt;The School of Engineering welcomes 15 new faculty members across six of its academic departments. This new cohort of faculty members, who have either recently started their roles at MIT or will start within the next year, conduct research across a diverse range of disciplines.&lt;/p&gt;&lt;p&gt;Many of these new faculty specialize in research that intersects with multiple fields. In addition to positions in the School of Engineering, a number of these faculty have positions at other units across MIT. Faculty with appointments in the Department of Electrical Engineering and Computer Science (EECS) report into both the School of Engineering and the MIT Stephen A. Schwarzman College of Computing. This year, new faculty also have joint appointments between the School of Engineering and the School of Humanities, Arts, and Social Sciences and the School of Science.&lt;/p&gt;&lt;p&gt;“I am delighted to welcome this cohort of talented new faculty to the School of Engineering,” says Anantha Chandrakasan, chief innovation and strategy officer, dean of engineering, and Vannevar Bush Professor of Electrical Engineering and Computer Science. “I am particularly struck by the interdisciplinary approach many of these new faculty take in their research. They are working in areas that are poised to have tremendous impact. I look forward to seeing them grow as researchers and educators.”&lt;/p&gt;&lt;p&gt;The new engineering faculty include:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Stephen Bates&lt;/strong&gt; joined the Department of Electrical Engineering and Computer Science as an assistant professor in September 2023. He is also a member of the Laboratory for Information and Decision Systems (LIDS). Bates uses data and AI for reliable decision-making in the presence of uncertainty. In particular, he develops tools for statistical inference with AI models, data impacted by strategic behavior, and settings with distribution shift. Bates also works on applications in life sciences and sustainability. He previously worked as a postdoc in the Statistics and EECS departments at the University of California at Berkeley (UC Berkeley). Bates received a BS in statistics and mathematics at Harvard University and a PhD from Stanford University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Abigail Bodner&lt;/strong&gt; joined the Department of EECS and Department of Earth, Atmospheric and Planetary Sciences as an assistant professor in January. She is also a member of the LIDS. Bodner’s research interests span climate, physical oceanography, geophysical fluid dynamics, and turbulence. Previously, she worked as a Simons Junior Fellow at the&amp;nbsp;Courant Institute&amp;nbsp;of Mathematical Sciences at New York University. Bodner received her BS in geophysics and mathematics and MS in geophysics from Tel Aviv University, and her SM in applied mathematics and PhD from Brown University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Andreea Bobu&lt;/strong&gt; ’17 will join the Department of Aeronautics and Astronautics as an assistant professor in July. Her research sits at the intersection of robotics, mathematical human modeling, and deep learning. Previously, she was a research scientist at the Boston Dynamics AI Institute, focusing on how robots and humans can efficiently arrive at shared representations of their tasks for more seamless and reliable interactions. Bobu earned a BS in computer science and engineering from MIT and a PhD in electrical engineering and computer science from UC Berkeley.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Suraj Cheema&lt;/strong&gt; will join the Department of Materials Science and Engineering, with a joint appointment in the Department of EECS, as an assistant professor in July. His research explores atomic-scale engineering of electronic materials to tackle challenges related to energy consumption, storage, and generation, aiming for more sustainable microelectronics. This spans computing and energy technologies via integrated ferroelectric devices. He previously worked as a postdoc at UC Berkeley. Cheema earned a BS in applied physics and applied mathematics from Columbia University and a PhD in materials science and engineering from UC Berkeley.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Samantha Coday&lt;/strong&gt; joins the Department of EECS as an assistant professor in July. She will also be a member of the MIT Research Laboratory of Electronics. Her research interests include ultra-dense power converters enabling renewable energy integration, hybrid electric aircraft and future space exploration. To enable high-performance converters for these critical applications her research focuses on the optimization, design, and control of hybrid switched-capacitor converters. Coday earned a BS in electrical engineering and mathematics from Southern Methodist University and an MS and a PhD in electrical engineering and computer science from UC Berkeley.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mitchell Gordon&lt;/strong&gt; will join the Department of EECS as an assistant professor in July 2025. He will also be a member of the MIT Computer Science and Artificial Intelligence Laboratory. In his research, Gordon designs interactive systems and evaluation approaches that bridge principles of human-computer interaction with the realities of machine learning. He currently works as a postdoc at the University of Washington. Gordon received a BS from the University of Rochester, and MS and PhD from Stanford University, all in computer science.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Kaiming He&lt;/strong&gt; joined the Department of EECS as an associate professor in February. He will also be a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). His research interests cover a wide range of topics in computer vision and deep learning. He is currently focused on building computer models that can learn representations and develop intelligence from and for the complex world. Long term, he hopes to augment human intelligence with improved artificial intelligence. Before joining MIT, He was a research scientist at Facebook AI. He earned a BS from Tsinghua University and a PhD from the Chinese University of Hong Kong.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Anna Huang&lt;/strong&gt; SM ’08 will join the departments of EECS and Music and Theater Arts as assistant professor in September. She will help develop graduate programming focused on music technology. Previously, she spent eight years with Magenta at Google Brain and DeepMind, spearheading efforts in generative modeling, reinforcement learning, and human-computer interaction to support human-AI partnerships in music-making. She is the creator of Music Transformer and Coconet (which powered the Bach Google Doodle). She was a judge and organizer for the AI Song Contest. Anna holds a Canada CIFAR AI Chair at Mila, a BM in music composition, and BS in computer science from the University of Southern California, an MS from the MIT Media Lab, and a PhD from Harvard University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Yael Kalai&lt;/strong&gt; PhD ’06 will join the Department of EECS as a professor in September. She is also a member of CSAIL. Her research interests include cryptography, the theory of computation, and security and privacy. Kalai currently focuses on both the theoretical and real-world applications of cryptography, including work on succinct and easily verifiable non-interactive proofs.&amp;nbsp;She received her bachelor’s degree from the Hebrew University of Jerusalem, a master’s degree at the Weizmann Institute of Science, and a PhD from MIT.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Sendhil Mullainathan&lt;/strong&gt; will join the departments of EECS and Economics as a professor in July. His research uses machine learning to understand complex problems in human behavior, social policy, and medicine. Previously, Mullainathan spent five years at MIT before joining the faculty at Harvard in 2004, and then the University of Chicago in 2018. He received his BA in computer science, mathematics, and economics from Cornell University and his PhD from Harvard University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Alex Rives&amp;nbsp;&lt;/strong&gt;will join the Department of EECS as an assistant professor in September, with a core membership in the Broad Institute of MIT and Harvard. In his research, Rives is focused on AI for scientific understanding, discovery, and design for biology. Rives worked with Meta as a New York University graduate student, where he founded and led the Evolutionary Scale Modeling team that developed large language models for proteins. Rives received his BS in philosophy and biology from Yale University and is completing his PhD in computer science at NYU.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Sungho Shin&lt;/strong&gt; will join the Department of Chemical Engineering as an assistant professor in July. His research interests include control theory, optimization algorithms, high-performance computing, and their applications to decision-making in complex systems, such as energy infrastructures. Shin is a postdoc at the Mathematics and Computer Science Division at Argonne National Laboratory. He received a BS in mathematics and chemical engineering from Seoul National University and a PhD in chemical engineering from the University of Wisconsin-Madison.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Jessica Stark&lt;/strong&gt; joined the Department of Biological Engineering as an assistant professor in&amp;nbsp;January. In her research, Stark is developing technologies to realize the largely untapped potential of cell-surface sugars, called glycans, for immunological discovery and immunotherapy. Previously, Stark was an American Cancer Society postdoc at Stanford University. She earned a BS in chemical and biomolecular engineering from Cornell University and a PhD in chemical and biological engineering at Northwestern University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thomas John “T.J.” Wallin&lt;/strong&gt; joined the Department of Materials Science and Engineering as an assistant professor in January. As a researcher, Wallin’s interests lay in advanced manufacturing of functional soft matter, with an emphasis on soft wearable technologies and their applications in human-computer interfaces. Previously, he was a research scientist at Meta’s Reality Labs Research working in their haptic interaction team. Wallin earned a BS in physics and chemistry from the College of William and Mary, and an MS and PhD in materials science and engineering from Cornell University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Gioele Zardini&lt;/strong&gt; joined the Department of Civil and Environmental Engineering as an assistant professor in September. He will also join LIDS and the Institute for Data, Systems, and Society. Driven by societal challenges, Zardini’s research interests include the co-design of sociotechnical systems, compositionality in engineering, applied category theory, decision and control, optimization, and game theory, with society-critical applications to intelligent transportation systems, autonomy, and complex networks and infrastructures. He received his BS, MS, and PhD in mechanical engineering with a focus on robotics, systems, and control from ETH Zurich, and spent time at MIT, Stanford University, and Motional.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/mit-soe-faculty-2024.jpg?itok=eM1_bIIV" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Top row, left to right: Stephen Bates, Abigail Bodner, Andreea Bobu, Suraj Cheema, and Samantha Coday. Middle row, left to right: Mitchell Gordon, Kaiming He, Anna Huang, Yael Kalai, and Sendhil Mullainathan. Bottom row, left to right: Alex Rives, Sungho Shin, Jessica Stark, Thomas John "T.J." Wallin, and Gioele Zardini.]]></media:description>
          </media:content>
        <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/biological-engineering">Biological engineering</category>
      <category domain="https://news.mit.edu/topic/chemical-engineering">Chemical engineering</category>
      <category domain="https://news.mit.edu/topic/civil-engineering">Civil and environmental engineering</category>
      <category domain="https://news.mit.edu/topic/economics">Economics</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/dmse">DMSE</category>
      <category domain="https://news.mit.edu/topic/music-and-theater-arts">Music and theater arts</category>
      <category domain="https://news.mit.edu/topic/broad-institute">Broad Institute</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/research-laboratory-electronics-1">Research Laboratory of Electronics</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/data">Data</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/electronics">Electronics</category>
      <category domain="https://news.mit.edu/topic/environment">Environment</category>
      <category domain="https://news.mit.edu/topic/medicine">Medicine</category>
      <category domain="https://news.mit.edu/topic/immunology">Immunology</category>
      <category domain="https://news.mit.edu/topic/music2">Music</category>
      <category domain="https://news.mit.edu/topic/cryptography">Cryptography</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
    </item>
<item>
  <title>MIT scientists learn how to control muscles with light</title>
  <link>https://news.mit.edu/2024/mit-scientists-learn-to-control-muscles-with-light-0522</link>
  <description><![CDATA[A new study suggests optogenetics can drive muscle contraction with greater control and less fatigue than electrical stimulation.]]></description>
  <pubDate>Wed, 22 May 2024 14:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/mit-scientists-learn-to-control-muscles-with-light-0522</guid>
        <dc:creator>Anne Trafton | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;For people with paralysis or amputation, neuroprosthetic systems that artificially stimulate muscle contraction with electrical current can help them regain limb function. However, despite many years of research, this type of prosthesis is not widely used because it leads to rapid muscle fatigue and poor control.&lt;/p&gt;&lt;p&gt;MIT researchers have developed a new approach that they hope could someday offer better muscle control with less fatigue. Instead of using electricity to stimulate muscles, they used light. In a study in mice, the researchers showed that this optogenetic technique offers more precise muscle control, along with a dramatic decrease in fatigue.&lt;/p&gt;&lt;p&gt;“It turns out that by using light, through optogenetics, one can control muscle more naturally. In terms of clinical application, this type of interface could have very broad utility,” says Hugh Herr, a professor of media arts and sciences, co-director of the K. Lisa Yang Center for Bionics at MIT, and an associate member of MIT’s McGovern Institute for Brain Research.&lt;/p&gt;&lt;p&gt;Optogenetics is a method based on genetically engineering cells to express light-sensitive proteins, which allows researchers to control activity of those cells by exposing them to light. This approach is currently not feasible in humans, but Herr, MIT graduate student Guillermo Herrera-Arcos, and their colleagues at the K. Lisa Yang Center for Bionics are now working on ways to deliver light-sensitive proteins&amp;nbsp;safely and effectively into human tissue.&lt;/p&gt;&lt;p&gt;Herr is the senior author of the study, which &lt;a href="https://www.science.org/doi/10.1126/scirobotics.adi8995" target="_blank"&gt;appears today&lt;/a&gt; in &lt;em&gt;Science Robotics&lt;/em&gt;. Herrera-Arcos is the lead author of the paper.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Optogenetic control&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For decades, researchers have been exploring the use of functional electrical stimulation (FES) to control muscles in the body. This method involves implanting electrodes that stimulate nerve fibers, causing a muscle to contract. However, this stimulation tends to activate the entire muscle at once, which is not the way that the human body naturally controls muscle contraction.&lt;/p&gt;&lt;p&gt;“Humans have this incredible control fidelity that is achieved by a natural recruitment of the muscle, where small motor units, then moderate-sized, then large motor units are recruited, in that order, as signal strength is increased,” Herr says. “With FES, when you artificially blast the muscle with electricity, the largest units are recruited first. So, as you increase signal, you get no force at the beginning, and then suddenly you get too much force.”&lt;/p&gt;&lt;p&gt;This large force not only makes it harder to achieve fine muscle control, it also wears out the muscle quickly, within five or 10 minutes.&lt;/p&gt;&lt;p&gt;The MIT team wanted to see if they could replace that entire interface with something different. Instead of electrodes, they decided to try controlling muscle contraction using optical molecular machines via optogenetics.&lt;/p&gt;&lt;p&gt;Using mice as an animal model, the researchers compared the amount of muscle force they could generate using the traditional FES approach with forces generated by their optogenetic method. For the optogenetic studies, they used mice that had already been genetically engineered to express a light-sensitive protein called channelrhodopsin-2. They implanted a small light source near the tibial nerve, which controls muscles of the lower leg.&lt;/p&gt;&lt;p&gt;The researchers measured muscle force as they gradually increased the amount of light stimulation, and found that, unlike FES stimulation, optogenetic control produced a steady, gradual increase in contraction of the muscle.&lt;/p&gt;&lt;p&gt;“As we change the optical stimulation that we deliver to the nerve, we can proportionally, in an almost linear way, control the force of the muscle. This is similar to how the signals from our brain control our muscles. Because of this, it becomes easier to control the muscle compared with electrical stimulation,” Herrera-Arcos says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Fatigue resistance&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Using data from those experiments, the researchers created a mathematical model of optogenetic muscle control. This model relates the amount of light going into the system to the output of the muscle (how much force is generated).&lt;/p&gt;&lt;p&gt;This mathematical model allowed the researchers to design a closed-loop controller. In this type of system, the controller delivers a stimulatory signal, and after the muscle contracts, a sensor can detect how much force the muscle is exerting. This information is sent back to the controller, which calculates if, and how much, the light stimulation needs to be adjusted to reach the desired force.&lt;/p&gt;&lt;p&gt;Using this type of control, the researchers found that muscles could be stimulated for more than an hour before fatiguing, while muscles became fatigued after only 15 minutes using FES stimulation.&lt;/p&gt;&lt;p&gt;One hurdle the researchers are now working to overcome is how to safely deliver light-sensitive proteins into human tissue. Several years ago, Herr’s lab reported that in rats, these proteins can trigger an immune response that inactivates the proteins and could also lead to muscle atrophy and cell death.&lt;/p&gt;&lt;p&gt;“A key objective of the K. Lisa Yang Center for Bionics is to solve that problem,” Herr says. “A multipronged effort is underway to design new light-sensitive proteins, and strategies to deliver them, without triggering an immune response.”&lt;/p&gt;&lt;p&gt;As additional steps toward reaching human patients, Herr’s lab is also working on new sensors that can be used to measure muscle force and length, as well as new ways to implant the light source. If successful, the researchers hope their strategy could benefit people who have experienced strokes, limb amputation, and spinal cord injuries, as well as others who have impaired ability to control their limbs.&lt;/p&gt;&lt;p&gt;“This could lead to a minimally invasive strategy that would change the game in terms of clinical care for persons suffering from limb pathology,” Herr says.&lt;/p&gt;&lt;p&gt;The research was funded by the K. Lisa Yang Center for Bionics at MIT.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/MIT-Optogenetic-Control-01-press.jpg?itok=Oyow5XMW" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT researchers have developed a way to help people with amputation or paralysis regain limb control. Instead of using electricity to stimulate muscles, they used light. Here, Guillermo Herrera-Arcos looks at light shining from an optical neurostimulator.]]></media:description>
              <media:credit>Photo: Steph Stevens</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/assistive-technology">Assistive technology</category>
      <category domain="https://news.mit.edu/topic/prosthetics">Prosthetics</category>
      <category domain="https://news.mit.edu/topic/neuroscience">Neuroscience</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/optogenetics">Optogenetics</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/disabilities">Disabilities</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/mcgovern-institute-0">McGovern Institute</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
    </item>
<item>
  <title>2024 MAD Design Fellows announced</title>
  <link>https://news.mit.edu/2024/mad-design-fellows-announced-0521</link>
  <description><![CDATA[The 10 Design Fellows are MIT graduate students working at the intersection of design and multiple disciplines across the Institute. ]]></description>
  <pubDate>Tue, 21 May 2024 15:40:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/mad-design-fellows-announced-0521</guid>
        <dc:creator>Adelaide Zollinger | MIT Morningside Academy for Design</dc:creator>
  <content:encoded>&lt;p&gt;Since its launch in 2022, the MIT Morningside Academy for Design (MAD) has supported MIT graduate students with a &lt;a href="https://design.mit.edu/about/design-courses" rel="noopener"&gt;fellowship&lt;/a&gt;, allowing recipients to pursue design research and projects while creating community. Pulling from different corners of design, they explore solutions in fields such as sustainability, health, architecture, urban planning, engineering, and social justice.&amp;nbsp;&lt;/p&gt;&lt;p&gt;On May 1, MAD announced the &lt;a href="https://design.mit.edu/people/fellows" rel="noopener"&gt;2024 cohort of Design Fellows&lt;/a&gt; at the MIT Museum.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/sofia-chiappero"&gt;&lt;strong&gt;Sofia Chiappero&lt;/strong&gt;&lt;/a&gt;, MCP student in the &lt;a href="https://dusp.mit.edu/" target="_blank" rel="noopener"&gt;Department of Urban Studies and Planning&lt;/a&gt; and &lt;a href="https://designx.mit.edu/" target="_blank" rel="noopener"&gt;MITdesignX&lt;/a&gt; affiliate: Chiappero is working around the intersection of community development and technology, aiming to address the challenges faced by underserved communities at risk of displacement in Latin America. Through a blend of social science and digital inclusion, she seeks to design a new approach to researching human interactions and replicating them in virtual settings, with the ultimate goal of preserving the identity of these communities and giving them visibility for resilient growth.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/clemence-couteau"&gt;&lt;strong&gt;Clemence Couteau&lt;/strong&gt;&lt;/a&gt;, MBA candidate in the &lt;a href="https://mitsloan.mit.edu/" target="_blank" rel="noopener"&gt;MIT Sloan School of Management&lt;/a&gt;: Couteau is tackling the rise of postpartum depression among U.S. mothers by aiming to develop a digital solution empowering at-risk pregnant women to improve mental health outcomes. This involves a self-directed therapy chatbot in a mobile app, based on the “ROSE” protocol.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/mateo-fernandez"&gt;&lt;strong&gt;Mateo Fernandez&lt;/strong&gt;&lt;/a&gt;, MArch student in the &lt;a href="https://architecture.mit.edu/" target="_blank" rel="noopener"&gt;Department of Architecture&lt;/a&gt;: Fernandez explores how to depart from the current construction industry, designing alternatives such as growing buildings with biomaterials, and deploying advanced 3D printing technologies for building.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/charlotte-folinus"&gt;&lt;strong&gt;Charlotte Folinus&lt;/strong&gt;&lt;/a&gt;, PhD candidate in the &lt;a href="https://meche.mit.edu/" target="_blank" rel="noopener"&gt;Department of Mechanical Engineering&lt;/a&gt;: Folinus creates new methods for designing soft robots, using these tools to design soft robots for gentle interactions, uncertain environments, and long mechanical lifetimes. “I am really excited to be surrounded by people who can do things I cannot. That's when I'm the best version of myself. I think that's the community I'll find here,” she says.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/alexander-htet-kyaw"&gt;&lt;strong&gt;Alexander Htet Kyaw&lt;/strong&gt;&lt;/a&gt;, master's student in the &lt;a href="https://architecture.mit.edu/" target="_blank" rel="noopener"&gt;Department of Architecture&lt;/a&gt; and the &lt;a href="https://www.eecs.mit.edu/" target="_blank" rel="noopener"&gt;Department of Electrical Engineering and Computer Science&lt;/a&gt; and &lt;a href="https://designx.mit.edu/" target="_blank" rel="noopener"&gt;MITdesignX&lt;/a&gt; affiliate: Htet Kyaw's current research utilizes robotic assembly, multimodal interaction, and generative AI&amp;nbsp;to challenge conventional manufacturing and fabrication practices. He is working on an AI-driven workflow that translates design intent into tangible objects through robotic assembly.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/deni-lopez"&gt;&lt;strong&gt;Dení López&lt;/strong&gt;&lt;/a&gt; PhD candidate in the &lt;a href="https://dusp.mit.edu/" target="_blank" rel="noopener"&gt;Department of Urban Studies and Planning&lt;/a&gt;: As a Design Fellow,&amp;nbsp;López&amp;nbsp;uses design research to evaluate and extend the scope of&amp;nbsp;Bicheeche Diidxa’,&amp;nbsp;a long-standing Participatory Action Research initiative for disaster resilience focused on five Zapotec communities along the Los Perros River in Oaxaca, Mexico.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/caitlin-morris"&gt;&lt;strong&gt;Caitlin Morris&lt;/strong&gt;&lt;/a&gt;, PhD candidate in &lt;a href="https://www.media.mit.edu/" target="_blank" rel="noopener"&gt;media arts and sciences&lt;/a&gt;: Morris’s research explores the role of multisensory influences on cognition and learning, and seeks to find and build the bridges between digital and computational interfaces and hands-on, community-centered learning and teaching practices.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/maxine-perroni-scharf"&gt;&lt;strong&gt;Maxine Perroni-Scharf&lt;/strong&gt;&lt;/a&gt;, PhD candidate in the &lt;a href="https://www.eecs.mit.edu/" target="_blank" rel="noopener"&gt;Department of Electrical Engineering and Computer Science&lt;/a&gt;: Perroni-Scharf is currently working on developing techniques that enable the discovery and design of extremal metamaterials — 3D printed materials that exhibit extreme properties arising not from their chemical composition, but rather from their structure. These can be applied to a variety of tasks, from battery design to accessibility.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/lyle-regenwetter"&gt;&lt;strong&gt;Lyle Regenwetter&lt;/strong&gt;&lt;/a&gt;, PhD candidate in the &lt;a href="https://meche.mit.edu/" target="_blank" rel="noopener"&gt;Department of Mechanical Engineering&lt;/a&gt;: Regenwetter develops methods to incorporate design requirements, such as safety constraints and performance objectives, into the training process of generative AI models.&lt;/p&gt;&lt;p&gt;&lt;a href="https://design.mit.edu/people/profile/zane-schemmer"&gt;&lt;strong&gt;Zane Schemmer&lt;/strong&gt;&lt;/a&gt;, PhD candidate in the &lt;a href="https://cee.mit.edu/" target="_blank" rel="noopener"&gt;Department of Civil and Environmental Engineering&lt;/a&gt;: Schemmer's research aims to minimize the carbon footprint of the built environment by designing efficient structures that consider the availability of local materials.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/MIT-MAD-Design-Fellows.jpg?itok=W84H3Kto" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Pulling from different corners of design, the Design Fellows supported by the MIT Morningside Academy for Design explore solutions in fields such as sustainability, health, architecture, urban planning, social justice, and education. ]]></media:description>
              <media:credit>Photos: Adelaide Zollinger</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/awards">Awards, honors and fellowships</category>
      <category domain="https://news.mit.edu/topic/architecture">Architecture</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/media-lab-0">Media Lab</category>
      <category domain="https://news.mit.edu/topic/media">Media</category>
      <category domain="https://news.mit.edu/topic/mit-museum">MIT Museum</category>
      <category domain="https://news.mit.edu/topic/students">Students</category>
      <category domain="https://news.mit.edu/topic/graduate">Graduate, postdoctoral</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/innovation">Innovation and Entrepreneurship (I&amp;E)</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
      <category domain="https://news.mit.edu/topic/mit-sloan-school-management">MIT Sloan School of Management</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>From NASA to MIT to Formlabs</title>
  <link>https://news.mit.edu/2024/nasa-mit-formlabs-audrey-chen-0521</link>
  <description><![CDATA[Audrey Chen ’24 landed an internship at NASA before she was old enough to drive. Here’s her secret to success.]]></description>
  <pubDate>Tue, 21 May 2024 11:45:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/nasa-mit-formlabs-audrey-chen-0521</guid>
        <dc:creator>Sonny Oram | Edgerton Center</dc:creator>
  <content:encoded>&lt;p&gt;Audrey Chen ’24 lives by the philosophy that “a lot of opportunities only present themselves if you ask for them.” This approach has served her well, from becoming a NASA intern at 15 to running MIT’s autonomous boat team &lt;a href="https://arcturus.mit.edu/"&gt;Arcturus&lt;/a&gt; to entering a leadership position at 3D printing technology company Formlabs right out of undergrad.&lt;/p&gt;&lt;p&gt;Growing up in Los Angeles, Chen showed a strong aptitude and passion for engineering at a young age and skipped several grades in math. In her first year of high school, she saw a posting about the JPL Space Academy at NASA’s Jet Propulsion Lab. Though the program was for juniors and seniors, she inquired if they would make an exception for her and they agreed. By her junior year she was helping run the program as deputy.&lt;/p&gt;&lt;p&gt;But Chen didn’t stop there: She had dreams of interning at NASA. She asked her mentor and became a drone air traffic control researcher at NASA at 15. “I was not old enough to drive,” Chen says. “High school would end, the bell would ring, and I would put on my backpack and I would run down the street to JPL. Can you imagine you're the security guard at the gate of the Jet Propulsion Laboratory and a kid shows up for work?”&lt;/p&gt;&lt;p&gt;Chen worked on the Orbiting Arid Subsurfaces and Ice Sheet Sounder (OASIS) project, whose goal is&amp;nbsp;to find and examine freshwater aquifers and ice sheets. “It was very early in the mission, so I was doing system and objective definition,” Chen says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Next stop: MIT&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After graduating high school, Chen ventured across the country to explore her eclectic interests at MIT. When she wasn’t fulfilling the requirements for her mechanical engineering degree, she could be found leather crafting, glass blowing, or table welding in one of MIT’s makerspaces,&amp;nbsp;&lt;a href="https://www.instagram.com/p/CZzWnUru4dk/"&gt;documenting MIT student life&lt;/a&gt; with her camera (garnering the acclimation&amp;nbsp;“The Eyes of MIT” by MIT Admissions), working as a researcher sampling deep-sea sediment, or&amp;nbsp;notably, running the award-winning&amp;nbsp;autonomous boat team&amp;nbsp;&lt;a href="https://arcturus.mit.edu/"&gt;Arcturus&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;“Arcturus has been the highlight of my MIT career,” Chen says. She founded the team at&amp;nbsp;&lt;a href="https://seagrant.mit.edu/seaperch2/"&gt;MIT Sea Grant&lt;/a&gt; in 2022 along with a group of equally impassioned students who elected Chen as captain.&lt;/p&gt;&lt;p&gt;“I didn't have any background in marine autonomy, so we pushed very hard to institute trainings and have lots of workshops so that they would feel comfortable coming in and contributing as soon as possible,” she recalls. Seeking additional funding and support, the team found a home at the&amp;nbsp;&lt;a href="https://edgerton.mit.edu/"&gt;MIT Edgerton Center&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Launching Arcturus&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“Whenever I think about how Arcturus started and how it somehow still continues, I think it’s a miracle,” Chen says. “Our very first year, there were five of us at the Roboboat competition, and if any individual one of us had not decided to join the team, we either would not have a boat, we would not have electronics, we would not have code to run the boat, or we wouldn’t have funding to run the team.”&lt;/p&gt;&lt;p&gt;Chen’s first year as captain was a tremendous amount of work because the team was so small. In addition to managing the team and assuring they met their goals on time, Chen also acted as the team’s business lead, treasurer, media lead, and photographer. “I was juggling a lot of things. Since then, those roles have further split amongst more people within the team,” she says.&lt;/p&gt;&lt;p&gt;Recruiting isn’t easy for an autonomous boat team, as many students don’t get marine robotics experience in high school.&amp;nbsp;To keep their recruitment pool wide, Chen didn’t expect students to have background in autonomy or in marine systems. “Creating an environment that’s welcoming and friendly and supportive of people’s learning is crucial, because otherwise you won’t have a team. We’ve really pushed hard to recruit from a large body of people. We make sure to emphasize that we’re open to all majors, all years. As an industry, marine robotics, like most engineering, is very male-dominated. We work hard to recruit people of all genders and ethnicities.”&lt;/p&gt;&lt;p&gt;With Chen’s skillful recruiting, Arcturus increased from five to 74 members in 2024.&amp;nbsp;Arcturus flourished under Chen’s leadership, winning&amp;nbsp;First Place Design Overall at the&amp;nbsp;Roboboat competition in 2023.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The challenges with autonomous boats&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Chen was drawn to autonomous boats because the field is so full of potential. “You leave a robot on land and you turn it off, it doesn't move by itself, versus you put it in a body of water and you don't do anything, then it still moves because of the currents. It needs to be constantly taking in that input and trying to localize where it is,” Chen says.&lt;/p&gt;&lt;p&gt;Chen sees a lot of potential in the marine biotics industry to gather crucial data about our environment. “Autonomy in the marine space is not as well researched as land autonomy is. There’s immense potential for marine autonomy to benefit the world. You think about mapping ocean topology or looking for endangered species or habitat protection or surveying bleached coral reefs. As a vehicle, you have more flexibility to move around versus a buoy. That gives you the ability to take water and sediment samples across a wider spread of area. And by making it autonomous, you eliminate high labor costs, so the price per sample for a researcher would go down. These are different ways in which autonomy has potential to benefit the research sphere, but also, more broadly, the world.”&lt;/p&gt;&lt;p&gt;Chen graduated early this past February and passed Arcturus on to captains and rising juniors Ami Shi and Karen Guo. “They’re rock stars. The team is in good hands,” Chen says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Becoming a project manager at Formlabs&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Chen graduated a semester early and accepted a&amp;nbsp;project manager position&amp;nbsp;at Formlabs. She brings many lessons from MIT to her work. “The biggest thing that I’ve learned is that I don’t need to know everything. Part of being successful is knowing what you don’t know. So I’m always aware that in every Arcturus meeting, and probably every technical meeting that I’ll be in at Formlabs, that I will not be the smartest person in the room. And that’s fine. I don’t need to be the smartest person ever because that’s not my job. My job is to bring these projects together and know enough about all the systems to integrate them.”&lt;/p&gt;&lt;p&gt;Chen is thrilled to stay near MIT after graduation, allowing her the opportunity to visit her friends and continue mentoring Arcturus. Upon announcing her new job, she remarked, “To my friends at MIT, I’ll be just down the street, so you won’t be able to get rid of me that easily!”&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/audrey-chen-mit-00_0.jpg?itok=6JPo8UK5" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[As a photographer, Audrey Chen became known as the "The Eyes of MIT" by MIT Admissions.]]></media:description>
              <media:credit>Photo: Tony Pulsone</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/admissions">Admissions</category>
      <category domain="https://news.mit.edu/topic/edgerton">Edgerton</category>
      <category domain="https://news.mit.edu/topic/mit-sea-grant">MIT Sea Grant</category>
      <category domain="https://news.mit.edu/topic/oceans">Oceanography and ocean engineering</category>
      <category domain="https://news.mit.edu/topic/profile">Profile</category>
      <category domain="https://news.mit.edu/topic/nasa">NASA</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/clubs-and-activities">Clubs and activities</category>
      <category domain="https://news.mit.edu/topic/equity-and-inclusion">Equity and inclusion</category>
    </item>
<item>
  <title>Robotic palm mimics human touch</title>
  <link>https://news.mit.edu/2024/robotic-palm-mimics-human-touch-0520</link>
  <description><![CDATA[MIT CSAIL researchers enhance robotic precision with sophisticated tactile sensors in the palm and agile fingers, setting the stage for improvements in human-robot interaction and prosthetic technology.]]></description>
  <pubDate>Mon, 20 May 2024 15:50:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/robotic-palm-mimics-human-touch-0520</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p dir="ltr" id="docs-internal-guid-2f41b63b-7fff-a2e7-4354-f02aa321862d"&gt;“I'll have you eating out of the palm of my hand” is an unlikely utterance you'll hear from a robot. Why? Most of them don't have palms.&lt;/p&gt;&lt;p dir="ltr"&gt;If you have kept up with the protean field, gripping and grasping more like humans has been an ongoing Herculean effort. Now, a new robotic hand design developed in MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) has rethought the oft-overlooked palm. The new design uses advanced sensors for a highly sensitive touch, helping the “extremity” handle objects with more detailed and delicate precision.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;a href="https://arxiv.org/pdf/2404.08227" target="_blank"&gt;GelPalm&lt;/a&gt; has a gel-based, flexible sensor embedded in the palm, drawing inspiration from the soft, deformable nature of human hands. The sensor uses a special color illumination tech that uses red, green, and blue LEDs to light an object, and a camera to capture reflections. This mixture generates detailed 3D surface models for precise robotic interactions.&lt;/p&gt;&lt;p dir="ltr"&gt;And what would the palm be without its facilitative fingers? The team also developed some robotic phalanges, called ROMEO (“RObotic Modular Endoskeleton Optical”), with flexible materials and similar sensing technology as the palm. The fingers have something called “passive compliance,” which is when a robot can adjust to forces naturally, without needing motors or extra control. This in turn helps with the larger objective: increasing the surface area in contact with objects so they can be fully enveloped. Manufactured as single, monolithic structures via 3D printing, the finger designs are a cost-effective production.&lt;/p&gt;&lt;p dir="ltr"&gt;Beyond improved dexterity, GelPalm offers safer interaction with objects, something that’s especially handy for potential applications like human-robot collaboration, prosthetics, or robotic hands with human-like sensing for biomedical uses.&lt;/p&gt;&lt;p dir="ltr"&gt;Many previous robotic designs have typically focused on enhancing finger dexterity. Liu's approach shifts the focus to create a more human-like, versatile end effector that interacts more naturally with objects and performs a broader range of tasks.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We draw inspiration from human hands, which have rigid bones surrounded by soft, compliant tissue,” says recent MIT graduate Sandra Q. Liu SM ’20, PhD ’24, the lead designer of GelPalm, who developed the system as a CSAIL affiliate and PhD student in mechanical engineering. “By combining rigid structures with deformable, compliant materials, we can better achieve that same adaptive talent as our skillful hands. A major advantage is that we don't need extra motors or mechanisms to actuate the palm's deformation — the inherent compliance allows it to automatically conform around objects, just like our human palms do so dexterously.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers put the palm design to the test. Liu compared the tactile sensing performance of two different illumination systems — blue LEDs versus white LEDs — integrated into the ROMEO fingers. “Both yielded similar high-quality 3D tactile reconstructions when pressing objects into the gel surfaces,” says Liu.&lt;/p&gt;&lt;p dir="ltr"&gt;But the critical experiment, she says, was to examine how well the different palm configurations could envelop and stably grasp objects. The team got hands-on, literally slathering plastic shapes in paint and pressing them against four palm types: rigid, structurally compliant, gel compliant, and their dual compliant design. “Visually, and by analyzing the painted surface area contacts, it was clear having both structural and material compliance in the palm provided significantly more grip than the others,” says Liu. “It's an elegant way to maximize the palm's role in achieving stable grasps.”&lt;/p&gt;&lt;p dir="ltr"&gt;One notable limitation is the challenge of integrating sufficient sensory technology within the palm without making it bulky or overly complex. The use of camera-based tactile sensors introduces issues with size and flexibility, the team says, as the current tech doesn't easily allow for extensive coverage without trade-offs in design and functionality. Addressing this could mean developing more flexible materials for mirrors, and enhancing sensor integration to maintain functionality, without compromising practical usability.&lt;/p&gt;&lt;p dir="ltr"&gt;“The palm is almost completely overlooked in the development of most robotic hands,” says Columbia University Associate Professor Matei Ciocarlie, who wasn’t involved in the paper. “This work is remarkable because it introduces a purposefully designed, useful palm that combines two key features, articulation and sensing, whereas most robot palms lack either. The human palm is both subtly articulated and highly sensitive, and this work is a relevant innovation in this direction.”&lt;/p&gt;&lt;p dir="ltr"&gt;“I hope we're moving toward more advanced robotic hands that blend soft and rigid elements with tactile sensitivity, ideally within the next five to 10 years. It's a complex field without a clear consensus on the best hand design, which makes this work especially thrilling,” says Liu. “In developing GelPalm and the ROMEO fingers, I focused on modularity and transferability to encourage a wide range of designs. Making this technology low-cost and easy to manufacture allows more people to innovate and explore. As just one lab and one person in this vast field, my dream is that sharing this knowledge could spark advancements and inspire others.”&lt;/p&gt;&lt;p&gt;Ted Adelson, the John and Dorothy Wilson Professor of Vision Science in the Department of Brain and Cognitive Sciences and CSAIL member, is the senior author on a &lt;a href="https://arxiv.org/pdf/2404.08227" target="_blank"&gt;paper describing the work&lt;/a&gt;. The research was supported, in part, by the Toyota Research Institute, Amazon Science Hub, and the SINTEF BIFROST project. Liu presented the research at the International Conference on Robotics and Automation (ICRA) earlier this month.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/mit-csail-sandra-liu.png?itok=FcnNbIpO" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT CSAIL student Sandra Q. Liu displays her innovative GelPalm robotic design in her lab workspace.]]></media:description>
              <media:credit>Photo: Michael Grimmett/MIT CSAIL </media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/electronics">Electronics</category>
      <category domain="https://news.mit.edu/topic/sensors">Sensors</category>
      <category domain="https://news.mit.edu/topic/bioinspiration">Bioinspiration</category>
      <category domain="https://news.mit.edu/topic/prosthetics">Prosthetics</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
    </item>
<item>
  <title>Robotic “SuperLimbs” could help moonwalkers recover from falls</title>
  <link>https://news.mit.edu/2024/robotic-superlimbs-could-help-moonwalkers-recover-from-falls-0515</link>
  <description><![CDATA[A new MIT system could help astronauts conserve energy and extend missions on the lunar surface.]]></description>
  <pubDate>Wed, 15 May 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/robotic-superlimbs-could-help-moonwalkers-recover-from-falls-0515</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Need a moment of levity? Try watching &lt;a href="https://www.newsweek.com/watch-apollo-astronauts-falling-moon-1449937" target="_blank"&gt;videos&lt;/a&gt; of astronauts falling on the moon. NASA’s outtakes of Apollo astronauts tripping and stumbling as they bounce in slow motion are delightfully relatable.&lt;/p&gt;&lt;p&gt;For MIT engineers, the lunar bloopers also highlight an opportunity to innovate.&lt;/p&gt;&lt;p&gt;“Astronauts are physically very capable, but they can struggle on the moon, where gravity is one-sixth that of Earth’s but their inertia is still the same. Furthermore, wearing a spacesuit is a significant burden and can constrict their movements,” says Harry Asada, professor of mechanical engineering at MIT. “We want to provide a safe way for astronauts to get back on their feet if they fall.”&lt;/p&gt;&lt;p&gt;Asada and his colleagues are designing a pair of wearable robotic limbs that can physically support an astronaut and lift them back on their feet after a fall. The system, which the researchers have dubbed Supernumerary Robotic Limbs or “SuperLimbs” is designed to extend from a backpack, which would also carry the astronaut’s life support system, along with the controller and motors to power the limbs.&lt;/p&gt;&lt;p&gt;The researchers have built a physical prototype, as well as a control system to direct the limbs, based on feedback from the astronaut using it. The team tested a preliminary version on healthy subjects who also volunteered to wear a constrictive garment similar to an astronaut’s spacesuit. When the volunteers attempted to get up from a sitting or lying position, they did so with less effort when assisted by SuperLimbs, compared to when they had to recover on their own.&lt;/p&gt;&lt;p&gt;The MIT team envisions that SuperLimbs can physically assist astronauts after a fall and, in the process, help them conserve their energy for other essential tasks. The design could prove especially useful in the coming years, with the launch of NASA’s Artemis mission, which plans to send astronauts back to the moon for the first time in over 50 years. Unlike the largely exploratory mission of Apollo, Artemis astronauts will endeavor to build the first permanent moon base — a physically demanding task that will require multiple extended extravehicular activities (EVAs).&lt;/p&gt;&lt;p&gt;“During the Apollo era, when astronauts would fall, 80 percent of the time it was when they were doing excavation or some sort of job with a tool,” says team member and MIT doctoral student Erik Ballesteros. “The Artemis missions will really focus on construction and excavation, so the risk of falling is much higher. We think that SuperLimbs can help them recover so they can be more productive, and extend their EVAs.”&lt;/p&gt;&lt;p&gt;Asada, Ballesteros, and their colleagues will present their design and study this week at the IEEE International Conference on Robotics and Automation (ICRA). Their co-authors include MIT postdoc&amp;nbsp;Sang-Yoep Lee and Kalind Carpenter of the Jet Propulsion Laboratory.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Taking a stand&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team’s design is the latest application of SuperLimbs, which Asada first developed about a decade ago and has since adapted for a range of applications, including assisting workers in aircraft manufacturing, construction, and ship building.&lt;/p&gt;&lt;p&gt;Most recently, Asada and Ballesteros wondered whether SuperLimbs might assist astronauts, particularly as NASA plans to send astronauts back to the surface of the moon.&lt;/p&gt;&lt;p&gt;“In communications with NASA, we learned that this issue of falling on the moon is a serious risk,” Asada says. “We realized that we could make some modifications to our design to help astronauts recover from falls and carry on with their work.”&lt;/p&gt;&lt;p&gt;The team first took a step back, to study the ways in which humans naturally recover from a fall. In their new study, they asked several healthy volunteers to attempt to stand upright after lying on their side, front, and back.&lt;/p&gt;&lt;p&gt;The researchers then looked at how the volunteers’ attempts to stand changed when their movements were constricted, similar to the way astronauts’ movements are limited by the bulk of their spacesuits. The team built a suit to mimic the stiffness of traditional spacesuits, and had volunteers don the suit before again attempting to stand up from various fallen positions. The volunteers’ sequence of movements was similar, though required much more effort compared to their unencumbered attempts.&lt;/p&gt;&lt;p&gt;The team mapped the movements of each volunteer as they stood up, and found that they each carried out a common sequence of motions, moving from one pose, or “waypoint,” to the next, in a predictable order.&lt;/p&gt;&lt;p&gt;“Those ergonomic experiments helped us to model in a straightforward way, how a human stands up,” Ballesteros says. “We could postulate that about 80 percent of humans stand up in a similar way. Then we designed a controller around that trajectory.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Helping hand&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team developed software to generate a trajectory for a robot, following a sequence that would help support a human and lift them back on their feet. They applied the controller to a heavy, fixed robotic arm, which they attached to a large backpack. The researchers then attached the backpack to the bulky suit and helped volunteers back into the suit. They asked the volunteers to again lie on their back, front, or side, and then had them attempt to stand as the robot sensed the person’s movements and adapted to help them to their feet.&lt;/p&gt;&lt;p&gt;Overall, the volunteers were able to stand stably with much less effort when assisted by the robot, compared to when they tried to stand alone while wearing the bulky suit.&lt;/p&gt;&lt;p&gt;“It feels kind of like an extra force moving with you,” says Ballesteros, who also tried out the suit and arm assist. “Imagine wearing a backpack and someone grabs the top and sort of pulls you up. Over time, it becomes sort of natural.”&lt;/p&gt;&lt;p&gt;The experiments confirmed that the control system can successfully direct a robot to help a person stand back up after a fall. The researchers plan to pair the control system with their latest version of SuperLimbs, which comprises two multijointed robotic arms that can extend out from a backpack. The backpack would also contain the robot’s battery and motors, along with an astronaut’s ventilation system.&lt;/p&gt;&lt;p&gt;“We designed these robotic arms based on an AI search and design optimization, to look for designs of classic robot manipulators with certain engineering constraints,” Ballesteros says. “We filtered through many designs and looked for the design that consumes the least amount of energy to lift a person up. This version of SuperLimbs is the product of that process.”&lt;/p&gt;&lt;p&gt;Over the summer, Ballesteros will build out the full SuperLimbs system at NASA’s Jet Propulsion Laboratory, where he plans to streamline the design and minimize the weight of its parts and motors using advanced, lightweight materials. Then, he hopes to pair the limbs with astronaut suits, and test them in low-gravity simulators, with the goal of someday assisting astronauts on future missions to the moon and Mars.&lt;/p&gt;&lt;p&gt;“Wearing a spacesuit can be a physical burden,” Asada notes. “Robotic systems can help ease that burden, and help astronauts be more productive during their missions.”&lt;/p&gt;&lt;p&gt;This research was supported, in part, by NASA.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/MIT-SuperLimbs-A1.jpg?itok=3LNIz1sp" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[SuperLimbs, a system of wearable robotic limbs built by MIT engineers, is designed to physically support an astronaut and lift them back on their feet after a fall, helping them conserve energy for other essential tasks. Pictured, from left, is Sang-Yoep Lee, Harry Asada, and Erik Ballesteros.]]></media:description>
              <media:credit>Photo: Jennifer Chu</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/assistive-technology">Assistive technology</category>
      <category domain="https://news.mit.edu/topic/biomechanics">Biomechanics</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/nasa">NASA</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/space">Space</category>
      <category domain="https://news.mit.edu/topic/space-exploration">Space exploration</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Using ideas from game theory to improve the reliability of language models</title>
  <link>https://news.mit.edu/2024/consensus-game-elevates-ai-text-comprehension-generation-skills-0514</link>
  <description><![CDATA[A new “consensus game,” developed by MIT CSAIL researchers, elevates AI’s text comprehension and generation skills. ]]></description>
  <pubDate>Tue, 14 May 2024 11:30:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/consensus-game-elevates-ai-text-comprehension-generation-skills-0514</guid>
        <dc:creator>Rachel Gordon | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p&gt;Imagine you and a friend are playing a game where your goal is to communicate secret messages to each other using only cryptic sentences. Your friend's job is to guess the secret message behind your sentences. Sometimes, you give clues directly, and other times, your friend has to guess the message by asking yes-or-no questions about the clues you've given. The challenge is that both of you want to make sure you're understanding each other correctly and agreeing on the secret message.&lt;/p&gt;&lt;p&gt;MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers have created a similar "game" to help improve how AI understands and generates text. It is known as a “consensus game” and it involves two parts of an AI system — one part tries to generate sentences (like giving clues), and the other part tries to understand and evaluate those sentences (like guessing the secret message).&lt;/p&gt;&lt;p&gt;The researchers discovered that by treating this interaction as a game, where both parts of the AI work together under specific rules to agree on the right message, they could significantly improve the AI's ability to give correct and coherent answers to questions. They tested this new game-like approach on a variety of tasks, such as reading comprehension, solving math problems, and carrying on conversations, and found that it helped the AI perform better across the board.&lt;/p&gt;&lt;p&gt;Traditionally, large language models answer one of two ways: generating answers directly from the model (generative querying) or using the model to score a set of predefined answers (discriminative querying), which can lead to differing and sometimes incompatible results. With the generative approach, "Who is the president of the United States?" might yield a straightforward answer like "Joe Biden." However, a discriminative query could incorrectly dispute this fact when evaluating the same answer, such as "Barack Obama."&lt;/p&gt;&lt;p&gt;So, how do we reconcile mutually incompatible scoring procedures to achieve coherent, efficient predictions?&amp;nbsp;&lt;/p&gt;&lt;p&gt;"Imagine a new way to help language models understand and generate text, like a game. We've developed a training-free, game-theoretic method that treats the whole process as a complex game of clues and signals, where a generator tries to send the right message to a discriminator using natural language. Instead of chess pieces, they're using words and sentences," says Athul Jacob, an MIT PhD student in electrical engineering and computer science and CSAIL affiliate. "Our way to navigate this game is finding the 'approximate equilibria,' leading to a new decoding algorithm called 'equilibrium ranking.' It's a pretty exciting demonstration of how bringing game-theoretic strategies into the mix can tackle some big challenges in making language models more reliable and consistent."&lt;/p&gt;&lt;p&gt;When tested across many tasks, like reading comprehension, commonsense reasoning, math problem-solving, and dialogue, the team's algorithm consistently improved how well these models performed. Using the ER algorithm with the LLaMA-7B model even outshone the results from much larger models. "Given that they are already competitive, that people have been working on it for a while, but the level of improvements we saw being able to outperform a model that's 10 times the size was a pleasant surprise," says Jacob.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Game on&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;"Diplomacy," a strategic board game set in pre-World War I Europe, where players negotiate alliances, betray friends, and conquer territories without the use of dice&amp;nbsp;— relying purely on skill, strategy, and interpersonal manipulation — recently had a second coming. In November 2022, computer scientists, including Jacob, developed “Cicero,” an AI agent that achieves human-level capabilities in the mixed-motive seven-player game, which requires the same aforementioned skills, but with natural language. The math behind this partially inspired the Consensus Game.&amp;nbsp;&lt;/p&gt;&lt;p&gt;While the history of AI agents long predates when OpenAI's software entered the chat in November 2022, it's well documented that they can still cosplay as your well-meaning, yet pathological friend.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The consensus game system reaches equilibrium as an agreement, ensuring accuracy and fidelity to the model's original insights. To achieve this, the method iteratively adjusts the interactions between the generative and discriminative components until they reach a consensus on an answer that accurately reflects reality and aligns with their initial beliefs. This approach effectively bridges the gap between the two querying methods.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In practice, implementing the consensus game approach to language model querying, especially for question-answering tasks, does involve significant computational challenges. For example, when using datasets like MMLU, which have thousands of questions and multiple-choice answers, the model must apply the mechanism to each query. Then, it must reach a consensus between the generative and discriminative components for every question and its possible answers.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The system did struggle with a grade school right of passage: math word problems. It couldn't generate wrong answers, which is a critical component of understanding the process of coming up with the right one.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“The last few years have seen really impressive progress in both strategic decision-making and language generation from AI systems, but we’re just starting to figure out how to put the two together. Equilibrium ranking is a first step in this direction, but I think there’s a lot we’ll be able to do to scale this up to more complex problems,” says Jacob.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;An avenue of future work involves enhancing the base model by integrating the outputs of the current method. This is particularly promising since it can yield more factual and consistent answers across various tasks, including factuality and open-ended generation. The potential for such a method to significantly improve the base model's performance is high, which could result in more reliable and factual outputs from ChatGPT and similar language models that people use daily.&amp;nbsp;&lt;/p&gt;&lt;p&gt;"Even though modern language models, such as ChatGPT and Gemini, have led to solving various tasks through chat interfaces, the statistical decoding process that generates a response from such models has remained unchanged for decades," says Google Research Scientist Ahmad Beirami, who was not involved in the work. "The proposal by the MIT researchers is an innovative game-theoretic framework for decoding from language models through solving the equilibrium of a consensus game. The significant performance gains reported in the research paper are promising, opening the door to a potential paradigm shift in language model decoding that may fuel a flurry of new applications."&lt;/p&gt;&lt;p&gt;Jacob wrote the paper with MIT-IBM Watson Lab researcher Yikang Shen and MIT Department of Electrical Engineering and Computer Science assistant professors Gabriele Farina and Jacob Andreas, who is also a CSAIL member. They presented their work at the International Conference on Learning Representations (ICLR) earlier this month, where it was highlighted as a "spotlight paper." The research also received a “best paper award” at the NeurIPS R0-FoMo Workshop in December 2023.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202404/MIT-Consensus-Game.png?itok=aKJSV4WZ" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT researchers’ "consensus game" is a game-theoretic approach for language model decoding. The equilibrium-ranking algorithm harmonizes generative and discriminative querying to enhance prediction accuracy across various tasks, outperforming larger models and demonstrating the potential of game theory in improving language model consistency and truthfulness.]]></media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/natural-language-processing">Natural language processing</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/game-theory">Game theory</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>A better way to control shape-shifting soft robots</title>
  <link>https://news.mit.edu/2024/better-way-control-shape-shifting-soft-robots-0510</link>
  <description><![CDATA[A new algorithm learns to squish, bend, or stretch a robot’s entire body to accomplish diverse tasks like avoiding obstacles or retrieving items.]]></description>
  <pubDate>Fri, 10 May 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/better-way-control-shape-shifting-soft-robots-0510</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Imagine a slime-like robot that can seamlessly change its shape to squeeze through narrow spaces, which could be deployed inside the human body to remove an unwanted item.&lt;/p&gt;&lt;p&gt;While such a robot does not yet exist outside a laboratory, researchers are working to develop reconfigurable soft robots for applications in health care, wearable devices, and industrial systems.&lt;/p&gt;&lt;p&gt;But how can one control a squishy robot that doesn’t have joints, limbs, or fingers that can be manipulated, and instead can drastically alter its entire shape at will? MIT researchers are working to answer that question.&lt;/p&gt;&lt;p&gt;They developed a control algorithm that can autonomously learn how to move, stretch, and shape a reconfigurable robot to complete a specific task, even when that task requires the robot to change its morphology multiple times. The team also built a simulator to test control algorithms for deformable soft robots on a series of challenging, shape-changing tasks.&lt;/p&gt;&lt;p&gt;Their method completed each of the eight tasks they evaluated while outperforming other algorithms. The technique worked especially well on multifaceted tasks. For instance, in one test, the robot had to reduce its height while growing two tiny legs to squeeze through a narrow pipe, and then un-grow those legs and extend its torso to open the pipe’s lid.&lt;/p&gt;&lt;p&gt;While reconfigurable soft robots are still in their infancy, such a technique could someday enable general-purpose robots that can adapt their shapes to accomplish diverse tasks.&lt;/p&gt;&lt;p&gt;“When people think about soft robots, they tend to think about robots that are elastic, but return to their original shape. Our robot is like slime and can actually change its morphology. It is very striking that our method worked so well because we are dealing with something very new,” says Boyuan Chen, an electrical engineering and computer science (EECS) graduate student and co-author of a &lt;a href="https://arxiv.org/pdf/2401.13231" target="_blank"&gt;paper on this approach&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Chen’s co-authors include lead author Suning Huang, an undergraduate student at Tsinghua University in China who completed this work while a visiting student at MIT; Huazhe Xu, an assistant professor at Tsinghua University; and senior author Vincent Sitzmann, an assistant professor of EECS at MIT who leads the Scene Representation Group in the Computer Science and Artificial Intelligence Laboratory. The research will be presented at the International Conference on Learning Representations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Controlling dynamic motion&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Scientists often teach robots to complete tasks using a machine-learning approach known as reinforcement learning, which is a trial-and-error process in which the robot is rewarded for actions that move it closer to a goal.&lt;/p&gt;&lt;p&gt;This can be effective when the robot’s moving parts are consistent and well-defined, like a gripper with three fingers. With a robotic gripper, a reinforcement learning algorithm might move one finger slightly, learning by trial and error whether that motion earns it a reward. Then it would move on to the next finger, and so on.&lt;/p&gt;&lt;p&gt;But shape-shifting robots, which are controlled by magnetic fields, can dynamically squish, bend, or elongate their entire bodies.&lt;/p&gt;&lt;img src="/sites/default/files/images/inline/Ditto-gym-1.gif" data-align="center" data-entity-uuid="c91539e7-bfcf-4ff4-bde2-a70e46dcc3ce" data-entity-type="file" alt="An orange rectangular-like blob shifts and elongates itself out of a three-walled maze structure to reach a purple target." width="512" height="512" data-caption="The researchers built a simulator to test control algorithms for deformable soft robots on a series of challenging, shape-changing tasks. Here, a reconfigurable robot learns to elongate and curve its soft body to weave around obstacles and reach a target.&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Image: Courtesy of the researchers"&gt;&lt;p&gt;“Such a robot could have thousands of small pieces of muscle to control, so it is very hard to learn in a traditional way,” says Chen.&lt;/p&gt;&lt;p&gt;To solve this problem, he and his collaborators had to think about it differently. Rather than moving each tiny muscle individually, their reinforcement learning algorithm begins by learning to control groups of adjacent muscles that work together.&lt;/p&gt;&lt;p&gt;Then, after the algorithm has explored the space of possible actions by focusing on groups of muscles, it drills down into finer detail to optimize the policy, or action plan, it has learned. In this way, the control algorithm follows a coarse-to-fine methodology.&lt;/p&gt;&lt;p&gt;“Coarse-to-fine means that when you take a random action, that random action is likely to make a difference. The change in the outcome is likely very significant because you coarsely control several muscles at the same time,” Sitzmann says.&lt;/p&gt;&lt;p&gt;To enable this, the researchers treat a robot’s action space, or how it can move in a certain area, like an image.&lt;/p&gt;&lt;p&gt;Their machine-learning model uses images of the robot’s environment to generate a 2D action space, which includes the robot and the area around it. They simulate robot motion using what is known as the material-point-method, where the action space is covered by points, like image pixels, and overlayed with a grid.&lt;/p&gt;&lt;p&gt;The same way nearby pixels in an image are related (like the pixels that form a tree in a photo), they built their algorithm to understand that nearby action points have stronger correlations. Points around the robot’s “shoulder” will move similarly when it changes shape, while points on the robot’s “leg” will also move similarly, but in a different way than those on the “shoulder.”&lt;/p&gt;&lt;p&gt;In addition, the researchers use the same machine-learning model to look at the environment and predict the actions the robot should take, which makes it more efficient.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building a simulator&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After developing this approach, the researchers needed a way to test it, so they created a simulation environment called DittoGym.&lt;/p&gt;&lt;p&gt;DittoGym features eight tasks that evaluate a reconfigurable robot’s ability to dynamically change shape. In one, the robot must elongate and curve its body so it can weave around obstacles to reach a target point. In another, it must change its shape to mimic letters of the alphabet.&lt;/p&gt;&lt;img src="/sites/default/files/images/inline/Ditto-gym-2.gif" data-align="center" data-entity-uuid="73bd6a67-6cdd-4520-b7a9-7f5ed250c5a7" data-entity-type="file" alt="Animation of orange blob shifting into shapes such as a star, and the letters “M,” “I,” and “T.”" width="512" height="512" data-caption="In this simulation, the reconfigurable soft robot, trained using the researchers&amp;apos; control algorithm, must change its shape to mimic objects, like stars, and the letters M-I-T.&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;Image: Courtesy of the researchers"&gt;&lt;p&gt;“Our task selection in DittoGym follows both generic reinforcement learning benchmark design principles and the specific needs of reconfigurable robots. Each task is designed to represent certain properties that we deem important, such as the capability to navigate through long-horizon explorations, the ability to analyze the environment, and interact with external objects,” Huang says. “We believe they together can give users a comprehensive understanding of the flexibility of reconfigurable robots and the effectiveness of our reinforcement learning scheme.”&lt;/p&gt;&lt;p&gt;Their algorithm outperformed baseline methods and was the only technique suitable for completing multistage tasks that required several shape changes.&lt;/p&gt;&lt;p&gt;“We have a stronger correlation between action points that are closer to each other, and I think that is key to making this work so well,” says Chen.&lt;/p&gt;&lt;p&gt;While it may be many years before shape-shifting robots are deployed in the real world, Chen and his collaborators hope their work inspires other scientists not only to study reconfigurable soft robots but also to think about leveraging 2D action spaces for other complex control problems.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202405/MIT-Ditto-Gym-01-press.jpg?itok=Fs_VNVBo" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[A new machine-learning technique can train and control a reconfigurable soft robot that can dynamically change its shape to complete a task. The researchers, from MIT and elsewhere, also built a simulator that can evaluate control algorithms for shape-shifting soft robots. ]]></media:description>
              <media:credit>Image: Courtesy of the researchers; MIT News</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Three from MIT named 2024-25 Goldwater Scholars</title>
  <link>https://news.mit.edu/2024/mit-goldwater-scholars-0502</link>
  <description><![CDATA[Undergraduates Ben Lou, Srinath Mahankali, and Kenta Suzuki, whose research explores math and physics, are honored for their academic excellence.]]></description>
  <pubDate>Thu, 02 May 2024 14:25:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/mit-goldwater-scholars-0502</guid>
        <dc:creator>Leah Campbell | School of Science</dc:creator>
  <content:encoded>&lt;p&gt;MIT students Ben Lou, Srinath Mahankali, and Kenta Suzuki have been selected to receive Barry Goldwater Scholarships for the 2024-25 academic year. They are among just 438 recipients from across the country selected based on academic merit from an estimated pool of more than 5,000 college sophomores and juniors, approximately 1,350 of whom were nominated by their academic institution to compete for the scholarship.&lt;/p&gt;&lt;p&gt;Since 1989, the Barry Goldwater Scholarship and Excellence in Education Foundation has awarded nearly 11,000 Goldwater scholarships to support undergraduates who intend to pursue research careers in the natural sciences, mathematics, and engineering and have the potential to become leaders in their respective fields. Past scholars have gone on to win an impressive array of prestigious postgraduate fellowships. Almost all, including the three MIT recipients, intend to obtain doctorates in their area of research.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Ben Lou&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Ben Lou is a third-year student originally from San Diego, California, majoring in physics and math with a minor in philosophy.&lt;/p&gt;&lt;p&gt;“My research interests are scattered across different disciplines,” says Lou. “I want to draw from a wide range of topics in math and physics, finding novel connections between them, to push forward the frontier of knowledge.”&lt;/p&gt;&lt;p&gt;Since January 2022, he has worked with Nergis Mavalvala, dean of the School of Science, and Hudson Loughlin, a graduate student in the LIGO group, which studies the detection of gravitational waves. Lou is working with them to advance the field of quantum measurement and better understand quantum gravity.&lt;/p&gt;&lt;p&gt;“Ben has enormous intellectual horsepower and works with remarkable independence,” writes Mavalvala in her recommendation letter. “I have no doubt he has an outstanding career in physics ahead of him.”&lt;/p&gt;&lt;p&gt;Lou, for his part, is grateful to Mavalvala and Loughlin, as well as all of his scientific mentors that have supported him along his research path. That includes MIT professors Alan Guth and Barton Zwiebach, who introduced him to quantum physics, as well as his first-year advisor, Richard Price; current advisor, Janet Conrad; Elijah Bodish and Roman Bezrukavnikov in the Department of Mathematics; and David W. Brown of the San Diego Math Circle.&lt;/p&gt;&lt;p&gt;In terms of his future career goals, Lou wants to be a professor of theoretical physics and study, as he says, the “fundamental aspects of reality” while also inspiring students to love math and physics.&lt;/p&gt;&lt;p&gt;In addition to his research, Lou is currently the vice president of the Assistive Technology Club at MIT and actively engaged in raising money for Spinal Muscular Atrophy research. In the future, he’d like to continue his philanthropy work and use his personal experience to advise an assistive technology company.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Srinath Mahankali&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Srinath Mahankali is a third-year student from New York City majoring in computer science.&lt;/p&gt;&lt;p&gt;Since June 2022, Mahankali has been an undergraduate researcher in the MIT Computer Science and Artificial Intelligence Laboratory. Working with Pulkit Agrawal, assistant professor of electrical engineering and computer science and head of the Improbable AI Lab, Mahankali’s research is on training robots. Currently, his focus is on training quadruped robots to move in an energy-efficient manner and training agents to interact in environments with minimal feedback. But in the future, he’d like to develop robots that can complete athletic tasks like gymnastics.&lt;/p&gt;&lt;p&gt;“The experience of discussing research with Srinath is similar to discussions with the best PhD students in my group,” writes Agrawal in his recommendation letter. “He is fearless, willing to take risks, persistent, creative, and gets things done.”&lt;/p&gt;&lt;p&gt;Before coming to MIT, Mahankali was a 2021 Regeneron STS scholar, which is one of the oldest and most prestigious awards for math and science students. In 2020, he was also a participant in the MIT PRIMES program, studying objective functions in optimization problems with Yunan Yang, an assistant professor of math at Cornell University.&lt;/p&gt;&lt;p&gt;“I’m deeply grateful to all my research advisors for their invaluable mentorship and guidance,” says Mahankali, extending his thanks to PhD students Zhang-Wei Hong and Gabe Margolis, as well as assistant professor of math at Brandeis, Promit Ghosal, and all of the organizers of the PRIMES program. “I’m also very grateful to all the members of the Improbable AI Lab for their support, encouragement, and willingness to help and discuss any questions I have,”&lt;/p&gt;&lt;p&gt;In the future, Mahankali wants to obtain a PhD and one day lead his own lab in robotics and artificial intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Kenta Suzuki&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Kenta Suzuki is a third-year student majoring in mathematics from Bloomfield Hills, Michigan, and Tokyo, Japan.&lt;/p&gt;&lt;p&gt;Currently, Suzuki works with professor of mathematics Roman Bezrukavnikov on research at the intersection of number and representation theory, using geometric methods to represent p-adic groups. Suzuki has also previously worked with math professors Wei Zhang and Zhiwei Yun, crediting the latter with inspiring him to pursue research in representation theory.&lt;/p&gt;&lt;p&gt;In his recommendation letter, Yun writes, “Kenta is the best undergraduate student that I have worked with in terms of the combination of raw talent, mathematical maturity, and research abilities.”&lt;/p&gt;&lt;p&gt;Before coming to MIT, Suzuki was a Yau Science Award USA finalist in 2020, receiving a gold in math, and he received honorable mention from the Davidson Institute Fellows program in 2021. He also participated in the MIT PRIMES program in 2020. Suzuki credits his PRIMES mentor, Michael Zieve at the University of Michigan, with giving him his first taste of mathematical research. In addition, he extended his thanks to all of his math mentors, including the organizers of MIT Summer Program in Undergraduate Research.&lt;/p&gt;&lt;p&gt;After MIT, Suzuki intends to obtain a PhD in pure math, continuing his research in representation theory and number theory and, one day, teaching at a research-oriented institution.&lt;/p&gt;&lt;p&gt;The Barry Goldwater Scholarship and Excellence in Education Program was established by U.S. Congress in 1986 to honor Senator Barry Goldwater, a soldier and national leader who served the country for 56 years. Awardees receive scholarships of up to $7,500 a year to cover costs related to tuition, room and board, fees, and books.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202404/mit-goldwater-scholars-2024.png?itok=jDBFgI9Q" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT students (from left to right) Ben Lou, Srinath Mahankali, and Kenta Suzuki have been selected to receive Barry Goldwater Scholarships for the 2024-25 academic year. ]]></media:description>
              <media:credit>Photos courtesy of the students.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/awards">Awards, honors and fellowships</category>
      <category domain="https://news.mit.edu/topic/students">Students</category>
      <category domain="https://news.mit.edu/topic/undergraduate">Undergraduate</category>
      <category domain="https://news.mit.edu/topic/mathematics">Mathematics</category>
      <category domain="https://news.mit.edu/topic/philosophy">Philosophy</category>
      <category domain="https://news.mit.edu/topic/physics">Physics</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/school-humanities-arts-and-social-sciences">School of Humanities Arts and Social Sciences</category>
    </item>
<item>
  <title>Natural language boosts LLM performance in coding, planning, and robotics</title>
  <link>https://news.mit.edu/2024/natural-language-boosts-llm-performance-coding-planning-robotics-0501</link>
  <description><![CDATA[Three neurosymbolic methods help language models find better abstractions within natural language, then use those representations to execute complex tasks.]]></description>
  <pubDate>Wed, 01 May 2024 16:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/natural-language-boosts-llm-performance-coding-planning-robotics-0501</guid>
        <dc:creator>Alex Shipps | MIT CSAIL</dc:creator>
  <content:encoded>&lt;p&gt;Large language models (LLMs) are becoming increasingly useful for programming and robotics tasks, but for more complicated reasoning problems, the gap between these systems and humans looms large. Without the ability to learn new concepts like humans do, these systems fail to form good abstractions — essentially, high-level representations of complex concepts that skip less-important details — and thus sputter when asked to do more sophisticated tasks.&lt;br&gt;&lt;br&gt;Luckily, MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers have found a treasure trove of abstractions within natural language. In three papers to be presented at the International Conference on Learning Representations this month, the group shows how our everyday words are a rich source of context for language models, helping them build better overarching representations for code synthesis, AI planning, and robotic navigation and manipulation.&lt;br&gt;&lt;br&gt;The three separate frameworks build libraries of abstractions for their given task: &lt;a href="https://arxiv.org/abs/2310.19791" target="_blank"&gt;LILO&lt;/a&gt; (library induction from language observations) can synthesize, compress, and document code; &lt;a href="https://arxiv.org/abs/2312.08566" target="_blank"&gt;Ada&lt;/a&gt; (action domain acquisition) explores sequential decision-making for artificial intelligence agents; and &lt;a href="https://arxiv.org/abs/2402.18759 " target="_blank"&gt;LGA&lt;/a&gt; (language-guided abstraction) helps robots better understand their environments to develop more feasible plans. Each system is a neurosymbolic method, a type of AI that blends human-like neural networks and program-like logical components.&lt;br&gt;&lt;br&gt;&lt;strong&gt;LILO: A neurosymbolic framework that codes&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Large language models can be used to quickly write solutions to small-scale coding tasks, but cannot yet architect entire software libraries like the ones written by human software engineers. To take their software development capabilities further, AI models need to refactor (cut down and combine) code into libraries of succinct, readable, and reusable programs.&lt;br&gt;&lt;br&gt;Refactoring tools like the previously developed MIT-led &lt;a href="https://mlb2251.github.io/stitch_jul11.pdf" target="_blank"&gt;Stitch&lt;/a&gt; algorithm can automatically identify abstractions, so, in a nod to the Disney movie “Lilo &amp;amp; Stitch,” CSAIL researchers combined these algorithmic refactoring approaches with LLMs. Their neurosymbolic method LILO uses a standard LLM to write code, then pairs it with Stitch to find abstractions that are comprehensively documented in a library.&lt;br&gt;&lt;br&gt;LILO’s unique emphasis on natural language allows the system to do tasks that require human-like commonsense knowledge, such as identifying and removing all vowels from a string of code and drawing a snowflake. In both cases, the CSAIL system outperformed standalone LLMs, as well as a previous library learning algorithm from MIT called DreamCoder, indicating its ability to build a deeper understanding of the words within prompts. These encouraging results point to how LILO could assist with things like writing programs to manipulate documents like Excel spreadsheets, helping AI answer questions about visuals, and drawing 2D graphics.&lt;/p&gt;&lt;p&gt;“Language models prefer to work with functions that are named in natural language,” says Gabe Grand SM '23, an MIT PhD student in electrical engineering and computer science, CSAIL affiliate, and lead author on the research. “Our work creates more straightforward abstractions for language models and assigns natural language names and documentation to each one, leading to more interpretable code for programmers and improved system performance.”&lt;/p&gt;&lt;p&gt;When prompted on a programming task, LILO first uses an LLM to quickly propose solutions based on data it was trained on, and then the system slowly searches more exhaustively for outside solutions. Next, Stitch efficiently identifies common structures within the code and pulls out useful abstractions. These are then automatically named and documented by LILO, resulting in simplified programs that can be used by the system to solve more complex tasks.&lt;/p&gt;&lt;p&gt;The MIT framework writes programs in domain-specific programming languages, like Logo, a language developed at MIT in the 1970s to teach children about programming. Scaling up automated refactoring algorithms to handle more general programming languages like Python will be a focus for future research. Still, their work represents a step forward for how language models can facilitate increasingly elaborate coding activities.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Ada: Natural language guides AI task planning&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Just like in programming, AI models that automate multi-step tasks in households and command-based video games lack abstractions. Imagine you’re cooking breakfast and ask your roommate to bring a hot egg to the table — they’ll intuitively abstract their background knowledge about cooking in your kitchen into a sequence of actions. In contrast, an LLM trained on similar information will still struggle to reason about what they need to build a flexible plan.&lt;br&gt;&lt;br&gt;Named after the famed mathematician Ada Lovelace, who many consider the world’s first programmer, the CSAIL-led “Ada” framework makes headway on this issue by developing libraries of useful plans for virtual kitchen chores and gaming. The method trains on potential tasks and their natural language descriptions, then a language model proposes action abstractions from this dataset. A human operator scores and filters the best plans into a library, so that the best possible actions can be implemented into hierarchical plans for different tasks.&lt;br&gt;&lt;br&gt;“Traditionally, large language models have struggled with more complex tasks because of problems like reasoning about abstractions,” says Ada lead researcher Lio Wong, an MIT graduate student in brain and cognitive sciences, CSAIL affiliate, and LILO coauthor. “But we can combine the tools that software engineers and roboticists use with LLMs to solve hard problems, such as decision-making in virtual environments.”&lt;/p&gt;&lt;p&gt;When the researchers incorporated the widely-used large language model GPT-4 into Ada, the system completed more tasks in a kitchen simulator and Mini Minecraft than the AI decision-making baseline “Code as Policies.” Ada used the background information hidden within natural language to understand how to place chilled wine in a cabinet and craft a bed. The results indicated a staggering 59 and 89 percent task accuracy improvement, respectively.&lt;br&gt;&lt;br&gt;With this success, the researchers hope to generalize their work to real-world homes, with the hopes that Ada could assist with other household tasks and aid multiple robots in a kitchen. For now, its key limitation is that it uses a generic LLM, so the CSAIL team wants to apply a more powerful, fine-tuned language model that could assist with more extensive planning. Wong and her colleagues are also considering combining Ada with a robotic manipulation framework fresh out of CSAIL: LGA (language-guided abstraction).&lt;br&gt;&lt;br&gt;&lt;strong&gt;Language-guided abstraction: Representations for robotic tasks&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Andi Peng SM ’23, an MIT graduate student in electrical engineering and computer science and CSAIL affiliate, and her coauthors designed a method to help machines interpret their surroundings more like humans, cutting out unnecessary details in a complex environment like a factory or kitchen. Just like LILO and Ada, LGA has a novel focus on how natural language leads us to those better abstractions.&lt;br&gt;&lt;br&gt;In these more unstructured environments, a robot will need some common sense about what it’s tasked with, even with basic training beforehand. Ask a robot to hand you a bowl, for instance, and the machine will need a general understanding of which features are important within its surroundings. From there, it can reason about how to give you the item you want.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In LGA’s case, humans first provide a pre-trained language model with a general task description using natural language, like “bring me my hat.” Then, the model translates this information into abstractions about the essential elements needed to perform this task. Finally, an imitation policy trained on a few demonstrations can implement these abstractions to guide a robot to grab the desired item.&lt;br&gt;&lt;br&gt;Previous work required a person to take extensive notes on different manipulation tasks to pre-train a robot, which can be expensive. Remarkably, LGA guides language models to produce abstractions similar to those of a human annotator, but in less time. To illustrate this, LGA developed robotic policies to help Boston Dynamics’ Spot quadruped pick up fruits and throw drinks in a recycling bin. These experiments show how the MIT-developed method can scan the world and develop effective plans in unstructured environments, potentially guiding autonomous vehicles on the road and robots working in factories and kitchens.&lt;/p&gt;&lt;p&gt;“In robotics, a truth we often disregard is how much we need to refine our data to make a robot useful in the real world,” says Peng. “Beyond simply memorizing what’s in an image for training robots to perform tasks, we wanted to leverage computer vision and captioning models in conjunction with language. By producing text captions from what a robot sees, we show that language models can essentially build important world knowledge for a robot.”&lt;br&gt;&lt;br&gt;The challenge for LGA is that some behaviors can’t be explained in language, making certain tasks underspecified. To expand how they represent features in an environment, Peng and her colleagues are considering incorporating multimodal visualization interfaces into their work. In the meantime, LGA provides a way for robots to gain a better feel for their surroundings when giving humans a helping hand.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;An “exciting frontier” in AI&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;“Library learning represents one of the most exciting frontiers in artificial intelligence, offering a path towards discovering and reasoning over compositional abstractions,” says assistant professor at the University of Wisconsin-Madison Robert Hawkins, who was not involved with the papers. Hawkins notes that previous techniques exploring this subject have been “too computationally expensive to use at scale” and have an issue with the lambdas, or keywords used to describe new functions in many languages, that they generate. “They tend to produce opaque 'lambda salads,' big piles of hard-to-interpret functions. These recent papers demonstrate a compelling way forward by placing large language models in an interactive loop with symbolic search, compression, and planning algorithms. This work enables the rapid acquisition of more interpretable and adaptive libraries for the task at hand.”&lt;br&gt;&lt;br&gt;By building libraries of high-quality code abstractions using natural language, the three neurosymbolic methods make it easier for language models to tackle more elaborate problems and environments in the future. This deeper understanding of the precise keywords within a prompt presents a path forward in developing more human-like AI models.&lt;br&gt;&lt;br&gt;MIT CSAIL members are senior authors for each paper: Joshua Tenenbaum, a professor of brain and cognitive sciences, for both LILO and Ada; Julie Shah, head of the Department of Aeronautics and Astronautics, for LGA; and Jacob Andreas, associate professor of electrical engineering and computer science, for all three. The additional MIT authors are all PhD students: Maddy Bowers and Theo X. Olausson for LILO, Jiayuan Mao and Pratyusha Sharma for Ada, and Belinda Z. Li for LGA.&amp;nbsp;Muxin Liu of Harvey Mudd College was a coauthor on LILO; Zachary Siegel of Princeton University, Jaihai Feng of the University of California at Berkeley, and Noa Korneev of Microsoft were coauthors on Ada; and Ilia Sucholutsky, Theodore R. Sumers, and Thomas L. Griffiths of Princeton were coauthors on LGA.&amp;nbsp;&lt;br&gt;&lt;br&gt;LILO and Ada were supported, in part, by ​​MIT Quest for Intelligence, the MIT-IBM Watson AI Lab, Intel, U.S. Air Force Office of Scientific Research, the U.S. Defense Advanced Research Projects Agency, and the U.S. Office of Naval Research, with the latter project also receiving funding from the Center for Brains, Minds and Machines. LGA received funding from the U.S. National Science Foundation, Open Philanthropy, the Natural Sciences and Engineering Research Council of Canada, and the U.S. Department of Defense.&lt;/p&gt;</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202404/natural-language-models.png?itok=NMoeSx87" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Three new frameworks from MIT CSAIL reveal how natural language can provide important context for language models that perform coding, AI planning, and robotics tasks.]]></media:description>
              <media:credit>Image: Alex Shipps/MIT CSAIL, with components from the researchers and Pixabay.</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/natural-language-processing">Natural language processing</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/programming">Programming</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/computer-vision">Computer vision</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/brain-cognitive">Brain and cognitive sciences</category>
      <category domain="https://news.mit.edu/topic/programming-languages">Programming languages</category>
      <category domain="https://news.mit.edu/topic/center-brains-minds-and-machines">Center for Brains Minds and Machines</category>
      <category domain="https://news.mit.edu/topic/quest-intelligence">Quest for Intelligence</category>
      <category domain="https://news.mit.edu/topic/mit-ibm-watson-ai-lab">MIT-IBM Watson AI Lab</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/nsf">National Science Foundation (NSF)</category>
      <category domain="https://news.mit.edu/topic/department-defense-dod">Department of Defense (DoD)</category>
      <category domain="https://news.mit.edu/topic/darpa">Defense Advanced Research Projects Agency (DARPA)</category>
    </item>
<item>
  <title>Julie Shah named head of the Department of Aeronautics and Astronautics</title>
  <link>https://news.mit.edu/2024/julie-shah-named-head-department-aeronautics-astronautics-0429</link>
  <description><![CDATA[An expert in robotics and AI, Shah succeeds Steven Barrett at AeroAstro. <br>
]]></description>
  <pubDate>Mon, 29 Apr 2024 13:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/julie-shah-named-head-department-aeronautics-astronautics-0429</guid>
        <dc:creator>Mary Beth Gallagher | School of Engineering</dc:creator>
  <content:encoded>&lt;p&gt;Julie Shah ’04, SM ’06, PhD ’11, the H.N. Slater Professor in Aeronautics and Astronautics, has been named the new head of the Department of Aeronautics and Astronautics (AeroAstro), effective May 1.&lt;/p&gt;

&lt;p&gt;“Julie brings an exceptional record of visionary and interdisciplinary leadership to this role. She has made substantial technical contributions in the field of robotics and AI, particularly as it relates to the future of work, and has bridged important gaps in the social, ethical, and economic implications of AI and computing,” says Anantha Chandrakasan, MIT’s chief innovation and strategy officer, dean of the School of Engineering, and the Vannevar Bush Professor of Electrical Engineering and Computer Science.&lt;/p&gt;

&lt;p&gt;In addition to her role as a faculty member in AeroAstro, Shah served as associate dean of Social and Ethical Responsibilities of Computing in the MIT Schwarzman College of Computing from 2019 to 2022, helping launch a coordinated curriculum that engages more than 2,000 students a year at the Institute. She currently directs the Interactive Robotics Group in MIT’s Computer Science and Artificial Intelligence Lab (CSAIL), and MIT’s Industrial Performance Center.&lt;/p&gt;

&lt;p&gt;Shah and her team at the Interactive Robotics Group conduct research that aims to imagine the future of work by designing collaborative robot teammates that enhance human capability. She is expanding the use of human cognitive models for artificial intelligence and has translated her work to manufacturing assembly lines, health-care applications, transportation, and defense. In 2020, Shah co-authored the popular book “What to Expect When You’re Expecting Robots,” which explores the future of human-robot collaboration.&lt;/p&gt;

&lt;p&gt;As an expert on how humans and robots interact in the workforce, Shah was named co-director of the Work of the Future Initiative, a successor group of MIT’s Task Force on the Work of the Future, alongside Ben Armstrong, executive director and research scientist at MIT’s Industrial Performance Center. In March of this year, Shah was named a co-leader of the Working Group on Generative AI and the Work of the Future, alongside Armstrong and Kate Kellogg, the David J. McGrath Jr. Professor of Management and Innovation. The group is examining how generative AI tools can contribute to higher-quality jobs and inclusive access to the latest technologies across sectors.&lt;/p&gt;

&lt;p&gt;Shah’s contributions as both a researcher and educator have been recognized with many awards and honors throughout her career. She was named an associate fellow of the American Institute of Aeronautics and Astronautics (AIAA) in 2017, and in 2018 she was the recipient of the IEEE Robotics and Automation Society Academic Early Career Award. Shah was also named a Bisplinghoff Faculty Fellow, was named to &lt;em&gt;MIT Technology Review&lt;/em&gt;’s TR35 List, and received an NSF Faculty Early Career Development Award. In 2013, her work on human-robot collaboration was included on &lt;em&gt;MIT Technology Review&lt;/em&gt;’s list of 10 Breakthrough Technologies.&lt;/p&gt;

&lt;p&gt;In January 2024, she was appointed to the first-ever AIAA Aerospace Artificial Intelligence Advisory Group, which was founded “to advance the appropriate use of AI technology particularly in aeronautics, aerospace R&amp;amp;D, and space.” Shah currently serves as editor-in-chief of &lt;em&gt;Foundations and Trends in Robotics&lt;/em&gt;, as an editorial board member of the AIAA Progress Series, and as an executive council member of the Association for the Advancement of Artificial Intelligence.&lt;/p&gt;

&lt;p&gt;A dedicated educator, Shah has been recognized for her collaborative and supportive approach as a mentor. She was honored by graduate students as “Committed to Caring” (C2C) in 2019. For the past 10 years, she has served as an advocate, community steward, and mentor for students in her role as head of house of the Sidney Pacific Graduate Community.&lt;/p&gt;

&lt;p&gt;Shah received her bachelor’s and master’s degrees in aeronautical and astronautical engineering, and her PhD in autonomous systems, all from MIT. After receiving her doctoral degree, she joined Boeing as a postdoc, before returning to MIT in 2011 as a faculty member.&lt;/p&gt;

&lt;p&gt;Shah succeeds Professor Steven Barrett, who has led AeroAstro as both interim department head and then department head since May 2023.&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202404/Julie-Shah.jpg?itok=4gVCN8Jl" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Professor Julie Shah will lead AeroAstro as of May 1.]]></media:description>
              <media:credit>Photo: M. Scott Brauer</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/alumni">Alumni/ae</category>
      <category domain="https://news.mit.edu/topic/leadership">Leadership</category>
      <category domain="https://news.mit.edu/topic/administration">Administration</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>The MIT Edgerton Center’s third annual showcase dazzles onlookers</title>
  <link>https://news.mit.edu/2024/mit-edgerton-center-showcase-0425</link>
  <description><![CDATA[Fourteen Edgerton Center student-led engineering teams displayed their latest creations, from solar cars to rockets to assistive eating devices.]]></description>
  <pubDate>Thu, 25 Apr 2024 16:10:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/mit-edgerton-center-showcase-0425</guid>
        <dc:creator>Sonny Oram | Edgerton Center</dc:creator>
  <content:encoded>&lt;p&gt;On April 9, a trailer with the words “Born by Fire” emblazoned on the back pulled down MIT's North Corridor (a.k.a. the Outfinite). Students, clad in orange construction vests, maneuvered their futuristic creation out of the trailer, eliciting a surge of curious bystanders. The aerodynamic shell is covered by 5 square meters of solar panels. This multi-occupancy solar car, Gemini, designed and built by the&amp;nbsp;&lt;a href="https://www.mitsolar.com/"&gt;Solar Electric Vehicle Team&lt;/a&gt;&amp;nbsp;(SEVT), is slated to race in the 2024 American Solar Challenge. Positioned just outside Building 13, Gemini made its inaugural public appearance at this year’s Edgerton Center Student Teams Showcase. The team’s first-place trophy from an earlier competition sat atop, glistening in the sunlight.&lt;/p&gt;

&lt;p&gt;Next, MIT Motorsports arrived with their shiny red electric race car, MY24.&amp;nbsp;SEVT, embodying MIT's spirit of collaboration, paused their own installation to assist the Motorsports team in transporting MY24 into Lobby 13. Such camaraderie is commonplace among Edgerton teams. MY24 is slated to compete in two upcoming events: the FSAE Hybrid event in Loudon, New Hampshire on May 1, followed by the FSAE Motorsports event in Michigan, later in June.&lt;/p&gt;

&lt;p&gt;At the Third Annual Edgerton Center Showcase, Lobby 13 was abuzz with students, faculty, and visitors drawn in by the passion and excitement of members of 14 Edgerton Center student teams. Team members excitedly unveiled a wide range of technologies, including autonomous waterborne craft, rockets, wind turbines, assistive devices, and hydrogen-powered turbine engines. “Seeing the culmination of what MIT students can build in so many different forms was inspiring. It was great to see everyone's passion and creativity thriving in each of the team's projects,” says junior Anhad Sawhney, president of the MIT Electronics Research Society (MITERS) and captain of the Combat Robotics Club.&lt;/p&gt;

&lt;p&gt;In one corner, children congregated around the Combat Robotics table, captivated by clips of the team competing on the Discovery channel’s Battlebots series. Nearby, towering rockets almost brushing the ceiling captured the gaze of onlookers. Suddenly, a symphony of electrical crackles filled the air. Visitors quickly discovered the source was not an AV malfunction, but a Tesla coil created by MITERS, where lightning danced to the pitch input using a computer keyboard. Established in 1973, MITERS — a member-run project space and machine shop — continues to give students the chance to tinker and create quirky inventions such as the motorized shopping cart, DOOMsled.&lt;/p&gt;

&lt;p&gt;Adjacent to MITERS, students on the Spokes team dished ice cream into a bike-powered blender. A quick ride down the street created milkshakes for many to enjoy. Spokes is an Edgerton team of students who will bike across the country this summer, teaching STEM outreach classes along the way. Their curriculum is inspired by MIT's hands-on approach to education.&lt;/p&gt;

&lt;p&gt;One of the newest Edgerton Center teams, The Assistive Technology Club, showed an array of innovations poised to revolutionize lives. Their&amp;nbsp;blind assistance team is designing an app that uses machine learning to describe the most relevant features of the environment to visually impaired users. Their adaptive game controller team is designing a one-handed game controller for a user who is paralyzed on one side of her body due to a stroke. Junior Ben Lou, from the robotic self-feeding device team, has a rare disease called spinal muscular atrophy. He shares, “Eating is a basic necessity, but current devices that help people like me eat are not versatile with different foods, unaccommodating to users with different positional needs, generally difficult to set up, and extremely expensive.&amp;nbsp;The self-feeding team is completely re-imagining the way a self-feeding device can work. Instead of operating with a spoon, which cannot handle a wide range of foods and is prone to spillage (among other issues), our device operates with an entirely new utensil.”&lt;/p&gt;

&lt;p&gt;Beyond showcasing projects, the event served as a forum for idea exchange and collaboration. The MIT Wind team brought their first working prototype of their model wind turbine, which they will use as a baseline for competing in the Collegiate Wind Competition next year. “We hope to continue working on rotor optimization and blade fabrication, power conversion, and offshore foundation design to be competitive with the other CWC teams next year,” says team captain Kirby Heck. “As a new Edgerton Center team, the showcase was an amazing opportunity for our team members to engage with industry partners, interact with the MIT community, and explore how we fit within the broader constellation of teams within Edgerton at MIT. We also received helpful feedback on our current design and have plenty of new ideas on how we can innovate for our next design iteration.”&amp;nbsp;&lt;/p&gt;

&lt;p&gt;The event included a short program, where SEVT captain Adrienne Wing Suen Lai and first-year Rachel&amp;nbsp;Mohommed of the Electric Vehicle Team gave a shout-out to all the teams. A special tribute was also paid to Peggy Eysenbach, the event's organizer and the development officer at the Edgerton Center, with a bouquet of flowers. Edgerton Center Director and Professor Kim Vandiver welcomed the MIT community to the event and gave a brief review of the 30-year history of engineering teams sponsored by the Edgerton Center.&lt;/p&gt;

&lt;p&gt;Vandiver believes that through all the fun and creativity, strong careers emerge. “Participation in an engineering team is great professional preparation. Upon graduation, these leaders are unafraid of hard problems, and rapidly rise in project management roles,” Vandiver says.&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202404/mit-motorsports-car-2024.JPG?itok=HJrqSfoa" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Students help the MIT Motorsports team push their car into Lobby 13 for the 2024 MIT Edgerton Center student showcase. ]]></media:description>
              <media:credit>Photo: Sonny Oram/MIT Edgerton Center</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/special-events">Special events and guest speakers</category>
      <category domain="https://news.mit.edu/topic/edgerton">Edgerton</category>
      <category domain="https://news.mit.edu/topic/clubs-and-activities">Clubs and activities</category>
      <category domain="https://news.mit.edu/topic/community">Community</category>
      <category domain="https://news.mit.edu/topic/student-life">Student life</category>
      <category domain="https://news.mit.edu/topic/students">Students</category>
      <category domain="https://news.mit.edu/topic/transportation">Transportation</category>
      <category domain="https://news.mit.edu/topic/autonomous-vehicles">Autonomous vehicles</category>
      <category domain="https://news.mit.edu/topic/electric-vehicles">Electric vehicles</category>
      <category domain="https://news.mit.edu/topic/solar-power">Solar power</category>
      <category domain="https://news.mit.edu/topic/wind">Wind</category>
      <category domain="https://news.mit.edu/topic/alternative-energy">Alternative energy</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/assistive-technology">Assistive technology</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/innovation">Innovation and Entrepreneurship (I&amp;E)</category>
    </item>
<item>
  <title>Extracting hydrogen from rocks</title>
  <link>https://news.mit.edu/2024/iwnetim-abate-aims-extract-hydrogen-rocks-0408</link>
  <description><![CDATA[Iwnetim Abate aims to stimulate natural hydrogen production underground, potentially unearthing a new path to a cheap, carbon-free energy source.]]></description>
  <pubDate>Mon, 08 Apr 2024 17:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/iwnetim-abate-aims-extract-hydrogen-rocks-0408</guid>
        <dc:creator>Jason Sparapani | Department of Materials Science and Engineering</dc:creator>
  <content:encoded>&lt;p&gt;It’s commonly thought that the most abundant element in the universe, hydrogen, exists mainly alongside other elements — with oxygen in water, for example, and with carbon in methane. But naturally occurring underground pockets of pure hydrogen are punching holes in that notion — and generating attention as a potentially unlimited source of carbon-free power.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
One interested party is the U.S. Department of Energy, which last month awarded $20 million in research grants to 18 teams from laboratories, universities, and private companies to develop technologies that can lead to cheap, clean fuel from the subsurface.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
Geologic hydrogen, as it’s known, is produced when water reacts with iron-rich rocks, causing the iron to oxidize. One of the grant recipients, MIT Assistant Professor Iwnetim Abate’s research group, will use its $1.3 million grant to determine the ideal conditions for producing hydrogen underground — considering factors such as catalysts to initiate the chemical reaction, temperature, pressure, and pH levels. The goal is to improve efficiency for large-scale production, meeting global energy needs at a competitive cost.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
The U.S. Geological Survey estimates there are potentially billions of tons of geologic hydrogen buried in the Earth’s crust. Accumulations have been discovered worldwide, and a slew of startups are searching for extractable deposits. Abate is looking to jump-start the natural hydrogen production process, implementing “proactive” approaches that involve stimulating production and harvesting the gas.&lt;br /&gt;
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;br /&gt;
“We aim to optimize the reaction parameters to make the reaction faster and produce hydrogen in an economically feasible manner,” says Abate, the Chipman Development Professor in the Department of Materials Science and Engineering (DMSE).&amp;nbsp;Abate’s research&amp;nbsp;centers on designing materials and technologies for the renewable energy transition, including next-generation batteries and novel chemical methods for energy storage.&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sparking innovation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interest in geologic hydrogen is growing at a time when governments worldwide are seeking carbon-free energy alternatives to oil and gas. In December, French President Emmanuel Macron said&amp;nbsp;his government would &lt;a href="https://sciencebusiness.net/news/universities/emmanuel-macron-announces-ambitious-research-reforms"&gt;provide funding&lt;/a&gt;&amp;nbsp;to explore natural hydrogen. And in February, government and private sector witnesses&amp;nbsp;&lt;a href="https://www.energy.senate.gov/hearings/2024/2/full-committee-hearing-to-examine-the-opportunities-and-challenges-associated-with-developing-geologic-hydrogen-in-the-united-states"&gt;briefed U.S. lawmakers&lt;/a&gt;&amp;nbsp;on opportunities to extract hydrogen from the ground.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
Today commercial hydrogen is manufactured at $2 a kilogram, mostly for fertilizer and chemical and steel production, but most methods involve burning fossil fuels, which release Earth-heating carbon.&amp;nbsp;“&lt;a href="https://dmse.mit.edu/news/dmse-alums-green-hydrogen-innovation-for-clean-"&gt;Green hydrogen&lt;/a&gt;,” produced with renewable energy, is promising, but at $7 per kilogram, it’s expensive.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“If you get hydrogen at a dollar a kilo, it’s competitive with natural gas on an energy-price basis,” says Douglas Wicks, a program director at Advanced Research Projects Agency - Energy (ARPA-E), the Department of Energy organization leading the geologic hydrogen grant program.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
Recipients of the&amp;nbsp;&lt;a href="https://arpa-e.energy.gov/technologies/exploratory-topics/geologic-hydrogen"&gt;ARPA-E grants&lt;/a&gt;&amp;nbsp;include Colorado School of Mines, Texas Tech University, and Los Alamos National Laboratory, plus private companies including Koloma, a hydrogen production startup that has received funding from Amazon and Bill Gates. The projects themselves are diverse, ranging from applying industrial oil and gas methods for hydrogen production and extraction to developing models to understand hydrogen formation in rocks. The purpose: to address questions in what Wicks calls a “total white space.”&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“In geologic hydrogen, we don’t know how we can accelerate the production of it, because it’s a chemical reaction, nor do we really understand how to engineer the subsurface so that we can safely extract it,” Wicks says. “We’re trying to bring in the best skills of each of the different groups to work on this under the idea that the ensemble should be able to give us good answers in a fairly rapid timeframe.”&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
Geochemist Viacheslav Zgonnik, one of the foremost experts in the natural hydrogen field, agrees that the list of unknowns is long, as is the road to the first commercial projects. But he says efforts to stimulate hydrogen production — to harness the natural reaction between water and rock — present “tremendous potential.”&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“The idea is to find ways we can accelerate that reaction and control it so we can produce hydrogen on demand in specific places,” says Zgonnik, CEO and founder of Natural Hydrogen Energy, a Denver-based startup that has mineral leases for exploratory drilling in the United States. “If we can achieve that goal, it means that we can potentially replace fossil fuels with stimulated hydrogen.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“A full-circle moment”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For Abate, the connection to the project is personal. As a child in his hometown in Ethiopia, power outages were a usual occurrence — the lights would be out three, maybe four days a week. Flickering candles or pollutant-emitting kerosene lamps were often the only source of light for doing homework at night.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“And for the household, we had to use wood and charcoal for chores such as cooking,” says Abate. “That was my story all the way until the end of high school and before I came to the U.S. for college.”&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
In 1987, well-diggers drilling for water in Mali in Western Africa&amp;nbsp;&lt;a href="https://www.science.org/content/article/hidden-hydrogen-earth-may-hold-vast-stores-renewable-carbon-free-fuel"&gt;uncovered a natural hydrogen deposit&lt;/a&gt;, causing an explosion. Decades later, Malian entrepreneur Aliou Diallo and his Canadian oil and gas company tapped the well and used an engine to burn hydrogen and power electricity in the nearby village.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
Ditching oil and gas, Diallo launched Hydroma, the world’s first hydrogen exploration enterprise. The company is drilling wells near the original site that have yielded high concentrations of the gas.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“So, what used to be known as an energy-poor continent now is generating hope for the future of the world,” Abate says. “Learning about that was a full-circle moment for me. Of course, the problem is global; the solution is global. But then the connection with my personal journey, plus the solution coming from my home continent, makes me personally connected to the problem and to the solution.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiments that scale&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Abate and researchers in his lab are formulating a recipe for a fluid that will induce the chemical reaction that triggers hydrogen production in rocks. The main ingredient is water, and the team is testing “simple” materials for catalysts that will speed up the reaction and in turn increase the amount of hydrogen produced, says postdoc Yifan Gao.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“Some catalysts are very costly and hard to produce, requiring complex production or preparation,” Gao says. “A catalyst that’s inexpensive and abundant will allow us to enhance the production rate — that way, we produce it at an economically feasible rate, but also with an economically feasible yield.”&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
The iron-rich rocks in which the chemical reaction happens can be found across the United States and the world. To optimize the reaction across a diversity of geological compositions and environments, Abate and Gao are developing what they call a high-throughput system, consisting of artificial intelligence software and robotics, to test different catalyst mixtures and simulate what would happen when applied to rocks from various regions, with different external conditions like temperature and pressure.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“And from that we measure how much hydrogen we are producing for each possible combination,” Abate says. “Then the AI will learn from the experiments and suggest to us, ‘Based on what I’ve learned and based on the literature, I suggest you test this composition of catalyst material for this rock.’”&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
The team is writing a paper on its project and aims to publish its findings in the coming months.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
The next milestones for the project, after developing the catalyst recipe, is designing a reactor that will serve two purposes. First, fitted with technologies such as Raman spectroscopy, it will allow researchers to identify and optimize the chemical conditions that lead to improved rates and yield of hydrogen production. The lab-scale device will also inform the design of a real-world reactor that can accelerate hydrogen production in the field.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“That would be a plant-scale reactor that would be implanted into the subsurface,” Abate says.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
The cross-disciplinary project is also tapping the expertise of Yang Shao-Horn, of MIT’s Department of Mechanical Engineering and DMSE, for computational analysis of the catalyst, and Esteban Gazel, a Cornell University scientist who will lend his expertise in geology and geochemistry. He’ll focus on understanding the iron-rich ultramafic rock formations across the United States and the globe and how they react with water.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
For Wicks at ARPA-E, the questions Abate and the other grant recipients are asking are just the first, critical steps in uncharted energy territory.&lt;br /&gt;
&amp;nbsp;&lt;br /&gt;
“If we can understand how to stimulate these rocks into generating hydrogen, safely getting it up, it really unleashes the potential energy source,” he says. Then the emerging industry will look to oil and gas for the drilling, piping, and gas extraction know-how. “As I like to say, this is enabling technology that we hope to, in a very short term, enable us to say, ‘Is there really something there?’”&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/mit-Iwnetim-Abate.jpg?itok=kGePT3Z7" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Assistant Professor Iwnetim Abate is leading an effort to determine the ideal conditions for producing hydrogen underground.]]></media:description>
              <media:credit>Photo: Gretchen Ertl</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/dmse">DMSE</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/energy">Energy</category>
      <category domain="https://news.mit.edu/topic/alternative-energy">Alternative energy</category>
      <category domain="https://news.mit.edu/topic/renewable-energy">Renewable energy</category>
      <category domain="https://news.mit.edu/topic/geology">Geology</category>
      <category domain="https://news.mit.edu/topic/faculty">Faculty</category>
      <category domain="https://news.mit.edu/topic/doe">Department of Energy (DoE)</category>
      <category domain="https://news.mit.edu/topic/climate-change">Climate change</category>
      <category domain="https://news.mit.edu/topic/sustainability">Sustainability</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/funding">Funding</category>
    </item>
<item>
  <title>MIT engineers design flexible “skeletons” for soft, muscle-powered robots</title>
  <link>https://news.mit.edu/2024/mit-engineers-design-flexible-skeletons-muscle-powered-robots-0408</link>
  <description><![CDATA[New modular, spring-like devices maximize the work of live muscle fibers so they can be harnessed to power biohybrid bots.]]></description>
  <pubDate>Mon, 08 Apr 2024 11:40:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/mit-engineers-design-flexible-skeletons-muscle-powered-robots-0408</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Our muscles are nature’s perfect actuators — devices that turn energy into motion. For their size, muscle fibers are more powerful and precise than most synthetic actuators. They can even heal from damage and grow stronger with exercise.&lt;/p&gt;

&lt;p&gt;For these reasons, engineers are exploring ways to power robots with natural muscles. They’ve demonstrated a handful of “biohybrid” robots that use muscle-based actuators to power artificial skeletons that walk, swim, pump, and grip. But for every bot, there’s a very different build, and no general blueprint for how to get the most out of muscles for any given robot design.&lt;/p&gt;

&lt;p&gt;Now, MIT engineers have developed a spring-like device that could be used as a basic skeleton-like module for almost any muscle-bound bot. The new spring, or “flexure,” is designed to get the most work out of any attached muscle tissues. Like a leg press that’s fit with just the right amount of weight, the device maximizes the amount of movement that a muscle can naturally produce.&lt;/p&gt;

&lt;p&gt;The researchers found that when they fit a ring of muscle tissue onto the device, much like a rubber band stretched around two posts, the muscle pulled on the spring, reliably and repeatedly, and stretched it five times more, compared with other previous device designs.&lt;/p&gt;

&lt;p&gt;The team sees the flexure design as a new building block that can be combined with other flexures to build any configuration of artificial skeletons. Engineers can then fit the skeletons with muscle tissues to power their movements.&lt;/p&gt;

&lt;p&gt;“These flexures are like a skeleton that people can now use to turn muscle actuation into multiple degrees of freedom of motion in a very predictable way,” says Ritu Raman, the Brit and Alex d'Arbeloff Career Development Professor in Engineering Design at MIT. “We are giving roboticists a new set of rules to make powerful and precise muscle-powered robots that do interesting things.”&lt;/p&gt;

&lt;p&gt;Raman and her colleagues report the details of the new flexure design in a &lt;a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202300834" target="_blank"&gt;paper appearing today&lt;/a&gt; in the journal &lt;em&gt;Advanced Intelligent&amp;nbsp;Systems.&lt;/em&gt; The study’s MIT co-authors include Naomi Lynch ’12, SM ’23; undergraduate Tara Sheehan; graduate students Nicolas Castro, Laura Rosado, and Brandon Rios; and professor of mechanical engineering Martin Culpepper.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Muscle pull&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When left alone in a petri dish in favorable conditions, muscle tissue will contract on its own but in directions that are not entirely predictable or of much use.&lt;/p&gt;

&lt;p&gt;“If muscle is not attached to anything, it will move a lot, but with huge variability, where it’s just flailing around in liquid,” Raman says.&lt;/p&gt;

&lt;p&gt;To get a muscle to work like a mechanical actuator, engineers typically attach a band of muscle tissue between two small, flexible posts. As the muscle band naturally contracts, it can bend the posts and pull them together, producing some movement that would ideally power part of a robotic skeleton. But in these designs, muscles have produced limited movement, mainly because the tissues are so variable in how they contact the posts. Depending on where the muscles are placed on the posts, and how much of the muscle surface is touching the post, the muscles may succeed in pulling the posts together but at other times may wobble around in uncontrollable ways.&lt;/p&gt;

&lt;p&gt;Raman’s group looked to design a skeleton that focuses and maximizes a muscle’s contractions regardless of exactly where and how it is placed on a skeleton, to generate the most movement in a predictable, reliable way.&lt;/p&gt;

&lt;p&gt;“The question is: How do we design a skeleton that most efficiently uses the force the muscle&amp;nbsp;is generating?” Raman says.&lt;/p&gt;

&lt;p&gt;The researchers first considered the multiple directions that a muscle can naturally move. They reasoned that if a muscle is to pull two posts together along a specific direction, the posts should be connected to a spring that only allows them to move in that direction when pulled.&lt;/p&gt;

&lt;p&gt;“We need a device that is very soft and flexible in one direction, and very stiff in all other directions, so that when a muscle contracts, all that force gets efficiently converted into motion in one direction,” Raman says.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Soft flex&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, Raman found many such devices in Professor Martin Culpepper’s lab. Culpepper’s group at MIT specializes in the design and fabrication of machine elements such as miniature actuators, bearings, and other mechanisms, that can be built into machines and systems to enable ultraprecise movement, measurement, and control, for a wide variety of applications. Among the group’s precision machined elements are flexures — spring-like devices, often made from parallel beams, that can flex and stretch with nanometer precision.&lt;/p&gt;

&lt;p&gt;“Depending on how thin and far apart the beams are, you can change how stiff the spring appears to be,” Raman says.&lt;/p&gt;

&lt;p&gt;She and Culpepper teamed up to design a flexure specifically tailored with a configuration and stiffness to enable muscle tissue to naturally contract and maximally stretch the spring. The team designed the device’s configuration and dimensions based on numerous calculations they carried out to relate a muscle’s natural forces with a flexure’s stiffness and degree of movement.&lt;/p&gt;

&lt;p&gt;The flexure they ultimately designed is 1/100 the stiffness of muscle tissue itself. The device resembles a miniature, accordion-like structure, the corners of which are pinned to an underlying base by a small post, which sits near a neighboring post that is fit directly onto the base. Raman then wrapped a band of muscle around the two corner posts (the team molded the bands from live muscle fibers that they grew from mouse cells), and measured how close the posts were pulled together as the muscle band contracted.&lt;/p&gt;

&lt;p&gt;The team found that the flexure’s configuration enabled the muscle band to contract mostly along the direction between the two posts. This focused contraction allowed the muscle to pull the posts much closer together — five times closer — compared with previous muscle actuator designs.&lt;/p&gt;

&lt;p&gt;“The flexure is a skeleton that we designed to be very soft and flexible in one direction, and very stiff in all other directions,” Raman says. “When the muscle contracts, all the force is converted into movement in that direction. It’s a huge magnification.”&lt;/p&gt;

&lt;p&gt;The team found they could use the device to precisely measure muscle performance and endurance. When they varied the frequency of muscle contractions (for instance, stimulating the bands to contract once versus four times per second), they observed that the muscles “grew tired” at higher frequencies, and didn’t generate as much pull.&lt;/p&gt;

&lt;p&gt;“Looking at how quickly our muscles get tired, and how we can exercise them to have high-endurance responses — this is what we can uncover with this platform,” Raman says.&lt;/p&gt;

&lt;p&gt;The researchers are now adapting and combining flexures to build precise, articulated, and reliable robots, powered by natural muscles.&lt;/p&gt;

&lt;p&gt;“An example of a robot we are trying to build in the future is a surgical robot that can perform minimally invasive procedures inside the body,” Raman says. “Technically, muscles can power robots of any size, but we are particularly excited in making small robots, as this is where biological actuators excel in terms of strength, efficiency, and adaptability.”&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/MIT_Muscle-Flex-01-press.jpg?itok=qfcotChs" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT engineers have developed a new spring (shown in Petri dish) that maximizes the work of natural muscles. When living muscle tissue is attached to posts at the corners of the device, the muscle’s contractions pull on the spring, forming an effective, natural actuator. The spring can serve as a “skeleton” for future muscle-powered robots.]]></media:description>
              <media:credit>Image: Felice Frankel</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/bio-bioeng-biotech">Bioengineering and biotechnology</category>
      <category domain="https://news.mit.edu/topic/biomechanics">Biomechanics</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/medical-devices">Medical devices</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Engineering household robots to have a little common sense </title>
  <link>https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325</link>
  <description><![CDATA[With help from a large language model, MIT engineers enabled robots to self-correct after missteps and carry on with their chores.]]></description>
  <pubDate>Mon, 25 Mar 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;From wiping up spills to serving up food, robots are being taught to carry out increasingly complicated household tasks. Many such home-bot trainees are learning through imitation; they are programmed to copy the motions that a human physically guides them through.&lt;/p&gt;

&lt;p&gt;It turns out that robots are excellent mimics. But unless engineers also program them to adjust to every possible bump and nudge, robots don’t necessarily know how to handle these situations, short of starting their task from the top.&lt;/p&gt;

&lt;p&gt;Now MIT engineers are aiming to give robots a bit of common sense when faced with situations that push them off their trained path. They’ve developed a method that connects robot motion data with the “common sense knowledge” of large language models, or LLMs.&lt;/p&gt;

&lt;p&gt;Their approach enables a robot to logically parse many given household task into subtasks, and to physically adjust to disruptions within a subtask so that the robot can move on without having to go back and start a task from scratch — and without engineers having to explicitly program fixes for every possible failure along the way. &amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;img alt="A robotic hand tries to scoop up red marbles and put them into another bowl while a researcher’s hand frequently disrupts it. The robot eventually succeeds." data-align="center" data-caption="Image courtesy of the researchers." data-entity-type="file" data-entity-uuid="49962673-31d2-4023-893e-cb462fed9a91" src="/sites/default/files/images/inline/ComonsenseBots-ani_1.gif" /&gt;
&lt;p&gt;“Imitation learning is a mainstream approach enabling household robots. But if a robot is blindly mimicking a human’s motion trajectories, tiny errors can accumulate and eventually derail the rest of the execution,” says Yanwei Wang, a graduate student in MIT’s Department of Electrical Engineering and Computer Science (EECS). “With our method, a robot can self-correct execution errors and improve overall task success.”&lt;/p&gt;

&lt;p&gt;Wang and his colleagues detail their new approach in a &lt;a href="https://openreview.net/forum?id=qoHeuRAcSl" target="_blank"&gt;study&lt;/a&gt; they will present at the International Conference on Learning Representations (ICLR) in May. The study’s co-authors include EECS graduate students Tsun-Hsuan Wang and Jiayuan Mao, Michael Hagenow, a postdoc in MIT’s Department of Aeronautics and Astronautics (AeroAstro), and Julie Shah, the H.N. Slater Professor in Aeronautics and Astronautics at MIT.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Language task&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The researchers illustrate their new approach with a simple chore: scooping marbles from one bowl and pouring them into another. To accomplish this task, engineers would typically move a robot through the motions of scooping and pouring — all in one fluid trajectory. They might do this multiple times, to give the robot a number of human demonstrations to mimic.&lt;/p&gt;

&lt;p&gt;“But the human demonstration is one long, continuous trajectory,” Wang says.&lt;/p&gt;

&lt;p&gt;The team realized that, while a human might demonstrate a single task in one go, that task depends on a sequence of subtasks, or trajectories. For instance, the robot has to first reach into a bowl before it can scoop, and it must scoop up marbles before moving to the empty bowl, and so forth. If a robot is pushed or nudged to make a mistake during any of these subtasks, its only recourse is to stop and start from the beginning, unless engineers were to explicitly label each subtask and program or collect new demonstrations for the robot to recover from the said failure, to enable a robot to self-correct in the moment.&lt;/p&gt;

&lt;p&gt;“That level of planning is very tedious,” Wang says.&lt;/p&gt;

&lt;p&gt;Instead, he and his colleagues found some of this work could be done automatically by LLMs. These deep learning models process immense libraries of text, which they use to establish connections between words, sentences, and paragraphs. Through these connections, an LLM can then generate new sentences based on what it has learned about the kind of word that is likely to follow the last.&lt;/p&gt;

&lt;p&gt;For their part, the researchers found that in addition to sentences and paragraphs, an LLM can be prompted to produce a logical list of subtasks that would be involved in a given task. For instance, if queried to list the actions involved in scooping marbles from one bowl into another, an LLM might produce a sequence of verbs such as “reach,” “scoop,” “transport,” and “pour.”&lt;/p&gt;

&lt;p&gt;“LLMs have a way to tell you how to do each step of a task, in natural language. A human’s continuous demonstration is the embodiment of those steps, in physical space,” Wang says. “And we wanted to connect the two, so that a robot would automatically know what stage it is in a task, and be able to replan and recover on its own.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mapping marbles&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For their new approach, the team developed an algorithm to automatically connect an LLM’s natural language label for a particular subtask with a robot’s position in physical space or an image that encodes the robot state. Mapping a robot’s physical coordinates, or an image of the robot state, to a natural language label is known as “grounding.” The team’s new algorithm is designed to learn a grounding “classifier,” meaning that it learns to automatically identify what semantic subtask a robot is in — for example, “reach” versus “scoop” — given its physical coordinates or an image view.&lt;/p&gt;

&lt;p&gt;“The grounding classifier facilitates this dialogue between what the robot is doing in the physical space and what the LLM knows about the subtasks, and the constraints you have to pay attention to within each subtask,” Wang explains.&lt;/p&gt;

&lt;p&gt;The team demonstrated the approach in experiments with a robotic arm that they trained on a marble-scooping task. Experimenters trained the robot by physically guiding it through the task of first reaching into a bowl, scooping up marbles, transporting them over an empty bowl, and pouring them in. After a few demonstrations, the team then used a pretrained LLM and asked the model to list the steps involved in scooping marbles from one bowl to another. The researchers then used their new algorithm to connect the LLM’s defined subtasks with the robot’s motion trajectory data. The algorithm automatically learned to map the robot’s physical coordinates in the trajectories and the corresponding image view to a given subtask.&lt;/p&gt;

&lt;p&gt;The team then let the robot carry out the scooping task on its own, using the newly learned grounding classifiers. As the robot moved through the steps of the task, the experimenters pushed and nudged the bot off its path, and knocked marbles off its spoon at various points. Rather than stop and start from the beginning again, or continue blindly with no marbles on its spoon, the bot was able to self-correct, and completed each subtask before moving on to the next. (For instance, it would make sure that it successfully scooped marbles before transporting them to the empty bowl.)&lt;/p&gt;

&lt;p&gt;“With our method, when the robot is making mistakes, we don’t need to ask humans to program or give extra demonstrations of how to recover from failures,” Wang says. “That’s super exciting because there’s a huge effort now toward training household robots with data collected on teleoperation systems. Our algorithm can now convert that training data into robust robot behavior that can do complex tasks, despite external perturbations.”&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/CommonSense-01-press.jpg?itok=VbWDKM8h" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[In this collaged image, a robotic hand tries to scoop up red marbles and put them into another bowl while a researcher’s hand frequently disrupts it. The robot eventually succeeds.]]></media:description>
              <media:credit>Image: Jose-Luis Olivares, MIT. Stills courtesy of the researchers</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/assistive-technology">Assistive technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/computer-modeling">Computer modeling</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/human-computer-interaction">Human-computer interaction</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Researchers help robots navigate efficiently in uncertain environments</title>
  <link>https://news.mit.edu/2024/researchers-help-robots-navigate-efficiently-uncertain-environments-0314</link>
  <description><![CDATA[A new algorithm reduces travel time by identifying shortcuts a robot could take on the way to its destination. ]]></description>
  <pubDate>Thu, 14 Mar 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/researchers-help-robots-navigate-efficiently-uncertain-environments-0314</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;If a robot traveling to a destination has just two possible paths, it needs only to compare the routes’ travel time and probability of success. But if the robot is traversing a complex environment with many possible paths, choosing the best route amid so much uncertainty can quickly become an intractable problem.&lt;/p&gt;

&lt;p&gt;MIT researchers developed a method that could help this robot efficiently reason about the best routes to its destination. They created an algorithm for constructing roadmaps of an uncertain environment that balances the tradeoff between roadmap quality and computational efficiency, enabling the robot to quickly find a traversable route that minimizes travel time.&lt;/p&gt;

&lt;p&gt;The algorithm starts with paths that are certain to be safe and automatically finds shortcuts the robot could take to reduce the overall travel time. In simulated experiments, the researchers found that their algorithm can achieve a better balance between planning performance and efficiency in comparison to other baselines, which prioritize one or the other.&lt;/p&gt;

&lt;p&gt;This algorithm could have applications in areas like exploration, perhaps by helping a robot plan the best way to travel to the edge of a distant crater across the uneven surface of Mars. It could also aid a search-and-rescue drone in finding the quickest route to someone stranded on a remote mountainside.&lt;/p&gt;

&lt;p&gt;“It is unrealistic, especially in very large outdoor environments, that you would know exactly where you can and can’t traverse. But if we have just a little bit of information about our environment, we can use that to build a high-quality roadmap,” says Yasmin Veys, an electrical engineering and computer science (EECS) graduate student and lead author of a &lt;a href="https://groups.csail.mit.edu/rrg/papers/veys_icra_24.pdf" target="_blank"&gt;paper on this technique&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Veys wrote the paper with Martina Stadler Kurtz, a graduate student in the MIT Department of Aeronautics and Astronautics, and senior author Nicholas Roy, an MIT professor of aeronautics and astronautics and a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented at the International Conference on Robotics and Automation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generating graphs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To study motion planning, researchers often think about a robot’s environment like a graph, where a series of “edges,” or line segments, represent possible paths between a starting point and a goal.&lt;/p&gt;

&lt;p&gt;Veys and her collaborators used a graph representation called the Canadian Traveler’s Problem (CTP), which draws its name from frustrated Canadian motorists who must turn back and find a new route when the road ahead is blocked by snow.&lt;/p&gt;

&lt;p&gt;In a CTP, each edge of the graph has a weight associated with it, which represents how long that path will take to traverse, and a probability of how likely it is to be traversable. The goal in a CTP is to minimize travel time to the destination.&lt;/p&gt;

&lt;p&gt;The researchers focused on how to automatically generate a CTP graph that effectively represents an uncertain environment.&lt;/p&gt;

&lt;p&gt;“If we are navigating in an environment, it is possible that we have some information, so we are not just going in blind. While it isn’t a detailed navigation plan, it gives us a sense of what we are working with. The crux of this work is trying to capture that within the CTP graph,” adds Kurtz.&lt;/p&gt;

&lt;p&gt;Their algorithm assumes this partial information — perhaps a satellite image — can be divided into specific areas (a lake might be one area, an open field another, etc.)&lt;/p&gt;

&lt;p&gt;Each area has a probability that the robot can travel across it. For instance, it is more likely a nonaquatic robot can drive across a field than through a lake, so the probability for a field would be higher.&lt;/p&gt;

&lt;p&gt;The algorithm uses this information to build an initial graph through open space, mapping out a conservative path that is slow but definitely traversable. Then it uses a metric the team developed to determine which edges, or shortcut paths through uncertain regions, should be added to the graph to cut down on the overall travel time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Selecting shortcuts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By only selecting shortcuts that are likely to be traversable, the algorithm keeps the planning process from becoming needlessly complicated.&lt;/p&gt;

&lt;p&gt;“The quality of the motion plan is dependent on the quality of graph. If that graph doesn’t have good paths in it, then the algorithm can’t give you a good plan,” Veys explains.&lt;/p&gt;

&lt;p&gt;After testing the algorithm in more than 100 simulated experiments with increasingly complex environments, the researchers found that it could consistently outperform baseline methods that don’t consider probabilities. They also tested it using an aerial campus map of MIT to show that it could be effective in real-world, urban environments.&lt;/p&gt;

&lt;p&gt;In the future, they want to enhance the algorithm so it can work in more than two dimensions, which could enable its use for complicated robotic manipulation problems. They are also interested in studying the mismatch between CTP graphs and the real-world environments those graphs represent.&lt;/p&gt;

&lt;p&gt;“Robots that operate in the real world are plagued by uncertainty, whether in the available sensor data, prior knowledge about the environment, or about how other agents will behave. Unfortunately, dealing with these uncertainties incurs a high computational cost,” says Seth Hutchinson, professor and KUKA Chair for Robotics in the School of Interactive Computing at Georgia Tech, who was not involved with this research. “This work addresses these issues by proposing a clever approximation scheme that can be used to efficiently compute uncertainty-tolerant plans.”&lt;/p&gt;

&lt;p&gt;This research was funded, in part, by the U.S. Army Research Labs under the Distributed Collaborative Intelligent Systems and Technologies Collaborative Research Alliance and by the Joseph T. Corso and Lily Corso Graduate Fellowship.&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/MIT-Uncertain-Planning-01-press.jpg?itok=qz8J-0UF" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT researchers developed an algorithm that can automatically select the best shortcuts for a robot to take on its way to a destination that will reduce the overall travel time while limiting the likelihood that the robot will meet an impassable obstacle.]]></media:description>
              <media:credit>Image: Jose-Luis Olivares, MIT; iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>“Imagine it, build it” at MIT</title>
  <link>https://news.mit.edu/2024/imagine-it-build-it-at-mit-0314</link>
  <description><![CDATA[In class 2.679 (Electronics for Mechanical Systems II) a hands-on approach provides the skills engineers use to create and solve problems.]]></description>
  <pubDate>Thu, 14 Mar 2024 00:00:00 -0400</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/imagine-it-build-it-at-mit-0314</guid>
        <dc:creator>Michaela Jarvis | Department of Mechanical Engineering</dc:creator>
  <content:encoded>&lt;p&gt;MIT class 2.679 (Electronics for Mechanical Systems II) offers a sort of alchemy that transforms students from consumers of knowledge to explorers and innovators, and equips them with a range of important new tools at their disposal, students say.&lt;/p&gt;

&lt;p&gt;“Topics which could otherwise feel intimidating are well-scoped each week so that students come out knowing not only what a concept is, but why it’s useful and how to actually implement it,” says graduating senior Audrey Chen. “I could consistently come in with no background and come out with practical experience I could use in future projects. I’d describe the class as a series of small crash courses [each of which] answers, simply, ‘what do I need to know to do or use this thing?’”&lt;/p&gt;

&lt;p&gt;The course takes students through the process of design, fabrication, and assembly of a printed circuit board (PCB). Ultimately, that process, which has twists and turns depending on each student’s project idea, culminates in incorporating the PCB into a device — in a sense animating that device to perform a certain function.&lt;/p&gt;

&lt;p&gt;“The design intent of 2.679 is to empower students to ‘imagine it, build it,’” says Tonio Buonassisi, professor of mechanical engineering. "Between those two is a universe, and the purpose of this class is to aid aspiring engineers to bridge that gap.”&lt;/p&gt;
&lt;p&gt;Senior Jessica Lam marvels at how much she learned in the course over its one short semester, attributing that flood of education to the class labs being “incredibly well-structured.”&lt;/p&gt;

&lt;p&gt;“I’ve found that in a lot of other labs and project-based classes, they throw a lot of information at you at once with the expectation that you already have some experience with certain software or hardware, and most of it is scaffolded and feels like a black box,” without much understanding of what is actually happening, Lam says. “In 2.679, Steve Banzaert has a better understanding of what we already know and how to build on that.”&lt;/p&gt;

&lt;p&gt;After taking 2.679, she says she feels “a lot more confident in designing electrical systems, and I have a more comprehensive understanding of how to integrate mechanical systems and electronics.”&lt;/p&gt;

&lt;p&gt;Banzaert, technical instructor for the course, says the class is designed to guide students along their own chosen paths of discovery, showing them that they are able to address the challenges they encounter along the way.&lt;/p&gt;

&lt;p&gt;“Every semester we get to see really lovely examples of growth, not just in the course material but, in the best cases, in students’ understanding of what they’re really capable of,” he says.&lt;/p&gt;

&lt;p&gt;Chen, a mechanical engineering major who is graduating early to start a position as a hardware project manager at Formlabs, agrees that the class did just that.&lt;/p&gt;

&lt;p&gt;“Students are given tremendous freedom to pick their own final projects, allowing them to explore topics which are of special interest to them. And because each project is unique, there is less pressure to ‘perform’ in a traditional sense,” she says. “Rather, each student is learning different skills and is encouraged to get as far along with the project they choose as possible. Steve emphasized that the scope of our projects would inevitably change, because at the start you simply don’t yet know what you don’t know, and that’s totally okay!”&lt;/p&gt;

&lt;p&gt;Banzaert says, “We try to make it very clear that, yes, we are talking about important general concepts in theory and analysis, but that’s because they are tools that engineers use to solve problems. I think maybe this focus helps remind the students of what got them here in the first place — that the reason you’re an engineer is because there’s something about the world you wish was better, that you’re the person to do it (or at least help), and, if you want to do it well, you’re going to have to learn a bunch of things so you have more tools in your toolbox.”&lt;/p&gt;

&lt;p&gt;Senior Yasin Hamed designed a car in the class that uses computer vision to follow along a black line. The car has an attached camera that captures images and relays them to a Raspberry Pi computer that is also attached to the car. Processing the images in real time allows the car to locate the black line and turn or go straight while controlling the car’s speed.&lt;/p&gt;

&lt;p&gt;Although Hamed, who is majoring in mechanical engineering with a minor in computer science, had built another similar system in a previous class, he says the focus in the prior class was on the software. With his 2.679 car project, he learned about “the underlying foundation,” meaning “the design of the power electronics and control circuitry which is necessary for everything else to work.”&lt;/p&gt;

&lt;p&gt;“I derived much of the ‘enlightenment’ from this class from the little electronic bits and pieces of information I picked up along the course of the class, like learning/practicing soldering, understand how to use integrated circuits, learning how to design a PCB, etc.,” he says. “It was the collection of all of these things that benefited me the most.”&lt;/p&gt;

&lt;p&gt;Jordan Parker-Ashe, also a senior, appreciated how 2.679 combined lessons about electronics with research and presentations from Buonassisi’s lab. “It’s great seeing engineering applied in research,” she says.&lt;/p&gt;

&lt;p&gt;Although many of the skills she learned in the course were new to her, one was “an old foe,” she says, that 2.679 allowed her to befriend. Parker-Ashe, who is majoring in nuclear engineering, had used a computer vision program called OpenCV in her first Undergraduate Research Opportunities Program project as a first-year undergraduate.&lt;/p&gt;

&lt;p&gt;“It was the hardest thing ever, and it really felt like an insurmountable obstacle then,” she says. “Now, to be using OpenCV in labs and homework effortlessly — It was a very full-circle moment.”&lt;/p&gt;

&lt;p&gt;She says the class has opened up a whole new field to her, with Banzaert having “directly inspired” her to also take class 6.131 (Power Electronics), “which has been life-changing,” she says.&lt;/p&gt;

&lt;p&gt;“2.679 helped me believe in myself, which inspired me to take 6.131, a notorious electrical engineering capstone, which has made me realize that my future lies as a nuclear-electrical engineering engineer, not just a nuclear engineer,” Parker-Ashe says. “I want to pursue electrical engineering in my future, and that just wasn’t on the table beforehand.&lt;/p&gt;

&lt;p&gt;“Not to mention that it’s opened the doors to very rich landscapes for project ideas, creating explorations, art, stepping into new roles in group projects, etc,” she says. "I'm so glad that I've been able to find opportunities in Course 2&amp;nbsp; that helped give me hands-on, applied engineering experience."&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/mit-meche-lab.JPG?itok=UBbj_lHM" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[Mechanical engineering undergraduate Audrey Chen solders together a printed circuit board during a lab for class 2.679 (Electronics for Mechanical Systems II).]]></media:description>
              <media:credit>Photo: John Freidah</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/classes-and-programs">Classes and programs</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/stem-education">STEM education</category>
      <category domain="https://news.mit.edu/topic/electronics">Electronics</category>
      <category domain="https://news.mit.edu/topic/education">Education, teaching, academics</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
    </item>
<item>
  <title>Is this the future of fashion?</title>
  <link>https://news.mit.edu/2024/4d-knit-dress-future-of-fashion-0307</link>
  <description><![CDATA[Developed by the Self-Assembly Lab, the 4D Knit Dress uses several technologies to create a custom design and a custom fit, while addressing sustainability concerns. ]]></description>
  <pubDate>Thu, 07 Mar 2024 17:15:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/4d-knit-dress-future-of-fashion-0307</guid>
        <dc:creator>Maria Iacobo | Olivia Mintz | School of Architecture and Planning</dc:creator>
  <content:encoded>&lt;p&gt;Until recently, bespoke tailoring — clothing made to a customer’s individual specifications — was the only way to have garments that provided the perfect fit for your physique. For most people, the cost of custom tailoring is prohibitive. But the invention of active fibers and innovative knitting processes is changing the textile industry.&lt;/p&gt;

&lt;p&gt;“We all wear clothes and shoes,” says Sasha MicKinlay MArch ’23, a recent graduate of the MIT Department of Architecture. “It’s a human need. But there’s also the human need to express oneself. I like the idea of customizing clothes in a sustainable way. This dress promises to be more sustainable than traditional fashion to both the consumer and the producer.”&lt;/p&gt;

&lt;p&gt;McKinlay is a textile designer and researcher at the Self-Assembly Lab who designed the &lt;a href="https://selfassemblylab.mit.edu/4d-knit-dress"&gt;4D Knit Dress&lt;/a&gt; with Ministry of Supply, a fashion company specializing in high-tech apparel. The dress combines several technologies to create personalized fit and style. Heat-activated yarns, computerized knitting, and robotic activation around each garment generates the sculpted fit. A team at Ministry of Supply led the decisions on the stable yarns, color, original size, and overall design.&lt;/p&gt;

&lt;p&gt;“Everyone’s body is different,” says Skylar Tibbits, associate professor in the Department of Architecture and founder of the Self-Assembly Lab. “Even if you wear the same size as another person, you're not actually the same.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Active textiles&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Students in the Self-Assembly Lab have been working with dynamic textiles for several years. The yarns they create can change shape, change property, change insulation, or become breathable. Previous applications to tailor garments include &lt;a href="https://selfassemblylab.mit.edu/active-textile-tailoring"&gt;making sweaters&lt;/a&gt; and &lt;a href="https://news.mit.edu/2022/form-fitting-face-mask-lavender-tessmer-0718"&gt;face masks&lt;/a&gt;. Tibbits says the 4D Knit Dress is a culmination of everything the students have learned from working with active textiles.&lt;/p&gt;

&lt;p&gt;McKinlay helped produce the active yarns, created the concept design, developed the knitting technique, and programmed the lab’s industrial knitting machine. Once the garment design is programmed into the machine, it can quickly produce multiple dresses. Where the active yarns are placed in the design allows for the dress to take on a variety of styles such as pintucks, pleats, an empire waist, or a cinched waist.&lt;/p&gt;

&lt;p&gt;“The styling is important,” McKinlay says. “Most people focus on the size, but I think styling is what sets clothes apart. We’re all evolving as people, and I think our style evolves as well. After fit, people focus on personal expression.”&lt;/p&gt;

&lt;p&gt;Danny Griffin MArch ’22, a current graduate student in architectural design, doesn’t have a background in garment making or the fashion industry. Tibbits asked Griffin to join the team due to his experience with robotics projects in construction. Griffin translated the heat activation process into a programmable robotic procedure that would precisely control its application.&lt;/p&gt;

&lt;p&gt;“When we apply heat, the fibers shorten, causing the textile to bunch up in a specific zone, effectively tightening the shape as if we’re tailoring the garment,” says Griffin. “There was a lot of trial and error to figure out how to orient the robot and the heat gun. The heat needs to be applied in precise locations to activate the fibers on each garment. Another challenge was setting the temperature and the timing for the heat to be applied.”&lt;/p&gt;

&lt;p&gt;It took a while to determine how the robot could&lt;em&gt; &lt;/em&gt;reach all areas of the dress.&lt;/p&gt;

&lt;p&gt;“We couldn’t use a commercial heat gun — which is like a handheld hair dryer — because they’re too large,” says Griffin. “We needed a more compact design. Once we figured it out, it was a lot of fun to write the script for the robot to follow.”&lt;/p&gt;

&lt;p&gt;A dress can begin with one design — pintucks across the chest, for example — and be worn for months before having heat re-applied to alter its look. Subsequent applications of heat can tailor the dress further.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beyond fit and fashion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Efficiently producing garments is a “big challenge” in the fashion industry, according to Gihan Amarasiriwardena ’11, the co-founder and president of Ministry of Supply.&lt;/p&gt;

&lt;p&gt;“A lot of times you'll be guessing what a season's style is,” he says. “Sometimes the style doesn't do well, or some sizes don’t sell out. They may get discounted very heavily or eventually they end up going to a landfill.”&lt;/p&gt;

&lt;p&gt;“Fast fashion” is a term that describes clothes that are inexpensive, trendy, and easily disposed of by the consumer. They are designed and produced quickly to keep pace with current trends. The 4D Knit Dress, says Tibbits, is the opposite of fast fashion. Unlike the traditional “cut-and-sew” process in the fashion industry, the 4D Knit Dress is made entirely in one piece, which virtually eliminates waste.&lt;/p&gt;

&lt;p&gt;“From a global standpoint, you don’t have tons of excess inventory because the dress is customized to your size,” says Tibbits.&lt;/p&gt;

&lt;p&gt;McKinlay says she hopes use of this new technology will reduce the amount of waste in inventory that retailers usually have at the end of each season.&lt;/p&gt;

&lt;p&gt;“The dress could be tailored in order to adapt to these changes in styles and tastes,” she says. “It may also be able to absorb some of the size variations that retailers need to stock. Instead of extra-small, small, medium, large, and extra-large sizes, retailers may be able to have one dress for the smaller sizes and one for the larger sizes. Of course, these are the same sustainability points that would benefit the consumer.”&lt;/p&gt;

&lt;p&gt;The Self-Assembly Lab has collaborated with Ministry of Supply on projects with active textiles for several years. Late last year, the team debuted the &lt;a href="https://selfassemblylab.mit.edu/4d-knit-dress"&gt;4D Knit Dress&lt;/a&gt; at the company’s flagship store in Boston, complete with a robotic arm working its way around a dress as customers watched. For Amarasiriwardena, it was an opportunity to gauge interest and receive feedback from customers interested in trying the dress on.&lt;/p&gt;

&lt;p&gt;“If the demand is there, this is something we can create quickly” unlike the usual design and manufacturing process, which can take years, says Amarasiriwardena.&lt;/p&gt;

&lt;p&gt;Griffin and McKinlay were on hand for the demonstration and pleased with the results. For Griffin, with the “technical barriers” overcome, he sees many different avenues for the project.&lt;/p&gt;

&lt;p&gt;“This experience leaves me wanting to try more,” he says.&lt;/p&gt;

&lt;p&gt;McKinlay too would love to work on more styles.&lt;/p&gt;

&lt;p&gt;“I&lt;em&gt; &lt;/em&gt;hope this research project helps people rethink or reevaluate their relationship with clothes,” says McKinlay. “Right now when people purchase a piece of clothing it has only one ‘look.’ But, how exciting would it be to purchase one garment and reinvent it to change and evolve as you change or as the seasons or styles change? I'm hoping that's the takeaway that people will have.”&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/4d-knit-dress-mit-00.jpg?itok=ct4VoRVW" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[In late 2023 the MIT Self Assembly Lab and the high-tech fashion company Ministry of Supply debuted their 4D Knit Dress at the latter's store Boston, complete with a robotic arm working its way around a dress as customers watched. ]]></media:description>
              <media:credit>Photo: Olivia Mintz</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/self-assembly">Self-assembly</category>
      <category domain="https://news.mit.edu/topic/design">Design</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/collaboration">Collaboration</category>
      <category domain="https://news.mit.edu/topic/invention">Invention</category>
      <category domain="https://news.mit.edu/topic/architecture">Architecture</category>
      <category domain="https://news.mit.edu/topic/sustainability">Sustainability</category>
      <category domain="https://news.mit.edu/topic/industry">Industry</category>
      <category domain="https://news.mit.edu/topic/innovation">Innovation and Entrepreneurship (I&amp;E)</category>
      <category domain="https://news.mit.edu/topic/technology-society">Technology and society</category>
      <category domain="https://news.mit.edu/topic/school-architecture-and-planning">School of Architecture and Planning</category>
    </item>
<item>
  <title>Method rapidly verifies that a robot will avoid collisions </title>
  <link>https://news.mit.edu/2024/method-rapidly-verifies-robot-will-avoid-collisions-0307</link>
  <description><![CDATA[Faster and more accurate than some alternatives, this approach could be useful for robots that interact with humans or work in tight spaces.]]></description>
  <pubDate>Thu, 07 Mar 2024 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/method-rapidly-verifies-robot-will-avoid-collisions-0307</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Before a robot can grab dishes off a shelf to set the table, it must ensure its gripper and arm won’t crash into anything and potentially shatter the fine china. As part of its motion planning process, a robot typically runs “safety check” algorithms that verify its trajectory is collision-free.&lt;/p&gt;

&lt;p&gt;However, sometimes these algorithms generate false positives, claiming a trajectory is safe when the robot would actually collide with something. Other methods that can avoid false positives are typically too slow for robots in the real world.&lt;/p&gt;

&lt;p&gt;Now, MIT researchers have developed a safety check technique which can prove with 100 percent accuracy that a robot’s trajectory will remain collision-free (assuming the model of the robot and environment is itself accurate). Their method, which is so precise it can discriminate between trajectories that differ by only millimeters, provides proof in only a few seconds.&lt;/p&gt;

&lt;p&gt;But a user doesn’t need to take the researchers’ word for it — the mathematical proof generated by this technique can be checked quickly with relatively simple math.&lt;/p&gt;

&lt;p&gt;The researchers accomplished this using a special algorithmic technique, called sum-of-squares programming, and adapted it to effectively solve the safety check problem. Using sum-of-squares programming enables their method to generalize to a wide range of complex motions.&lt;/p&gt;

&lt;p&gt;This technique could be especially useful for robots that must move rapidly avoid collisions in spaces crowded with objects, such as food preparation robots in a commercial kitchen. It is also well-suited for situations where robot collisions could cause injuries, like home health robots that care for frail patients.&lt;/p&gt;

&lt;p&gt;“With this work, we have shown that you can solve some challenging problems with conceptually simple tools. Sum-of-squares programming is a powerful algorithmic idea, and while it doesn’t solve every problem, if you are careful in how you apply it, you can solve some pretty nontrivial problems,” says Alexandre Amice, an electrical engineering and computer science (EECS) graduate student and lead author of a &lt;a href="https://arxiv.org/pdf/2310.16603.pdf" target="_blank"&gt;paper on this technique&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Amice is joined on the paper fellow EECS graduate student Peter Werner and senior author Russ Tedrake, the Toyota Professor of EECS, Aeronautics and Astronautics, and Mechanical Engineering, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL). The work will be presented at the International Conference on Robots and Automation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Certifying safety&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Many existing methods that check whether a robot’s planned motion is collision-free do so by simulating the trajectory and checking every few seconds to see whether the robot hits anything. But these static safety checks can’t tell if the robot will collide with something in the intermediate seconds.&lt;/p&gt;

&lt;p&gt;This might not be a problem for a robot wandering around an open space with few obstacles, but for robots performing intricate tasks in small spaces, a few seconds of motion can make an enormous difference.&lt;/p&gt;

&lt;p&gt;Conceptually, one way to prove that a robot is not headed for a collision would be to hold up a piece of paper that separates the robot from any obstacles in the environment. Mathematically, this piece of paper is called a hyperplane. Many safety check algorithms work by generating this hyperplane at a single point in time. However, each time the robot moves, a new hyperplane needs to be recomputed to perform the safety check.&lt;/p&gt;

&lt;p&gt;Instead, this new technique generates a hyperplane function that moves with the robot, so it can prove that an entire trajectory is collision-free rather than working one hyperplane at a time.&lt;/p&gt;

&lt;p&gt;The researchers used sum-of-squares programming, an algorithmic toolbox that can effectively turn a static problem into a function. This function is an equation that describes where the hyperplane needs to be at each point in the planned trajectory so it remains collision-free.&lt;/p&gt;

&lt;p&gt;Sum-of-squares can generalize the optimization program to find a family of collision-free hyperplanes. Often, sum-of-squares is considered a heavy optimization that is only suitable for offline use, but the researchers have shown that for this problem it is extremely efficient and accurate.&lt;/p&gt;

&lt;p&gt;“The key here was figuring out how to apply sum-of-squares to our particular problem. The biggest challenge was coming up with the initial formulation. If I don’t want my robot to run into anything, what does that mean mathematically, and can the computer give me an answer?” Amice says.&lt;/p&gt;

&lt;p&gt;In the end, like the name suggests, sum-of-squares produces a function that is the sum of several squared values. The function is always positive, since the square of any number is always a positive value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Trust but verify&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By double-checking that the hyperplane function contains squared values, a human can easily verify that the function is positive, which means the trajectory is collision-free, Amice explains.&lt;/p&gt;

&lt;p&gt;While the method certifies with perfect accuracy, this assumes the user has an accurate model of the robot and environment; the mathematical certifier is only as good as the model.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;“One really nice thing about this approach is that the proofs are really easy to interpret, so you don’t have to trust me that I coded it right because you can check it yourself,” he adds.&lt;/p&gt;

&lt;p&gt;They tested their technique in simulation by certifying that complex motion plans for robots with one and two arms were collision-free. At its slowest, their method took just a few hundred milliseconds to generate a proof, making it much faster than some alternate techniques.&lt;/p&gt;

&lt;p&gt;“This new result suggests a novel approach to certifying that a complex trajectory of a robot manipulator is collision free, elegantly harnessing tools from mathematical optimization, turned into surprisingly fast (and publicly available) software. While not yet providing a complete solution to fast trajectory planning in cluttered environments, this result opens the door to several intriguing directions of further research,” says Dan Halperin, a professor of computer science at Tel Aviv University, who was not involved with this research.&lt;/p&gt;

&lt;p&gt;While their approach is fast enough to be used as a final safety check in some real-world situations, it is still too slow to be implemented directly in a robot motion planning loop, where decisions need to be made in microseconds, Amice says.&lt;/p&gt;

&lt;p&gt;The researchers plan to accelerate their process by ignoring situations that don’t require safety checks, like when the robot is far away from any objects it might collide with. They also want to experiment with specialized optimization solvers that could run faster.&lt;/p&gt;

&lt;p&gt;“Robots often get into trouble by scraping obstacles due to poor approximations that are made when generating their routes.&amp;nbsp;Amice, Werner, and Tedrake have come to the rescue with a powerful new algorithm to quickly ensure that robots never overstep their bounds, by carefully leveraging advanced methods from computational algebraic geometry,” adds Steven LaValle, professor in the Faculty of Information Technology and Electrical Engineering at the University of Oulu in Finland, and who was not involved with this work.&lt;/p&gt;

&lt;p&gt;This work was supported, in part, by Amazon and the U.S. Air Force Research Laboratory.&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/MIT_Motion-Planning-01.jpg?itok=sj72eS9i" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT researchers have developed a rapid safety check technique which can ensure a robot will avoid collisions while completing a task.]]></media:description>
              <media:credit>Credit: iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/mechanical-engineering">Mechanical engineering</category>
      <category domain="https://news.mit.edu/topic/computer-science-and-artificial-intelligence-laboratory-csail">Computer Science and Artificial Intelligence Laboratory (CSAIL)</category>
      <category domain="https://news.mit.edu/topic/electrical-engineering-computer-science-eecs">Electrical engineering and computer science (EECS)</category>
      <category domain="https://news.mit.edu/topic/aeronautics">Aeronautical and astronautical engineering</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
    </item>
<item>
  <title>Study determines the original orientations of rocks drilled on Mars</title>
  <link>https://news.mit.edu/2024/study-determines-original-orientations-rocks-drilled-mars-0304</link>
  <description><![CDATA[The “oriented” samples, the first of their kind from any planet, could shed light on Mars’ ancient magnetic field.]]></description>
  <pubDate>Mon, 04 Mar 2024 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/study-determines-original-orientations-rocks-drilled-mars-0304</guid>
        <dc:creator>Jennifer Chu | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;As it trundles around an ancient lakebed on Mars, NASA’s Perseverance rover is assembling a one-of-a-kind rock collection. The car-sized explorer is methodically drilling into the Red Planet’s surface and pulling out cores of bedrock that it’s storing in sturdy titanium tubes. Scientists hope to one day return the tubes to Earth and analyze their contents for traces of embedded microbial life.&lt;/p&gt;

&lt;p&gt;Since it touched down on the surface of Mars in 2021, the rover has filled 20 of its 43 tubes with cores of bedrock. Now, MIT geologists have remotely determined a crucial property of the rocks collected to date, which will help scientists answer key questions about the planet’s past.&lt;/p&gt;
&lt;img alt="Video shows the Perseverance drilling into Mars, with a pile of sand emerging from the hole. The photo is sepia-toned and sped-up, showing the shadows moving with the sun." data-align="center" data-caption="Here, the Perseverance drills into the surface of Mars.&amp;lt;br /&amp;gt;
&amp;lt;br /&amp;gt;
Image: NASA/JPL-Caltech/ASU/MSSS" data-entity-type="file" data-entity-uuid="13ae8b14-787b-48bc-af09-1eb55c52a154" src="/sites/default/files/images/inline/Movie_S1-convert.gif" /&gt;
&lt;p&gt;In a study &lt;a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023EA003322" target="_blank"&gt;appearing today&lt;/a&gt; in the journal &lt;em&gt;Earth and Space Science&lt;/em&gt;, an MIT team reports that they have determined the original orientation of most bedrock samples collected by the rover to date. By using the rover’s own engineering data, such as the positioning of the vehicle and its drill, the scientists could estimate the orientation of each sample of bedrock before it was drilled out from the Martian ground.&lt;/p&gt;

&lt;p&gt;The results represent the first time scientists have oriented samples of bedrock on another planet. The team’s method can be applied to future samples that the rover collects as it expands its exploration outside the ancient basin. Piecing together the orientations of multiple rocks at various locations can then give scientists clues to the conditions on Mars in which the rocks originally formed.&lt;/p&gt;

&lt;p&gt;“There are so many science questions that rely on being able to know the orientation of the samples we’re bringing back from Mars,” says study author Elias Mansbach, a graduate student in MIT’s Department of Earth, Atmospheric and Planetary Sciences.&lt;/p&gt;

&lt;p&gt;“The orientation of rocks can tell you something about any magnetic field that may have existed on the planet,” adds Benjamin Weiss, professor of planetary sciences at MIT. “You can also study how water and lava flowed on the planet, the direction of the ancient wind, and tectonic processes, like what was uplifted and what sunk. So it’s a dream to be able to orient bedrock on another planet, because it’s going to open up so many scientific investigations.”&lt;/p&gt;

&lt;p&gt;Weiss and Mansbach’s co-authors are Tanja Bosak and Jennifer Fentress at MIT, along with collaborators at multiple institutions including the Jet Propulsion Laboratory at Caltech.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Profound shift&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Perseverance rover, nicknamed “Percy,” is exploring the floor of Jezero Crater, a large impact crater layered with igneous rocks, which may have been deposited from past volcanic eruptions, as well as sedimentary rocks that likely formed from long-dried-out rivers that fed into the basin.&lt;/p&gt;
&lt;img alt="Against a dark grid with location points, a mosaic of photos is collaged together showing the rocky surface of mars. Some rocks create strong shadows and have a bluish-grey color, while most of the surface is tan." data-align="center" data-caption="An image mosaic, taken by the rover’s Mastcam-Z, shows a portion of the Jezero crater floor, where Perseverance drilled cores of Martian bedrock.&amp;lt;br /&amp;gt;
&amp;lt;br /&amp;gt;
Image: NASA/JPL-Caltech/ASU/MSSS" data-entity-type="file" data-entity-uuid="35447c53-f665-4cb1-9497-554b5db13bbe" src="/sites/default/files/images/inline/01-QZCAM_SOL0255_0265_ZCAM08278_ZCAM08283_L0_Z34_BRAC_WORKSPACE_E01-press.jpg" /&gt;
&lt;p&gt;&lt;/p&gt;
&lt;img alt="Photos show the Martian surface, filled with shards of rocks in grey and tan." data-align="center" data-caption="An image mosaic, taken by the rover’s Mastcam-Z, shows a region of the Jezero delta, where Perseverance drilled and collected cores.&amp;lt;br /&amp;gt;
&amp;lt;br /&amp;gt;
Image: NASA/JPL-Caltech/ASU/MSSS" data-entity-type="file" data-entity-uuid="a636cfe2-edf7-4c77-8f7b-4aa2d33d701d" src="/sites/default/files/images/inline/02-QZCAM_SOL0569_ZCAM08590_L0_Z110_RASPBERRY_ISLAND_AMALIK_WORKSPACE_E01-press_0.jpg" /&gt;
&lt;p&gt;“Mars was once warm and wet, and there’s a possibility there was life there at one time,” Weiss says. “It’s now cold and dry, and something profound must have happened on the planet.”&lt;/p&gt;

&lt;p&gt;Many scientists, including Weiss, suspect that Mars, like Earth, once harbored a magnetic field that shielded the planet from the sun’s solar wind. Conditions then may have been favorable for water and life, at least for a time.&lt;/p&gt;

&lt;p&gt;“Once that magnetic field went away, the sun’s solar wind — this plasma that boils off the sun and moves faster than the speed of sound — just slammed into Mars’ atmosphere and may have removed it over billions of years,” Weiss says. “We want to know what happened, and why.”&lt;/p&gt;

&lt;p&gt;The rocks beneath the Martian surface likely hold a record of the planet’s ancient magnetic field. When rocks first form on a planet’s surface, the direction of their magnetic minerals is set by the surrounding magnetic field. The orientation of rocks can thus help to retrace the direction and intensity of the planet’s magnetic field and how it changed over time.&lt;/p&gt;

&lt;p&gt;Since the Perseverance rover was collecting samples of bedrock, along with surface soil and air, as part of its exploratory mission, Weiss, who is a member of the rover’s science team, and Mansbach looked for ways to determine the original orientation of the rover’s bedrock samples as a first step toward reconstructing Mars’ magnetic history.&lt;/p&gt;

&lt;p&gt;“It was an amazing opportunity, but initially there was no mission requirement to orient bedrock,” Mansbach notes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Roll with it&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Over several months, Mansbach and Weiss met with NASA engineers to hash out a plan for how to estimate the original orientation of each sample of bedrock before it was drilled out of the ground. The problem was a bit like predicting what direction a small circle of sheetcake is pointing, before twisting a round cookie cutter in to pull out a piece. Similarly, to sample bedrock, Perseverance corkscrews a tube-shaped drill into the ground at a perpendicular angle, then pulls the drill directly back out, along with any rock that it penetrates.&lt;/p&gt;

&lt;p&gt;To estimate the orientation of the rock before it was drilled out of the ground, the team realized they need to measure three angles, the hade, azimuth, and roll, which are similar to the pitch, yaw, and roll of a boat. The hade is essentially the tilt of the sample, while the azimuth is the absolute direction the sample is pointing relative to true north. The roll refers to how much a sample must turn before returning to its original position.&lt;/p&gt;

&lt;p&gt;In talking with engineers at NASA, the MIT geologists found that the three angles they required were related to measurements that the rover takes on its own in the course of its normal operations. They realized that to estimate a sample’s hade and azimuth they could use the rover’s measurements of the drill’s orientation, as they could assume the tilt of the drill is parallel to any sample that it extracts.&lt;/p&gt;

&lt;p&gt;To estimate a sample’s roll, the team took advantage of one of the rover’s onboard cameras, which snaps an image of the surface where the drill is about to sample. They reasoned that they could use any distinguishing features on the surface image to determine how much the sample would have to turn in order to return to its original orientation.&lt;/p&gt;

&lt;p&gt;In cases where the surface bore no distinguishing features, the team used the rover’s onboard laser to make a mark in the rock, in the shape of the letter “L,” before drilling out a sample — a move that was jokingly referred to at the time as the first graffiti on another planet.&lt;/p&gt;

&lt;p&gt;By combining all the rover’s positioning, orienting, and imaging data, the team estimated the original orientations of all 20 of the Martian bedrock samples collected so far, with a precision that is comparable to orienting rocks on Earth.&lt;/p&gt;

&lt;p&gt;“We know the orientations to within 2.7 degrees uncertainty, which is better than what we can do with rocks in the Earth,” Mansbach says. “We’re working with engineers now to automate this orienting process so that it can be done with other samples in the future.”&lt;/p&gt;

&lt;p&gt;“The next phase will be the most exciting,” Weiss says. “The rover will drive outside the crater to get the oldest known rocks on Mars, and it’s an incredible opportunity to be able to orient these rocks, and hopefully uncover a lot of these ancient processes.”&lt;/p&gt;

&lt;p&gt;This research was supported, in part, by NASA and the Mars 2020 Participating Scientist program.&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202403/MIT-Orienting-Mars-01-press.jpg?itok=a5x1g5U8" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[MIT geologists determined the original orientation of many of the bedrock samples collected on Mars by the Perseverance rover, depicted in this image rendering. The findings can give scientists clues to the conditions in which the rocks originally formed. ]]></media:description>
              <media:credit>Image: NASA/JPL-Caltech</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/geology">Geology</category>
      <category domain="https://news.mit.edu/topic/mars">Mars</category>
      <category domain="https://news.mit.edu/topic/planetary-science">Planetary science and exploration</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
      <category domain="https://news.mit.edu/topic/space">Space</category>
      <category domain="https://news.mit.edu/topic/space-exploration">Space exploration</category>
      <category domain="https://news.mit.edu/topic/eaps">EAPS</category>
      <category domain="https://news.mit.edu/topic/school-science">School of Science</category>
      <category domain="https://news.mit.edu/topic/nasa">NASA</category>
    </item>
<item>
  <title>New AI model could streamline operations in a robotic warehouse</title>
  <link>https://news.mit.edu/2024/new-ai-model-could-streamline-operations-robotic-warehouse-0227</link>
  <description><![CDATA[By breaking an intractable problem into smaller chunks, a deep-learning technique identifies the optimal areas for thinning out traffic in a warehouse. ]]></description>
  <pubDate>Tue, 27 Feb 2024 00:00:00 -0500</pubDate>
    <guid isPermaLink="true">https://news.mit.edu/2024/new-ai-model-could-streamline-operations-robotic-warehouse-0227</guid>
        <dc:creator>Adam Zewe | MIT News</dc:creator>
  <content:encoded>&lt;p&gt;Hundreds of robots zip back and forth across the floor of a colossal robotic warehouse, grabbing items and delivering them to human workers for packing and shipping. Such warehouses are increasingly becoming part of the supply chain in many industries, from e-commerce to automotive production.&lt;/p&gt;

&lt;p&gt;However, getting 800 robots to and from their destinations efficiently while keeping them from crashing into each other is no easy task. It is such a complex problem that even the best path-finding algorithms struggle to keep up with the breakneck pace of e-commerce or manufacturing.&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In a sense, these robots are like cars trying to navigate a crowded city center. So, a group of MIT researchers who use AI to mitigate traffic congestion applied ideas from that domain to tackle this problem.&lt;/p&gt;

&lt;p&gt;They built a deep-learning model that encodes important information about the warehouse, including the robots, planned paths, tasks, and obstacles, and uses it to predict the best areas of the warehouse to decongest to improve overall efficiency.&lt;/p&gt;

&lt;p&gt;Their technique divides the warehouse robots into groups, so these smaller groups of robots can be decongested faster with traditional algorithms used to coordinate robots. In the end, their method decongests the robots nearly four times faster than a strong random search method.&lt;/p&gt;

&lt;p&gt;In addition to streamlining warehouse operations, this deep learning approach could be used in other complex planning tasks, like computer chip design or pipe routing in large buildings.&lt;/p&gt;

&lt;p&gt;“We devised a new neural network architecture that is actually suitable for real-time operations at the scale and complexity of these warehouses. It can encode hundreds of robots in terms of their trajectories, origins, destinations, and relationships with other robots, and it can do this in an efficient manner that reuses computation across groups of robots,” says Cathy Wu, the Gilbert W. Winslow Career Development Assistant Professor in Civil and Environmental Engineering (CEE), and a member of a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society (IDSS).&lt;/p&gt;

&lt;p&gt;Wu, senior author of a &lt;a href="https://openreview.net/pdf?id=2NpAw2QJBY" target="_blank"&gt;paper on this technique&lt;/a&gt;, is joined by lead author Zhongxia Yan, a graduate student in electrical engineering and computer science. The work will be presented at the International Conference on Learning Representations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Robotic Tetris&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;From a bird’s eye view, the floor of a robotic e-commerce warehouse looks a bit like a fast-paced game of “Tetris.”&lt;/p&gt;

&lt;p&gt;When a customer order comes in, a robot travels to an area of the warehouse, grabs the shelf that holds the requested item, and delivers it to a human operator who picks and packs the item. Hundreds of robots do this simultaneously, and if two robots’ paths conflict as they cross the massive warehouse, they might crash.&lt;/p&gt;

&lt;p&gt;Traditional search-based algorithms avoid potential crashes by keeping one robot on its course and replanning a trajectory for the other. But with so many robots and potential collisions, the problem quickly grows exponentially.&lt;/p&gt;

&lt;p&gt;“Because the warehouse is operating online, the robots are replanned about every 100 milliseconds. That means that every second, a robot is replanned 10 times. So, these operations need to be very fast,” Wu says.&lt;/p&gt;

&lt;p&gt;Because time is so critical during replanning, the MIT researchers use machine learning to focus the replanning on the most actionable areas of congestion — where there exists the most potential to reduce the total travel time of robots.&lt;/p&gt;

&lt;p&gt;Wu and Yan built a neural network architecture that considers smaller groups of robots at the same time. For instance, in a warehouse with 800 robots, the network might cut the warehouse floor into smaller groups that contain 40 robots each.&lt;/p&gt;

&lt;p&gt;Then, it predicts which group has the most potential to improve the overall solution if a search-based solver were used to coordinate trajectories of robots in that group.&lt;/p&gt;

&lt;p&gt;An iterative process, the overall algorithm picks the most promising robot group with the neural network, decongests the group with the search-based solver, then picks the next most promising group with the neural network, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Considering relationships&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The neural network can reason about groups of robots efficiently because it captures complicated relationships that exist between individual robots. For example, even though one robot may be far away from another initially, their paths could still cross during their trips.&lt;/p&gt;

&lt;p&gt;The technique also streamlines computation by encoding constraints only once, rather than repeating the process for each subproblem. For instance, in a warehouse with 800 robots, decongesting a group of 40 robots requires holding the other 760 robots as constraints. Other approaches require reasoning about all 800 robots once per group in each iteration.&lt;/p&gt;

&lt;p&gt;Instead, the researchers’ approach only requires reasoning about the 800 robots once across all groups in each iteration.&lt;/p&gt;

&lt;p&gt;“The warehouse is one big setting, so a lot of these robot groups will have some shared aspects of the larger problem. We designed our architecture to make use of this common information,” she adds.&lt;/p&gt;

&lt;p&gt;They tested their technique in several simulated environments, including some set up like warehouses, some with random obstacles, and even maze-like settings that emulate building interiors.&lt;/p&gt;

&lt;p&gt;By identifying more effective groups to decongest, their learning-based approach decongests the warehouse up to four times faster than strong, non-learning-based approaches. Even when they factored in the additional computational overhead of running the neural network, their approach still solved the problem 3.5 times faster.&lt;/p&gt;

&lt;p&gt;In the future, the researchers want to derive simple, rule-based insights from their neural model, since the decisions of the neural network can be opaque and difficult to interpret. Simpler, rule-based methods could also be easier to implement and maintain in actual robotic warehouse settings.&lt;/p&gt;

&lt;p&gt;“This approach is based on a novel architecture where convolution and attention mechanisms interact effectively and efficiently. Impressively, this leads to being able to take into account the spatiotemporal component of the constructed paths without the need of problem-specific feature engineering. The results are outstanding: Not only is it possible to improve on state-of-the-art large neighborhood search methods in terms of quality of the solution and speed, but the model generalizes to unseen cases wonderfully,” says Andrea Lodi, the Andrew H. and Ann R. Tisch Professor at Cornell Tech, and who was not involved with this research.&lt;/p&gt;

&lt;p&gt;This work was supported by Amazon and the MIT Amazon Science Hub.&lt;/p&gt;
</content:encoded>
      <media:content url="https://news.mit.edu/sites/default/files/styles/news_article__cover_image__original/public/images/202402/MIT-Neighborhood-Search-01.jpg?itok=NZ3yqJK-" medium="image" type="image/jpeg" width="390" height="260">
              <media:description type="plain"><![CDATA[A group of MIT researchers who use AI to mitigate traffic congestion applied ideas from that domain to tackle the problem of multiple robots in a warehouse setting.]]></media:description>
              <media:credit>Image: iStock</media:credit>
      </media:content>
        <category domain="https://news.mit.edu/topic/research">Research</category>
      <category domain="https://news.mit.edu/topic/computers">Computer science and technology</category>
      <category domain="https://news.mit.edu/topic/artificial-intelligence2">Artificial intelligence</category>
      <category domain="https://news.mit.edu/topic/machine-learning">Machine learning</category>
      <category domain="https://news.mit.edu/topic/algorithms">Algorithms</category>
      <category domain="https://news.mit.edu/topic/civil-engineering">Civil and environmental engineering</category>
      <category domain="https://news.mit.edu/topic/lids">Laboratory for Information and Decision Systems (LIDS)</category>
      <category domain="https://news.mit.edu/topic/idss">IDSS</category>
      <category domain="https://news.mit.edu/topic/school-engineering">School of Engineering</category>
      <category domain="https://news.mit.edu/topic/mit-schwarzman-college-computing">MIT Schwarzman College of Computing</category>
      <category domain="https://news.mit.edu/topic/robotics">Robotics</category>
    </item>

  </channel>
</rss>
